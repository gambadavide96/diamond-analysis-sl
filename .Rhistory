)
model <- lm(price ~ carat, data = Diamonds2[train,])
intercept <- coef(model)[1]
slope <- coef(model)[2]
abline(a = intercept, b = slope, col = "red", lwd = 2)
abline(a = 0.6, b = slope, col = "red", lwd = 2)
##Carat e price
plot(Diamonds2[train,"carat"],Diamonds2[train,"price"],
col=y[train] + 2,
ylab = "Price",
xlab="Carat")
legend(
"topright",  # Posizione della leggenda
legend = c("Low", "High"),  # Etichette della leggenda
col = c(2, 3),  # Colori corrispondenti
pch = 1  # Simbolo dei punti
)
model <- lm(price ~ carat, data = Diamonds2[train,])
intercept <- coef(model)[1]
slope <- coef(model)[2]
abline(a = 0.6, b = slope, col = "red", lwd = 2)
abline(a = 0.6, b = 0, col = "red", lwd = 2)
abline(a = 0.6, b = 0, col = "red", lwd = 2)
abline(a = 0.6, b = 1, col = "red", lwd = 2)
abline(a = intercept, b = slope, col = "red", lwd = 2)
abline(a = intercept, b = slope-1, col = "red", lwd = 2)
abline(a = intercept, b = slope-0.5, col = "red", lwd = 2)
##Carat e price
plot(Diamonds2[train,"carat"],Diamonds2[train,"price"],
col=y[train] + 2,
ylab = "Price",
xlab="Carat")
legend(
"topright",  # Posizione della leggenda
legend = c("Low", "High"),  # Etichette della leggenda
col = c(2, 3),  # Colori corrispondenti
pch = 1  # Simbolo dei punti
)
abline(a = intercept, b = slope-0.5, col = "red", lwd = 2)
abline(a = intercept, b = slope, col = "red", lwd = 2)
abline(a = intercept+1, b = slope, col = "red", lwd = 2)
abline(a = intercept+0.1, b = slope, col = "red", lwd = 2)
##Carat e price
plot(Diamonds2[train,"carat"],Diamonds2[train,"price"],
col=y[train] + 2,
ylab = "Price",
xlab="Carat")
legend(
"topright",  # Posizione della leggenda
legend = c("Low", "High"),  # Etichette della leggenda
col = c(2, 3),  # Colori corrispondenti
pch = 1  # Simbolo dei punti
)
#vettore per i colori:
y <- ifelse(Diamonds2$quality == "High", 1, 0)
##Carat e price
plot(Diamonds2[train,"carat"],Diamonds2[train,"price"],
col=y[train] + 2,
ylab = "Price",
xlab="Carat")
legend(
"topright",  # Posizione della leggenda
legend = c("Low", "High"),  # Etichette della leggenda
col = c(2, 3),  # Colori corrispondenti
pch = 1  # Simbolo dei punti
)
svm_linear2 <- svm(quality ~ carat + price, data = Diamonds2 ,subset = train ,
kernel = "linear",
cost = 1, scale = TRUE)
summary(svm_linear2)
plot(svm_linear2, Diamonds2[train,],price~carat)
#Previsioni sul dataset di test
svm_linear_pred2 <- predict(svm_linear2 , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_linear_pred2 , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_linear_pred2 != Diamonds2$quality[-train])
#Accuracy: 88.68%
mean(svm_linear_pred2 == Diamonds2$quality[-train])
svm_poly2 <- svm(quality ~ carat + price, data = Diamonds2 ,subset = train ,
kernel = "polyniomial",
cost = 1,
degree = 3,
scale = TRUE)
svm_poly2 <- svm(quality ~ carat + price, data = Diamonds2 ,subset = train ,
kernel = "polynomial",
cost = 1,
degree = 3,
scale = TRUE)
summary(svm_poly2)
plot(svm_poly2, Diamonds2[train,],price~carat)
#Previsioni sul dataset di test
svm_poly_pred2 <- predict(svm_poly2 , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_poly_pred2 , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_poly_pred2 != Diamonds2$quality[-train])
#Accuracy: 88.68%
mean(svm_linear_pred2 == Diamonds2$quality[-train])
#Accuracy: 88.68%
mean(svm_poly_pred2 == Diamonds2$quality[-train])
svm_poly2 <- svm(quality ~ carat + price, data = Diamonds2 ,subset = train ,
kernel = "polynomial",
cost = 1,
degree = 2,
scale = TRUE)
summary(svm_poly2)
plot(svm_poly2, Diamonds2[train,],price~carat)
#Previsioni sul dataset di test
svm_poly_pred2 <- predict(svm_poly2 , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_poly_pred2 , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_poly_pred2 != Diamonds2$quality[-train])
#Accuracy: 88.68%
mean(svm_poly_pred2 == Diamonds2$quality[-train])
svm_poly2 <- svm(quality ~ carat + price, data = Diamonds2 ,subset = train ,
kernel = "polynomial",
cost = 1,
degree = 3,
gamma=0.5,
scale = TRUE)
summary(svm_poly2)
plot(svm_poly2, Diamonds2[train,],price~carat)
#Previsioni sul dataset di test
svm_poly_pred2 <- predict(svm_poly2 , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_poly_pred2 , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_poly_pred2 != Diamonds2$quality[-train])
#Accuracy: 88.68%
mean(svm_poly_pred2 == Diamonds2$quality[-train])
svm_radial2 <- svm(quality ~ carat + price, data = Diamonds2 ,subset = train ,
kernel = "radial",
cost = 1,
gamma=0.5,
scale = TRUE)
summary(svm_radial2)
plot(svm_radial2, Diamonds2[train,],price~carat)
#Previsioni sul dataset di test
svm_radial_pred2 <- predict(svm_radial2 , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_radial_pred2 , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_radial_pred2 != Diamonds2$quality[-train])
#Accuracy: 88.68%
mean(svm_radial_pred2 == Diamonds2$quality[-train])
#Accuracy: 88.95%
mean(svm_radial_pred2 == Diamonds2$quality[-train])
#Accuracy 88.62%
mean(svm_linear_pred == Diamonds2$quality[-train])
#Accuracy: 88.68%
mean(svm_linear_pred2 == Diamonds2$quality[-train])
setwd("~/Documents/GitHub/Statistical_Learning")
################################################################################
######### Setting Dataset
################################################################################
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
### Transform Categorical Variables as factors
Diamonds$cut <- factor(Diamonds$cut,
levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))
Diamonds$color <- factor(Diamonds$color,
levels = c("J", "I", "H", "G", "F","E","D"))
Diamonds$clarity <- factor(Diamonds$clarity,
levels=c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"))
set.seed(2)
detect_outlier <- function(x) {
Quantile1 <- quantile(x, probs=.25)
Quantile3 <- quantile(x, probs=.75)
IQR = Quantile3 - Quantile1
x > Quantile3 + (IQR*1.5) | x < Quantile1 - (IQR*1.5)
}
remove_outlier <- function(dataframe,columns=names(dataframe)) {
for (col in columns) {
dataframe <- dataframe[!detect_outlier(dataframe[[col]]), ]
}
print("Remove outliers")
print(dataframe)
}
Diamonds <- remove_outlier(Diamonds, c('carat', 'depth_percentage', 'table', 'price',
"length", 'width', "depth"))
#train and test indexes
train <- sample(nrow(Diamonds),floor(nrow(Diamonds)*0.7),replace = FALSE)
price_test <- Diamonds$price[-train]
##### Model with interaction terms #####
lm_model_2 = lm(price ~ . + (length*width*depth) , data = Diamonds,
subset = train)
summary(lm_model_2)
#confidence interval 95%
confint(lm_model_2)
#R^2
summary(lm_model_2)$r.sq
#Train RMSE
lm_train_RMSE_2 = sqrt(mean((lm_model_2$residuals)^2))
lm_train_RMSE_2
#Test RMSE
fitt_value_lm_2 = predict(lm_model_2,newdata = Diamonds[-train,])
lm_test_RMSE_2 = sqrt(mean((fitt_value_lm_2 - Diamonds$price[-train])^2))
lm_test_RMSE_2
################################################################################
############################### Subset selection methods
################################################################################
library(leaps)
###### Best subset selection ######
model_bwd <- regsubsets(price ~ .+ (length*width*depth), data = Diamonds[train,],
nvmax = 27)
summary(model_bwd)
#We now compute the validation set error for the best
#model of each model size. We first make a model matrix from the test
#data.
#The model.matrix() function is used in many regression packages for building
#an “X” matrix from data.
test_mat <- model.matrix(price ~ . + (length*width*depth),
data = Diamonds[-train , ])
#We now compute the validation set error for the best
#model of each model size. We first make a model matrix from the test
#data.
#The model.matrix() function is used in many regression packages for building
#an “X” matrix from data.
test_mat <- model.matrix(price ~ . + (length*width*depth),
data = Diamonds[-train , ])
for (i in 1:27) {
coefi <- coef(model_bwd , id = i)
pred <- test_mat[, names(coefi)] %*% coefi #prodotto matrici coefficienti per la previsione di price
val_RMSE[i] <- sqrt(mean((Diamonds$price[-train] - pred)^2))
}
#Now we run a loop, and for each size i, we
#extract the coefficients from regfit.best for the best model of that size,
#multiply them into the appropriate columns of the test model matrix to
#form the predictions, and compute the test MSE.
val_RMSE <- rep(NA, 27)
for (i in 1:27) {
coefi <- coef(model_bwd , id = i)
pred <- test_mat[, names(coefi)] %*% coefi #prodotto matrici coefficienti per la previsione di price
val_RMSE[i] <- sqrt(mean((Diamonds$price[-train] - pred)^2))
}
val_RMSE #Test RMSE per tutti i modelli calcolati
#We find that the best model is the one that contains  variables.
min_RMSE = which.min(val_RMSE)
min_RMSE
val_RMSE[min_RMSE]
RMSE_subselection <- val_RMSE[min_RMSE]
coef(model_bwd , min_RMSE)
min_RMSE
val_RMSE[min_RMSE]
#R^2 e modello con R^2 più alto in grafico
summary(model_bwd)$rsq
plot(summary(model_bwd)$rsq,xlab = "N° regressor",ylab = "R^2")
max_r2 <- which.max(summary(model_bwd)$rsq)
points(max_r2, summary(model_bwd)$rsq[max_r2], col = "red", cex = 2, pch = 20)
#Modello con test RMSE più basso
plot(val_RMSE,xlab = "N° regressor",ylab = " Test RMSE")
points(min_RMSE, val_RMSE[min_RMSE], col = "blue", cex = 2, pch = 20)
legend("topright", legend = "Min Test RMSE", col = "blue", pch = 20)
coef(model_bwd , min_RMSE)
#preparing data
#creating regressor (automatic handle categorical variables in dummy variables)
x <- model.matrix ( price ~ . +(length * width * depth) ,
Diamonds )[,-1]  #tutte le righe - la prima colonna (intercetta)
y <- Diamonds$price
#Note that by default, the glmnet() function standardizes the
#variables so that they are on the same scale. To turn off this default setting,
#use the argument standardize = FALSE
ridge_model_1 <- glmnet(x[train , ], y[train], alpha = 0,
lambda = NULL,
standardize = TRUE)
################################################################################
############################### Ridge Regression
################################################################################
library(glmnet)
#Note that by default, the glmnet() function standardizes the
#variables so that they are on the same scale. To turn off this default setting,
#use the argument standardize = FALSE
ridge_model_1 <- glmnet(x[train , ], y[train], alpha = 0,
lambda = NULL,
standardize = TRUE)
dim(coef(ridge_model_1))
#Esempi di valori di lambda
#Seleziono lambda 5 (grande) e calcolo l1 norm per lambda 5 (piccola)
ridge_model_1$lambda[5]
coef(ridge_model_1)[, 5]
sqrt(sum(coef(ridge_model_1)[-1,5]^2)) ##l1 norm (escludendo intercetta)
####### Choosing the best lambda #######
cv_ridge_out <- cv.glmnet(x[train , ], y[train], alpha = 0,
lambda = NULL,
nfolds = 10)
plot(cv_ridge_out)
bestlam_ridge <- cv_ridge_out$lambda.min
bestlam_ridge
#Beta del modello trovato per il miglior lambda
coef(ridge_model_2)
### Test RMSE ###
ridge_model_2 <- glmnet(x[train , ], y[train], alpha = 0,
lambda = bestlam_ridge,
standardize = TRUE)
#Beta del modello trovato per il miglior lambda
coef(ridge_model_2)
fitt_value_ridge <- predict(ridge_model_2,newx = x[-train,])
test_RMSE_ridge = sqrt(mean((y[-train] - fitt_value_ridge)^2))
test_RMSE_ridge
lambda_grid <- 10^seq(-3,1,length = 100);
lasso_model_1 <- glmnet(x[train , ], y[train], alpha = 1,
lambda = NULL,
standardize = TRUE)
dim(coef(lasso_model_1))
### Andamento dei coefficienti al variare di lambda e l1 norm ###
plot(lasso_model_1, xvar = "lambda",xlab="Log(λ)")
plot(lasso_model_1, xvar = "norm",xlab="l1 norm")
####### Choosing the best lambda #######
cv_lasso_out <- cv.glmnet(x[train , ], y[train], alpha = 1,
lambda = NULL,
nfolds = 10)
plot(cv_lasso_out)
bestlam_lasso <- cv_lasso_out$lambda.min
bestlam_lasso #0.0001
### Final model e Test RMSE ###
lasso_model_2 <- glmnet(x[train , ], y[train], alpha = 1,
lambda = bestlam_lasso,
standardize = TRUE)
#Beta del modello trovato per il miglior lambda
coef(lasso_model_2)
fitt_value_lasso <- predict(lasso_model_2,newx = x[-train,])
test_RMSE_lasso = sqrt(mean((y[-train] - fitt_value_lasso)^2))
test_RMSE_lasso
fit_length_3 <- lm(price ~ poly(length, 3), data = Diamonds)
#Definisco i valori su cui fare la previsione
length_lims <- range(Diamonds$length)
length_grid <- seq(from = length_lims[1], to = length_lims[2],by=0.01)
#Calcolo previsioni
preds_length <- predict(fit_length_3,newdata = list(length = length_grid) ,se=TRUE)
plot(Diamonds$length, Diamonds$price,
main = "Poly(Length,5) vs Price",
xlab = "Lenght (mm)",
ylab = "Price (Thousands $)")
lines(length_grid, preds_length$fit, lwd = 2, col = "blue")
fit_length_2 <- lm(price ~ poly(length, 2), data = Diamonds)
#Calcolo previsioni
preds_length <- predict(fit_length_2,newdata = list(length = length_grid) ,se=TRUE)
plot(Diamonds$length, Diamonds$price,
main = "Poly(Length,5) vs Price",
xlab = "Lenght (mm)",
ylab = "Price (Thousands $)")
lines(length_grid, preds_length$fit, lwd = 2, col = "blue")
#Regressioni polinomiale
fit_length_1 <- lm(price ~ poly(length, 1), data = Diamonds)
#Calcolo previsioni
preds_length <- predict(fit_length_1,newdata = list(length = length_grid) ,se=TRUE)
plot(Diamonds$length, Diamonds$price,
main = "Poly(Length,5) vs Price",
xlab = "Lenght (mm)",
ylab = "Price (Thousands $)")
lines(length_grid, preds_length$fit, lwd = 2, col = "blue")
#Calcolo previsioni
preds_length <- predict(fit_length_2,newdata = list(length = length_grid) ,se=TRUE)
plot(Diamonds$length, Diamonds$price,
main = "Poly(Length,5) vs Price",
xlab = "Lenght (mm)",
ylab = "Price (Thousands $)")
lines(length_grid, preds_length$fit, lwd = 2, col = "blue")
#Calcolo previsioni
preds_length <- predict(fit_length_3,newdata = list(length = length_grid) ,se=TRUE)
plot(Diamonds$length, Diamonds$price,
main = "Poly(Length,5) vs Price",
xlab = "Lenght (mm)",
ylab = "Price (Thousands $)")
lines(length_grid, preds_length$fit, lwd = 2, col = "blue")
lines(length_grid, preds_length$fit, lwd = 2, col = "blue")
#Regressioni polinomiale
fit_length_1 <- lm(price ~ poly(length, 1), data = Diamonds)
summary(fit_length_1)
fit_length_2
summary(fit_length_2)
fit_width_3 <- lm(price ~ poly(width, 3), data = Diamonds)
#Definisco i valori su cui fare la previsione
width_lims <- range(Diamonds$width)
width_grid <- seq(from = width_lims[1], to = width_lims[2],by=0.01)
#Calcolo previsioni
preds_width <- predict(fit_width_3,newdata = list(width = width_grid) ,se=TRUE)
plot(Diamonds$width, Diamonds$price,
main = "Poly(Width,3) vs Price",
xlab = "Width (mm)",
ylab = "Price (Thousands $)")
lines(width_grid, preds_width$fit, lwd = 2, col = "blue")
fit_width_4 <- lm(price ~ poly(width, 4), data = Diamonds)
#Calcolo previsioni
preds_width <- predict(fit_width_4,newdata = list(width = width_grid) ,se=TRUE)
plot(Diamonds$width, Diamonds$price,
main = "Poly(Width,3) vs Price",
xlab = "Width (mm)",
ylab = "Price (Thousands $)")
lines(width_grid, preds_width$fit, lwd = 2, col = "blue")
fit_table_1 <- lm(price ~ poly(table , 1), data = Diamonds)
#Definisco i valori su cui fare la previsione
table_lims <- range(Diamonds$table)
table_grid <- seq(from = table_lims[1], to = table_lims[2],by=0.01)
#Calcolo previsioni
preds_table <- predict(fit_table_1,newdata = list(table = table_grid) ,se=TRUE)
#Grafico
plot(Diamonds$table, Diamonds$price,
main = "Poly(Table,1) vs Price",
xlab = "Table",
ylab = "Price (Thousands $)")
lines(table_grid, preds_table$fit, lwd = 2, col = "blue")
#Definisco i valori su cui fare la previsione
depthP_lims <- range(Diamonds$depth_percentage)
depthP_grid <- seq(from = depthP_lims[1], to = depthP_lims[2],by=0.01)
#Calcolo previsioni
preds_depthP <- predict(fit_depthP_1,newdata = list(depth_percentage = depthP_grid) ,se=TRUE)
fit_depthP_1 <- lm(price ~ poly(depth_percentage , 1), data = Diamonds)
#Calcolo previsioni
preds_depthP <- predict(fit_depthP_1,newdata = list(depth_percentage = depthP_grid) ,se=TRUE)
plot(Diamonds$depth_percentage, Diamonds$price,
main = "Poly(Depth%,1) vs Price",
xlab = "Depth percentage",
ylab = "Price (Thousands $)")
lines(depthP_grid, preds_depthP$fit, lwd = 2, col = "blue")
#Calcolo previsioni
preds_carat <- predict(fit_carat_5,newdata = list(carat = carat_grid) ,se=TRUE)
fit_carat_5 <- lm(price ~ poly(carat , 5), data = Diamonds)
#Calcolo previsioni
preds_carat <- predict(fit_carat_5,newdata = list(carat = carat_grid) ,se=TRUE)
#Definisco i valori su cui fare la previsione
carat_lims <- range(Diamonds$carat)
carat_grid <- seq(from = carat_lims[1], to = carat_lims[2],by=0.01)
#Calcolo previsioni
preds_carat <- predict(fit_carat_5,newdata = list(carat = carat_grid) ,se=TRUE)
#Grafico
plot(Diamonds$carat, Diamonds$price,
main = "Poly(Carat,5) vs Price",
xlab = "Carat",
ylab = "Price (Thousands $)")
lines(carat_grid, preds_carat$fit, lwd = 2, col = "blue")
fit_carat_3 <- lm(price ~ poly(carat , 3), data = Diamonds)
#Calcolo previsioni
preds_carat <- predict(fit_carat_3,newdata = list(carat = carat_grid) ,se=TRUE)
#Grafico
plot(Diamonds$carat, Diamonds$price,
main = "Poly(Carat,5) vs Price",
xlab = "Carat",
ylab = "Price (Thousands $)")
lines(carat_grid, preds_carat$fit, lwd = 2, col = "blue")
#Calcolo previsioni
preds_table <- predict(fit_table_1,newdata = list(table = table_grid) ,se=TRUE)
#Grafico
plot(Diamonds$table, Diamonds$price,
main = "Poly(Table,1) vs Price",
xlab = "Table",
ylab = "Price (Thousands $)")
lines(table_grid, preds_table$fit, lwd = 2, col = "blue")
fit_depthP_2 <- lm(price ~ poly(depth_percentage , 2), data = Diamonds)
#Calcolo previsioni
preds_depthP <- predict(fit_depthP_2,newdata = list(depth_percentage = depthP_grid) ,se=TRUE)
plot(Diamonds$depth_percentage, Diamonds$price,
main = "Poly(Depth%,1) vs Price",
xlab = "Depth percentage",
ylab = "Price (Thousands $)")
lines(depthP_grid, preds_depthP$fit, lwd = 2, col = "blue")
fit_table_2 <- lm(price ~ poly(table , 2), data = Diamonds)
#Calcolo previsioni
preds_table <- predict(fit_table_2,newdata = list(table = table_grid) ,se=TRUE)
#Grafico
plot(Diamonds$table, Diamonds$price,
main = "Poly(Table,1) vs Price",
xlab = "Table",
ylab = "Price (Thousands $)")
lines(table_grid, preds_table$fit, lwd = 2, col = "blue")
fit_depthP_2 <- lm(price ~ poly(depth_percentage , 2), data = Diamonds)
#Definisco i valori su cui fare la previsione
depthP_lims <- range(Diamonds$depth_percentage)
depthP_grid <- seq(from = depthP_lims[1], to = depthP_lims[2],by=0.01)
#Calcolo previsioni
preds_depthP <- predict(fit_depthP_2,newdata = list(depth_percentage = depthP_grid) ,se=TRUE)
plot(Diamonds$depth_percentage, Diamonds$price,
main = "Poly(Depth%,2) vs Price",
xlab = "Depth percentage",
ylab = "Price (Thousands $)")
lines(depthP_grid, preds_depthP$fit, lwd = 2, col = "blue")
poly_model <- lm(price ~ poly(carat,5)+poly(length,5)+poly(width,5)+
poly(depth,5)+poly(table,2)+poly(depth_percentage,2)
+color+cut+clarity,data=Diamonds,subset = train)
summary(poly_model)
y_hat_poly = predict(poly_model,newdata = Diamonds[-train,])
poly_test_RMSE = sqrt(mean((y_hat_poly - Diamonds$price[-train])^2))
poly_test_RMSE
####### GAM train and test #######
gam_model_1 <- gam(price~ s(carat,4) + cut + color + clarity +
s(depth_percentage,5) + s(table,3) + s(length,4) +
s(width,4) + s(depth,4),data=Diamonds[train, ])
library(gam)
####### GAM train and test #######
gam_model_1 <- gam(price~ s(carat,4) + cut + color + clarity +
s(depth_percentage,5) + s(table,3) + s(length,4) +
s(width,4) + s(depth,4),data=Diamonds[train, ])
par(mfrow = c(1,1))
plot(gam_model_1,se=TRUE)
####### GAM train and test #######
gam_model_1 <- gam(price~ s(carat,4) + cut + color + clarity +
s(depth_percentage,5) + s(table,2) + s(length,4) +
s(width,4) + s(depth,4),data=Diamonds[train, ])
par(mfrow = c(1,1))
plot(gam_model_1,se=TRUE)
#Test RMSE
gam_model_1_RMSE = sqrt(mean((Diamonds$price[-train] - gam_pred_value)^2))
##### Analisi dei residui #####
gam_pred_value <- predict(gam_model_1,newdata = Diamonds[-train,])
gam_model_1_residuals = Diamonds$price[-train] - gam_pred_value
#Test RMSE
gam_model_1_RMSE = sqrt(mean((Diamonds$price[-train] - gam_pred_value)^2))
gam_model_1_RMSE #Miglioramento significativo
