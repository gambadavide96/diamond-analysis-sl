train <- sample(1:nrow(Boston), nrow(Boston) / 2)
bag.boston <- randomForest(medv ~ ., data = Boston , subset = train,
mtry = 12, importance = TRUE)
####### Test MSE Bagging #####
yhat.bag <- predict(bag.boston , newdata = Boston[-train , ])
plot(yhat.bag , boston.test)
boston.test <- Boston[-train, "medv"] #Osservazioni reali
plot(yhat.bag , boston.test)
abline(0, 1)
mean((yhat.bag - boston.test)^2)
#We could change the number of trees grown by randomForest() using the
#ntree argument:
bag.boston <- randomForest(medv ! ., data = Boston , subset = train,
#We could change the number of trees grown by randomForest() using the
#ntree argument:
bag.boston <- randomForest(medv ~ ., data = Boston , subset = train,
mtry = 12, ntree = 25)
yhat.bag <- predict(bag.boston , newdata = Boston[-train , ])
mean((yhat.bag - boston.test)^2)
#Growing a random forest proceeds in exactly the same way, except that
#we use a smaller value of the mtry argument. By default, randomForest()
#uses p/3 variables when building a random forest of regression trees, and
#sqrt(p) variables when building a random forest of classification trees. Here we
#use mtry = 6.
set.seed(1)
rf.boston <- randomForest(medv ~ ., data = Boston ,
subset = train , mtry = 6, importance = TRUE)
yhat.rf <- predict(rf.boston, newdata = Boston[-train, ])
mean((yhat.rf - boston.test)^2)
#Using the importance() function, we can view the importance of each variable.
importance(rf.boston)
#Plots of these importance measures can be produced using the varImpPlot() function.
varImpPlot(rf.boston)
#The argument n.trees = 5000 indicates that we want 5000 trees, and the
#option interaction.depth = 4 limits the depth of each tree.
library(gbm)
set.seed(1)
boost.boston <- gbm(medv ~ ., data = Boston[train , ],
distribution = "gaussian", n.trees = 5000,
interaction.depth = 4)
summary(boost.boston)
#We can also produce partial dependence plots for these two variables. These plots
#illustrate the marginal effect of the selected variables on the response after
#integrating out the other variables. In this case, as we might expect, median
#house prices are increasing with rm and decreasing with lstat.
plot(boost.boston , i = "rm")
plot(boost.boston , i = "lstat")
##### Test MSE ####
#We now use the boosted model to predict medv on the test set:
yhat.boost <- predict(boost.boston , newdata = Boston[-train , ], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
#If we want to, we can perform boosting with a different
#value of the shrinkage parameter lambda in (8.10). The default value is 0.001,
#but this is easily modified. Here we take lambda = 0.2.
boost.boston <- gbm(medv ~ ., data = Boston[train , ],
distribution = "gaussian", n.trees = 5000,
interaction.depth = 4, shrinkage = 0.2, verbose = F)
yhat.boost <- predict(boost.boston ,newdata = Boston[-train , ], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
# Bagging
library ( randomForest )
set.seed(1)
library ( ISLR2 )
#mtry = Number of variables randomly sampled as candidates at each split.
#ntree = Number of trees to grow.
bagg_model <- randomForest(medv ~ . ,data = Boston, subset = train,
mtry = ncol(Boston)-1, importance = TRUE,replace = TRUE)
train <- sample(1:nrow(Boston),floor(nrow(Boston)*0.5))
#mtry = Number of variables randomly sampled as candidates at each split.
#ntree = Number of trees to grow.
bagg_model <- randomForest(medv ~ . ,data = Boston, subset = train,
mtry = ncol(Boston)-1, importance = TRUE,replace = TRUE)
bagg_model
#mtry = Number of variables randomly sampled as candidates at each split.
#ntree = Number of trees to grow.
bagg_model <- randomForest(medv ~ . ,data = Boston, subset = train,
mtry = ncol(Boston)-1,
importance = TRUE,
replace = TRUE,
ntree = 100)
bagg_model
#mtry = Number of variables randomly sampled as candidates at each split.
#ntree = Number of trees to grow.
bagg_model <- randomForest(medv ~ . ,data = Boston, subset = train,
mtry = ncol(Boston)-1,
importance = TRUE,
replace = TRUE,
ntree = 500)
bagg_model
plot(bagg_model)
yhat <- predict(bagg_model, newdata = Boston[-train,])
plot(yhat,Boston$medv[-train])
abline(0,1)
mse <- mean((yhat - Boston$medv[-train])^2)
mse
# how to change number of tree?
# add ntree oprion
bagg_model <- randomForest(medv ~ . ,data = Boston, subset = train,
mtry = ncol(Boston)-1, importance = TRUE, ntree = 100)
bagg_model
plot(bagg_model)
importance(bagg_model)
# Random Forest
# default p = sqrt(m) -> see doc
forest_model <- randomForest(medv ~ . ,data = Boston, subset = train,
mtry = floor(sqrt(ncol(Boston)-1)), importance = TRUE, ntree = 100)
forest_model
# if the object has a non-null test component, then the returned object is a
# matrix where the first column is the out-of-bag estimate of error,
# and the second column is for the test set.
plot(forest_model,type = 'b',col="green",pch = "+")
par(new=TRUE)
plot(bagg_model,type = 'b',col="red",pch='o')
yhat <- predict(forest_model, newdata = Boston[-train,])
mse <- mean((yhat - Boston$medv[-train])^2)
mse
importance(forest_model)
# Boosting
library ( gbm )
set.seed(1)
ntree = 5000;
boost_model
boost_model <- gbm(medv ~ . , data = Boston[train,],
distribution = "gaussian" , n.trees = ntree,
interaction.depth = 4, shrinkage = 0.01 , verbose = F)
boost_model
yhat <- predict(boost_model, newdata = Boston[-train,], n.trees = ntree)
mse <- mean((yhat - Boston$medv[-train])^2)
mse
plot(boost.boston)
plot(boost_model)
setwd("~/Documents/GitHub/Statistical_Learning")
################################################################################
######### Setting Dataset
################################################################################
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
### Transform Categorical Variables as factors
Diamonds$cut <- factor(Diamonds$cut,
levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))
Diamonds$color <- factor(Diamonds$color,
levels = c("J", "I", "H", "G", "F","E","D"))
Diamonds$clarity <- factor(Diamonds$clarity,
levels=c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"))
set.seed(2)
detect_outlier <- function(x) {
Quantile1 <- quantile(x, probs=.25)
Quantile3 <- quantile(x, probs=.75)
IQR = Quantile3 - Quantile1
x > Quantile3 + (IQR*1.5) | x < Quantile1 - (IQR*1.5)
}
remove_outlier <- function(dataframe,columns=names(dataframe)) {
for (col in columns) {
dataframe <- dataframe[!detect_outlier(dataframe[[col]]), ]
}
print("Remove outliers")
print(dataframe)
}
Diamonds <- remove_outlier(Diamonds, c('carat', 'depth_percentage', 'table', 'price',
"length", 'width', "depth"))
library(ROSE)
library(e1071)
#Setting dataset:
Diamonds$quality <- ifelse(Diamonds$cut %in% c("Very Good","Premium", "Ideal") &
Diamonds$color %in% c("G","F","E", "D") &
Diamonds$clarity %in% c("VS1","VVS2","VVS1", "IF"),
"High",
"Low")
Diamonds$quality <- factor(Diamonds$quality,levels=c("Low","High"))
View(Diamonds)
#classe sbilanciata
table(Diamonds$quality)
##Undersampling
new_n <- 3017 / 0.5
undersampling_result <- ovun.sample(quality ~ carat + depth_percentage + table
+length + width + depth+price,
data = Diamonds,
method = "under",N = new_n)
Diamonds2 <- undersampling_result$data
View(Diamonds2)
########################## SVM con kernel linear ###############################
svm_linear <- svm(quality ~ ., data = Diamonds2 ,subset = train ,
kernel = "linear",
cost = 10, scale = TRUE)
#Divido il dataset in train e test
train <- sample(nrow(Diamonds2),floor(nrow(Diamonds2)*0.7),replace = FALSE)
########################## SVM con kernel linear ###############################
svm_linear <- svm(quality ~ ., data = Diamonds2 ,subset = train ,
kernel = "linear",
cost = 10, scale = TRUE)
summary(svm_linear)
best_poly <- svm_linear <- svm(quality ~ ., data = Diamonds2 ,subset = train ,
kernel = "polynomial",
cost = 1, gamma=5,scale = TRUE)
summary(best_poly)
#Previsioni sul dataset di test
svm_poly_pred <- predict(best_poly , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_poly_pred , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_poly_pred != Diamonds2$quality[-train])
best_poly <- svm_linear <- svm(quality ~ ., data = Diamonds2 ,subset = train ,
kernel = "polynomial",
cost = 5, gamma=5,scale = TRUE)
summary(best_poly)
#Previsioni sul dataset di test
svm_poly_pred <- predict(best_poly , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_poly_pred , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_poly_pred != Diamonds2$quality[-train])
best_poly <- svm_linear <- svm(quality ~ ., data = Diamonds2 ,subset = train ,
kernel = "polynomial",
cost = 1, gamma=5,
degree=2,scale = TRUE)
summary(best_poly)
#Previsioni sul dataset di test
svm_poly_pred <- predict(best_poly , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_poly_pred , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_poly_pred != Diamonds2$quality[-train])
best_poly <- svm_linear <- svm(quality ~ ., data = Diamonds2 ,subset = train ,
kernel = "polynomial",
cost = 1, gamma=5,
degree=3,scale = TRUE)
summary(best_poly)
#Previsioni sul dataset di test
svm_poly_pred <- predict(best_poly , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_poly_pred , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_poly_pred != Diamonds2$quality[-train])
best_poly <- svm_linear <- svm(quality ~ ., data = Diamonds2 ,subset = train ,
kernel = "polynomial",
cost = 1, gamma=5,
degree=4,scale = TRUE)
summary(best_poly)
#Previsioni sul dataset di test
svm_poly_pred <- predict(best_poly , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_poly_pred , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_poly_pred != Diamonds2$quality[-train])
best_poly <- svm_linear <- svm(quality ~ ., data = Diamonds2 ,subset = train ,
kernel = "polynomial",
cost = 1, gamma=5,
degree=1,scale = TRUE)
summary(best_poly)
#Previsioni sul dataset di test
svm_poly_pred <- predict(best_poly , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_poly_pred , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_poly_pred != Diamonds2$quality[-train])
best_poly <- svm_linear <- svm(quality ~ ., data = Diamonds2 ,subset = train ,
kernel = "polynomial",
cost = 1, gamma=1,
degree=1,scale = TRUE)
summary(best_poly)
#Previsioni sul dataset di test
svm_poly_pred <- predict(best_poly , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_poly_pred , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_poly_pred != Diamonds2$quality[-train])
best_poly <- svm_linear <- svm(quality ~ ., data = Diamonds2 ,subset = train ,
kernel = "polynomial",
cost = 1, gamma=100,
degree=1,scale = TRUE)
summary(best_poly)
#Previsioni sul dataset di test
svm_poly_pred <- predict(best_poly , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_poly_pred , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_poly_pred != Diamonds2$quality[-train])
best_poly <- svm_linear <- svm(quality ~ ., data = Diamonds2 ,subset = train ,
kernel = "polynomial",
cost = 1, gamma=50,
degree=1,scale = TRUE)
summary(best_poly)
#Previsioni sul dataset di test
svm_poly_pred <- predict(best_poly , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_poly_pred , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_poly_pred != Diamonds2$quality[-train])
best_poly <- svm_linear <- svm(quality ~ ., data = Diamonds2 ,subset = train ,
kernel = "polynomial",
cost = 1, gamma=1,
degree=1,scale = TRUE)
summary(best_poly)
#Previsioni sul dataset di test
svm_poly_pred <- predict(best_poly , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_poly_pred , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_poly_pred != Diamonds2$quality[-train])
best_poly <- svm_linear <- svm(quality ~ ., data = Diamonds2 ,subset = train ,
kernel = "polynomial",
cost = 1, gamma=10,
degree=1,scale = TRUE)
summary(best_poly)
#Previsioni sul dataset di test
svm_poly_pred <- predict(best_poly , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_poly_pred , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_poly_pred != Diamonds2$quality[-train])
log_reg <- glm(quality ~ ., data = Diamonds2 ,family = binomial , subset = train)
summary(log_reg)
log_reg <- glm(quality ~ .(-length), data = Diamonds2 ,family = binomial , subset = train)
log_reg <- glm(quality ~ .+(-length), data = Diamonds2 ,family = binomial , subset = train)
summary(log_reg)
log_reg <- glm(quality ~ .-length), data = Diamonds2 ,family = binomial , subset = train)
log_reg <- glm(quality ~ .-length, data = Diamonds2 ,family = binomial , subset = train)
summary(log_reg)
log_reg <- glm(quality ~ ., data = Diamonds2 ,family = binomial , subset = train)
summary(log_reg)
log_reg <- glm(quality ~ .-length-depth, data = Diamonds2 ,family = binomial , subset = train)
summary(log_reg)
log_reg <- glm(quality ~ .-length-depth-width+(length:width:depth), data = Diamonds2 ,family = binomial , subset = train)
summary(log_reg)
log_reg <- glm(quality ~ ., data = Diamonds2 ,family = binomial , subset = train)
summary(log_reg)
?vif
??vif
?vim
??vim
log_reg <- glm(quality ~ ., data = Diamonds2 ,family = binomial , subset = train)
summary(log_reg)
install.packages(car)
"car"
install.packages("car")
library(car)
library(car)
vif(log_reg)
vif(log_reg)
library(car)
#Train model
lm_model_1 = lm(price ~ . , data = Diamonds,subset = train)
summary(lm_model_1)
vif(lm_model_1)
###### clearing environment
rm(list = ls())
graphics.off()
set.seed(2)
################################################################################
######### Setting Dataset
################################################################################
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
### Transform Categorical Variables as factors
Diamonds$cut <- factor(Diamonds$cut,
levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))
Diamonds$color <- factor(Diamonds$color,
levels = c("J", "I", "H", "G", "F","E","D"))
Diamonds$clarity <- factor(Diamonds$clarity,
levels=c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"))
set.seed(2)
detect_outlier <- function(x) {
Quantile1 <- quantile(x, probs=.25)
Quantile3 <- quantile(x, probs=.75)
IQR = Quantile3 - Quantile1
x > Quantile3 + (IQR*1.5) | x < Quantile1 - (IQR*1.5)
}
remove_outlier <- function(dataframe,columns=names(dataframe)) {
for (col in columns) {
dataframe <- dataframe[!detect_outlier(dataframe[[col]]), ]
}
print("Remove outliers")
print(dataframe)
}
Diamonds <- remove_outlier(Diamonds, c('carat', 'depth_percentage', 'table', 'price',
"length", 'width', "depth"))
#library(ggplot2)
#library(GGally)
library(corrplot)
par(mfrow = c(1,1))
cor_scores <- cor(subset(Diamonds , select = -c(color,clarity,cut)))
corrplot(cor_scores,method = "number")
#train and test indexes
train <- sample(nrow(Diamonds),floor(nrow(Diamonds)*0.7),replace = FALSE)
price_test <- Diamonds$price[-train]
################################################################################
######### Linear Regression
################################################################################
library(lmtest)
library(car)
#The contrasts() function returns the coding that R uses for the dummy variables.
contrasts(Diamonds$cut)
#Train model
lm_model_1 = lm(price ~ . , data = Diamonds,subset = train)
summary(lm_model_1)
vif(lm_model_1)
Diamonds2$volume <- Diamonds2$length * Diamonds2$width * Diamonds2$depth
Diamonds$volume <- Diamonds$length * Diamonds$width * Diamonds$depth
# Rimuovere le variabili originali
Diamonds <- Diamonds[, !names(Diamonds) %in% c("length", "width", "depth")]
View(Diamonds)
lm_model_1 = lm(price ~ . , data = Diamonds,subset = train)
summary(lm_model_1)
vif(lm_model_1)
cor_scores <- cor(subset(Diamonds , select = -c(color,clarity,cut)))
corrplot(cor_scores,method = "number")
lm_model_1 = lm(price ~ . , data = Diamonds,subset = train)
summary(lm_model_1)
vif(lm_model_1)
summary(lm_model_1)
#Train RMSE
lm_train_RMSE = sqrt(mean((Diamonds$price[train] - lm_model_1$fitted.values)^2))
lm_train_RMSE
#Train RMSE
lm_train_RMSE = sqrt(mean((lm_model_1$residuals)^2))
lm_train_RMSE
#test RMSE
pred_err = (Diamonds$price- predict(lm_model_1,Diamonds))^2
#using predict() function
y_hat_lm = predict(lm_model_1,newdata = Diamonds[-train,])
lm_test_RMSE_1 = sqrt(mean((y_hat_lm - Diamonds$price[-train])^2))
lm_test_RMSE_1
lm_model_1 = lm(price ~ . , data = Diamonds,subset = train)
summary(lm_model_1)
#R^2
summary(lm_model_1)$r.sq
#test RMSE
pred_err = (Diamonds$price- predict(lm_model_1,Diamonds))^2
train_rmse = sqrt(mean(pred_err[train]))
test_rmse = sqrt(mean(pred_err[-train]))
#using predict() function
y_hat_lm = predict(lm_model_1,newdata = Diamonds[-train,])
lm_test_RMSE_1 = sqrt(mean((y_hat_lm - Diamonds$price[-train])^2))
lm_test_RMSE_1
lm_model_1 = lm(price ~ . , data = Diamonds,subset = train)
summary(lm_model_1)
plot(Diamonds$volume, Diamonds$price,
main = "Volume vs Price",
xlab = "Volume (mm^3)",
ylab = "Price ($)")
###### Analisi dei Residui ######
par(mfrow = c(2,2))
#Grafici diagnostici
plot(lm_model_1)
library(gam)
####### GAM train and test #######
gam_model_1 <- gam(price~ s(carat) + cut + color + clarity +
s(depth_percentage) + s(table) + s(volume),
data=Diamonds[train, ])
par(mfrow = c(1,1))
plot(gam_model_1,se=TRUE)
#Test RMSE
gam_model_1_RMSE = sqrt(mean((Diamonds$price[-train] - gam_pred_value)^2))
##### Analisi dei residui #####
gam_pred_value <- predict(gam_model_1,newdata = Diamonds[-train,])
gam_model_1_residuals = Diamonds$price[-train] - gam_pred_value
#Test Predicted values vs Residuals
plot(gam_pred_value,gam_model_1_residuals,xlab = "Predicted values",
ylab = "Residuals",
main = "Predicted values vs Residuals",
cex = 1, col = "black")
abline(a=0,b=0,lwd=1.5,col="red")
#Residuals
plot(gam_model_1_residuals)
#Test RMSE
gam_model_1_RMSE = sqrt(mean((Diamonds$price[-train] - gam_pred_value)^2))
gam_model_1_RMSE #Miglioramento significativo
####### GAM train and test #######
gam_model_1 <- gam(price~ s(carat,4) + cut + color + clarity +
s(depth_percentage,5) + s(table,2) + s(volume,4),
data=Diamonds[train, ])
par(mfrow = c(1,1))
plot(gam_model_1,se=TRUE)
##### Analisi dei residui #####
gam_pred_value <- predict(gam_model_1,newdata = Diamonds[-train,])
#Test RMSE
gam_model_1_RMSE = sqrt(mean((Diamonds$price[-train] - gam_pred_value)^2))
gam_model_1_RMSE #Miglioramento significativo
####### GAM train and test #######
gam_model_1 <- gam(price~ s(carat,4) + cut + color + clarity +
s(depth_percentage,5) + s(table,2) + s(volume,4),
data=Diamonds[train, ])
##### Analisi dei residui #####
gam_pred_value <- predict(gam_model_1,newdata = Diamonds[-train,])
gam_model_1_residuals = Diamonds$price[-train] - gam_pred_value
#Test RMSE
gam_model_1_RMSE = sqrt(mean((Diamonds$price[-train] - gam_pred_value)^2))
gam_model_1_RMSE #Miglioramento significativo
plot(gam_model_1,se=TRUE)
####### GAM train and test #######
gam_model_1 <- gam(price~ s(carat,4) + cut + color + clarity +
s(depth_percentage,5) + s(table,3) + s(volume,4),
data=Diamonds[train, ])
plot(gam_model_1,se=TRUE)
####### GAM train and test #######
gam_model_1 <- gam(price~ s(carat,4) + cut + color + clarity +
s(depth_percentage,5) + s(table,2) + s(volume,4),
data=Diamonds[train, ])
plot(gam_model_1,se=TRUE)
####### GAM train and test #######
gam_model_1 <- gam(price~ s(carat,4) + cut + color + clarity +
s(depth_percentage,5) + s(table,4) + s(volume,4),
data=Diamonds[train, ])
plot(gam_model_1,se=TRUE)
vif(lm_model_1)
#nuova matrice di correlazione
cor_scores <- cor(subset(Diamonds , select = -c(color,clarity,cut)))
corrplot(cor_scores,method = "number")
