View(Boston)
head(Boston)
?Boston
dim(Boston)
### Fit Linear Model: mdev = b0 + b1*lstat + e
lm_fit <- lm(medv ~ lstat, data = Boston)
# View fitted results
lm_fit
summary(lm_fit)
lm_fit$coefficients
# Compute confident interval (CI) on coefficient
?confint
confint(lm_fit, level = 0.98)
# Get prediction with CI over new observation
predict(lm_fit, data.frame(lstat = (c(5, 10, 15))),
interval = "confidence")
# Plot linear regression results
plot(Boston$lstat, Boston$medv,pch = "+", ylab = "medv - median value of owner-occupied homes in $1000s",
xlab = "lstat - ower status of the population (percent).",
main = "Linear regression")
legend("topright",legend=c("Sampling point"),pch = "+")
# Add predicted value
abline(lm_fit, lwd = 3, col = "red")
#  Create Subplot region
par(mfrow = c(2, 2))
plot(lm_fit)
# Put plotting arrangement back to its original state
par(mfrow = c(1, 1))
plot(hatvalues(lm_fit), col="blue",
ylab = "leverage",
xlab = "Observation",
main = "leverage") # 1/n < leverage < 1
# y =b0 + b1*lstat + b2*age + e
lm_fit <- lm(medv ~ lstat + age, data = Boston)
summary(lm_fit)
# y = b*X + e (perform a regression using all of the predictors)
lm_fit <- lm(medv ~ ., data = Boston)
s <- summary(lm_fit)
View(s)
s <- summary(lm_fit)
s
# update regression, remove age predictor (large p-value)
lm_fit <- update(lm_fit, ~ . - age)
summary(lm_fit)
## Interaction Terms & Non-linear Transformations of the Predictors
summary(lm(medv ~ lstat + age + lstat:age, data = Boston))
summary(lm(medv ~ lstat + I(lstat^2), data = Boston))
# plot residual
lm_fit <- lm(medv ~ lstat + I(lstat^2) + log(rm), data = Boston);
par(mfrow = c(1,2))
summary(lm_fit)
# plot 1
plot(lm_fit$residuals, pch = "o", col = "blue" ,
ylab = "Residual", main = paste0("Residual plot - mean:",round(mean(lm_fit$residuals),digits = 4),
"- var:", round(var(lm_fit$residuals),digits = 2)))
abline(c(0,0),c(0,length(lm_fit$residuals)), col= "red", lwd = 2)
# plot 2
hist(lm_fit$residuals,40,
xlab = "Residual",
main = "Empirical residual distribution")
## Qualitative Predictors
head(Carseats)
View(Carseats)
# Given a qualitative variable such as Shelveloc , R generates dummy variables
# automatically in this way:
contrasts(Carseats$ShelveLoc)
# Regression
lm_fit <- lm(Sales ~ . + Income:Advertising + Price:Age,
data = Carseats)
summary(lm_fit)
View(Auto)
dim(Auto)
?Auto
# Validation Method
?sample
set.seed(2)
train <- sample (392 , 196, replace = FALSE)
train
err = (Auto$mpg - predict(lm_fit, Auto ))^2
lm_fit <- lm(mpg ~ horsepower , data = Auto , subset = train)#stimo il modello solo da indici vettore train
err = (Auto$mpg - predict(lm_fit, Auto ))^2
trai_err = mean(err[train])
test_err = mean(err[-train])
# quadratic fit
lm_fit_quad <- lm ( mpg ~ poly ( horsepower , 2, raw = TRUE) , data = Auto ,
subset = train )
test_quad = mean(( Auto$mpg - predict (lm_fit_quad,Auto)) [-train ]^2)
# cubic fit
lm_fit_cubic <- lm( mpg ~ poly ( horsepower, 3, raw = TRUE) , data = Auto ,
subset = train )
test_cubic = mean(( Auto$mpg - predict (lm_fit_cubic,Auto)) [-train ]^2)
lm_fit_4 <- lm( mpg ~ poly ( horsepower, 4, raw = TRUE) , data = Auto ,
subset = train )
test_4 = mean(( Auto$mpg - predict (lm_fit_cubic,Auto)) [-train ]^2)
lm_fit_5 <- lm( mpg ~ poly ( horsepower, 5, raw = TRUE) , data = Auto ,
subset = train )
test_5 = mean(( Auto$mpg - predict (lm_fit_cubic,Auto)) [-train ]^2)
par(mfrow = c(1, 1))
plot(1:5,c(test_err,test_quad,test_cubic,test_4,test_5), type = "b", col="blue",
ylab = "Test MSE",
xlab = "Flexibility",
main = "Validation Test MSE")
# Leave-one-out cross-validation (LOOCV) method
library(boot)
#1-Basic Commands
#Function c(), create a vector of numbers, ?funcName() ottieni info
#sulla funzione
x = (1,3,2,5);
#1-Basic Commands
#Function c(), create a vector of numbers, ?funcName() ottieni info
#sulla funzione
x = c(1,3,2,5)
y <- c(1,4,3)
?c
length(x)
lenght(y)
length(y)
x+y
y = c(1,4,3,7)
x+y
#The ls() function allows us to look at a list of all of the objects,
#such as data and functions, that we have saved so far. The
#function rm() can be #used to delete any that we don’t want.
ls()
#Insurance
Insurance <- read.table("insurance.csv", header = TRUE, sep = ",",quote = "\"", fileEncoding = "UTF-8")
#The tree library is used to construct classification and regression trees.
library(tree)
library(ISLR2)
attach(Carseats)
View(Carseats)
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
View(Carseats)
#Finally, we use the data.frame() function to merge High with the rest of
#the Carseats data.
Carseats <- data.frame(Carseats , High)
#We now use the tree() function to fit a classification tree in order to predict
#High using all variables but Sales. The syntax of the tree() function is quite
#similar to that of the lm() function.
tree.carseats <- tree(High ~ . - Sales, Carseats)
#The summary() function lists the variables that are used as internal nodes
#in the tree, the number of terminal nodes, and the (training) error rate.
summary(tree.carseats)
#One of the most attractive properties of trees is that they can be
#graphically displayed. We use the plot() function to display the tree structure,
#and the text() function to display the node labels. The argument
#pretty = 0 instructs R to include the category names for any qualitative
#predictors, rather than simply displaying a letter for each category.
plot(tree.carseats)
text(tree.carseats , pretty = 0)
tree.carseats
#If we just type the name of the tree object, R prints output corresponding
#to each branch of the tree. R displays the split criterion (e.g. Price < 92.5),
#the number of observations in that branch, the deviance, the overall prediction
#for the branch (Yes or No), and the fraction of observations in that
#branch that take on values of Yes and No. Branches that lead to terminal
#nodes are indicated using asterisks.
tree.carseats
#In the case of a classification tree, the argument type = "class" instructs R
#to return the actual class prediction. This approach leads to correct predictions
#for around 77% of the locations in the test data set.
set.seed(2)
train <- sample(1:nrow(Carseats), 200)
Carseats.test <- Carseats[-train, ]
View(Carseats.test)
High.test <- High[-train] #Osservazioni di test di High
tree.carseats <- tree(High ~ . - Sales, Carseats, subset = train)
tree.pred <- predict(tree.carseats , Carseats.test , type = "class")
table(tree.pred , High.test)
#The cv.tree() function reports the number of terminal nodes of each tree considered
#(size) as well as the corresponding error rate and the value of the
#cost-complexity parameter used (k, which corresponds to aplha ) in (8.4)).
set.seed(7)
cv.carseats <- cv.tree(tree.carseats , FUN = prune.misclass)
names(cv.carseats)
cv.carseats
#We plot the error rate as a function of both size and k.
par(mfrow = c(1, 2))
plot(cv.carseats$size , cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")
plot(cv.carseats$size , cv.carseats$dev, type = "b")
#We plot the error rate as a function of both size and k.
par(mfrow = c(1, 1))
plot(cv.carseats$size , cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")
#We now apply the prune.misclass() function in order to prune the tree to obtain
#the nine-node tree.
prune.carseats <- prune.misclass(tree.carseats , best = 9)
plot(prune.carseats)
text(prune.carseats , pretty = 0)
##### Test Error on the best sub-tree #####
tree.pred <- predict(prune.carseats , Carseats.test ,type = "class")
table(tree.pred , High.test)
(97 + 58) / 200
#If we increase the value of best, we obtain a larger pruned tree with lower
#classification accuracy:
prune.carseats <- prune.misclass(tree.carseats , best = 14)
plot(prune.carseats)
text(prune.carseats , pretty = 0)
tree.pred <- predict(prune.carseats , Carseats.test ,
type = "class")
table(tree.pred , High.test)
(102 + 52) / 200
library(tree)
library ( ISLR2 )
View(Carseats)
Carseats$High <- factor( ifelse(Carseats$Sales <= 8 , " No " , " Yes " ))
View(Carseats)
# train model
tree_model <- tree( High ~ . - Sales , Carseats, split = "gini")
# show result
summary(tree_model)
plot(tree_model)
text(tree_model,pretty = 0)
# Valuate performance with train and test data sets
set.seed(2)
train <- sample(1:nrow(Carseats),floor(nrow(Carseats)*0.5))
tree_model <- tree( High ~ . - Sales , Carseats, subset = train)
summary(tree_model)
# pred_value <- predict(tree_model, newdata = Carseats[-train,])
pred_value <- predict(tree_model, newdata = Carseats[-train,],type = "class")
table(pred_value,Carseats$High[-train])
# Cross validation
set.seed(2)
tree_cv <- cv.tree(tree_model, FUN = prune.misclass)
# for regression FUN = prune.tree
tree_cv
plot(tree_cv$size, tree_cv$dev)
best = min(tree_cv$size[tree_cv$dev == min(tree_cv$dev)])
best
#prendo la k con errore minore
k = min(tree_cv$k[tree_cv$dev == min(tree_cv$dev)]) #alpha in the book
k
prune <- prune.misclass(tree_model, best = best)
# prune <- prune.misclass(tree_model, k = k)
summary(prune)
plot(prune)
text(prune, pretty = 0)
pred_value <- predict(prune, newdata = Carseats[-train,],type = "class")
table(pred_value,Carseats$High[-train])
#we fit a regression tree to the Boston data set. First, we create a
#training set, and fit the tree to the training data
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston) / 2)
View(Boston)
tree.boston <- tree(medv ~ ., Boston , subset = train)
summary(tree.boston)
#Notice that the output of summary() indicates that only four of the variables
#have been used in constructing the tree. In the context of a regression tree,
#the deviance is simply the sum of squared errors for the tree.
plot(tree.boston)
text(tree.boston , pretty = 0)
#Now we use the cv.tree() function to see whether pruning the tree will
#improve performance.
cv.boston <- cv.tree(tree.boston)
cv.boston
plot(cv.boston$size , cv.boston$dev, type = "b")
plot(cv.boston$k , cv.boston$dev, type = "b")
plot(cv.boston$size , cv.boston$dev, type = "b")
summary(tree.boston)
#In this case, the most complex tree under consideration is selected by crossvalidation.
#However, if we wish to prune the tree, we could do so as follows,
#using the prune.tree() function:
prune.boston <- prune.tree(tree.boston , best = 5)
plot(prune.boston)
text(prune.boston , pretty = 0)
#In keeping with the cross-validation results, we use the unpruned tree to
#make predictions on the test set.
yhat <- predict(tree.boston , newdata = Boston[-train , ])
boston.test <- Boston[-train, "medv"]
plot(yhat , boston.test) #Previsioni vs dati reali
abline(0, 1)
mean((yhat - boston.test)^2) #Test MSE
#Recall that bagging is simply a special case of a random forest with m = p.
#Therefore, the randomForest() function can be used to perform both random forests and bagging
#We perform bagging as follows:
library(randomForest)
#Recall that bagging is simply a special case of a random forest with m = p.
#Therefore, the randomForest() function can be used to perform both random forests and bagging
#We perform bagging as follows:
library(randomForest)
set.seed(1)
bag.boston
bag.boston <- randomForest(medv ~ ., data = Boston , subset = train,
mtry = 12, importance = TRUE)
bag.boston
#The argument mtry = 12 indicates that all 12 predictors should be considered
#for each split of the tree—in other words, that bagging should be done.
#l parametro importance = TRUE nella funzione randomForest() indica che si desidera
#calcolare l'importanza delle variabili nel modello di random forest.
importance(bag.boston)
train <- sample(1:nrow(Boston), nrow(Boston) / 2)
bag.boston <- randomForest(medv ~ ., data = Boston , subset = train,
mtry = 12, importance = TRUE)
bag.boston
#The argument mtry = 12 indicates that all 12 predictors should be considered
#for each split of the tree—in other words, that bagging should be done.
#l parametro importance = TRUE nella funzione randomForest() indica che si desidera
#calcolare l'importanza delle variabili nel modello di random forest.
importance(bag.boston)
####### Test MSE Bagging #####
yhat.bag <- predict(bag.boston , newdata = Boston[-train , ])
plot(yhat.bag , boston.test)
abline(0, 1)
mean((yhat.bag - boston.test)^2)
#Recall that bagging is simply a special case of a random forest with m = p.
#Therefore, the randomForest() function can be used to perform both random forests and bagging
#We perform bagging as follows:
library(randomForest)
set.seed(1)
bag.boston <- randomForest(medv ~ ., data = Boston , subset = train,
mtry = 12, importance = TRUE)
train <- sample(1:nrow(Boston), nrow(Boston) / 2)
bag.boston <- randomForest(medv ~ ., data = Boston , subset = train,
mtry = 12, importance = TRUE)
####### Test MSE Bagging #####
yhat.bag <- predict(bag.boston , newdata = Boston[-train , ])
plot(yhat.bag , boston.test)
boston.test <- Boston[-train, "medv"] #Osservazioni reali
plot(yhat.bag , boston.test)
abline(0, 1)
mean((yhat.bag - boston.test)^2)
#We could change the number of trees grown by randomForest() using the
#ntree argument:
bag.boston <- randomForest(medv ! ., data = Boston , subset = train,
#We could change the number of trees grown by randomForest() using the
#ntree argument:
bag.boston <- randomForest(medv ~ ., data = Boston , subset = train,
mtry = 12, ntree = 25)
yhat.bag <- predict(bag.boston , newdata = Boston[-train , ])
mean((yhat.bag - boston.test)^2)
#Growing a random forest proceeds in exactly the same way, except that
#we use a smaller value of the mtry argument. By default, randomForest()
#uses p/3 variables when building a random forest of regression trees, and
#sqrt(p) variables when building a random forest of classification trees. Here we
#use mtry = 6.
set.seed(1)
rf.boston <- randomForest(medv ~ ., data = Boston ,
subset = train , mtry = 6, importance = TRUE)
yhat.rf <- predict(rf.boston, newdata = Boston[-train, ])
mean((yhat.rf - boston.test)^2)
#Using the importance() function, we can view the importance of each variable.
importance(rf.boston)
#Plots of these importance measures can be produced using the varImpPlot() function.
varImpPlot(rf.boston)
#The argument n.trees = 5000 indicates that we want 5000 trees, and the
#option interaction.depth = 4 limits the depth of each tree.
library(gbm)
set.seed(1)
boost.boston <- gbm(medv ~ ., data = Boston[train , ],
distribution = "gaussian", n.trees = 5000,
interaction.depth = 4)
summary(boost.boston)
#We can also produce partial dependence plots for these two variables. These plots
#illustrate the marginal effect of the selected variables on the response after
#integrating out the other variables. In this case, as we might expect, median
#house prices are increasing with rm and decreasing with lstat.
plot(boost.boston , i = "rm")
plot(boost.boston , i = "lstat")
##### Test MSE ####
#We now use the boosted model to predict medv on the test set:
yhat.boost <- predict(boost.boston , newdata = Boston[-train , ], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
#If we want to, we can perform boosting with a different
#value of the shrinkage parameter lambda in (8.10). The default value is 0.001,
#but this is easily modified. Here we take lambda = 0.2.
boost.boston <- gbm(medv ~ ., data = Boston[train , ],
distribution = "gaussian", n.trees = 5000,
interaction.depth = 4, shrinkage = 0.2, verbose = F)
yhat.boost <- predict(boost.boston ,newdata = Boston[-train , ], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
# Bagging
library ( randomForest )
set.seed(1)
library ( ISLR2 )
#mtry = Number of variables randomly sampled as candidates at each split.
#ntree = Number of trees to grow.
bagg_model <- randomForest(medv ~ . ,data = Boston, subset = train,
mtry = ncol(Boston)-1, importance = TRUE,replace = TRUE)
train <- sample(1:nrow(Boston),floor(nrow(Boston)*0.5))
#mtry = Number of variables randomly sampled as candidates at each split.
#ntree = Number of trees to grow.
bagg_model <- randomForest(medv ~ . ,data = Boston, subset = train,
mtry = ncol(Boston)-1, importance = TRUE,replace = TRUE)
bagg_model
#mtry = Number of variables randomly sampled as candidates at each split.
#ntree = Number of trees to grow.
bagg_model <- randomForest(medv ~ . ,data = Boston, subset = train,
mtry = ncol(Boston)-1,
importance = TRUE,
replace = TRUE,
ntree = 100)
bagg_model
#mtry = Number of variables randomly sampled as candidates at each split.
#ntree = Number of trees to grow.
bagg_model <- randomForest(medv ~ . ,data = Boston, subset = train,
mtry = ncol(Boston)-1,
importance = TRUE,
replace = TRUE,
ntree = 500)
bagg_model
plot(bagg_model)
yhat <- predict(bagg_model, newdata = Boston[-train,])
plot(yhat,Boston$medv[-train])
abline(0,1)
mse <- mean((yhat - Boston$medv[-train])^2)
mse
# how to change number of tree?
# add ntree oprion
bagg_model <- randomForest(medv ~ . ,data = Boston, subset = train,
mtry = ncol(Boston)-1, importance = TRUE, ntree = 100)
bagg_model
plot(bagg_model)
importance(bagg_model)
# Random Forest
# default p = sqrt(m) -> see doc
forest_model <- randomForest(medv ~ . ,data = Boston, subset = train,
mtry = floor(sqrt(ncol(Boston)-1)), importance = TRUE, ntree = 100)
forest_model
# if the object has a non-null test component, then the returned object is a
# matrix where the first column is the out-of-bag estimate of error,
# and the second column is for the test set.
plot(forest_model,type = 'b',col="green",pch = "+")
par(new=TRUE)
plot(bagg_model,type = 'b',col="red",pch='o')
yhat <- predict(forest_model, newdata = Boston[-train,])
mse <- mean((yhat - Boston$medv[-train])^2)
mse
importance(forest_model)
# Boosting
library ( gbm )
set.seed(1)
ntree = 5000;
boost_model
boost_model <- gbm(medv ~ . , data = Boston[train,],
distribution = "gaussian" , n.trees = ntree,
interaction.depth = 4, shrinkage = 0.01 , verbose = F)
boost_model
yhat <- predict(boost_model, newdata = Boston[-train,], n.trees = ntree)
mse <- mean((yhat - Boston$medv[-train])^2)
mse
plot(boost.boston)
plot(boost_model)
################################################################################
######### Setting Dataset
################################################################################
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
setwd("~/Documents/GitHub/Statistical_Learning")
################################################################################
######### Setting Dataset
################################################################################
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
### Transform Categorical Variables as factors
Diamonds$cut <- factor(Diamonds$cut,
levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))
Diamonds$color <- factor(Diamonds$color,
levels = c("J", "I", "H", "G", "F","E","D"))
Diamonds$clarity <- factor(Diamonds$clarity,
levels=c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"))
detect_outlier <- function(x) {
Quantile1 <- quantile(x, probs=.25)
Quantile3 <- quantile(x, probs=.75)
IQR = Quantile3 - Quantile1
x > Quantile3 + (IQR*1.5) | x < Quantile1 - (IQR*1.5)
}
remove_outlier <- function(dataframe,columns=names(dataframe)) {
for (col in columns) {
dataframe <- dataframe[!detect_outlier(dataframe[[col]]), ]
}
print("Remove outliers")
print(dataframe)
}
Diamonds <- remove_outlier(Diamonds, c('carat', 'depth_percentage', 'table', 'price',
"length", 'width', "depth"))
library(gam)
#train and test indexes
train <- sample(nrow(Diamonds),floor(nrow(Diamonds)*0.7),replace = FALSE)
price_test <- Diamonds$price[-train]
gam_model_1 <- gam(price~ s(carat,4) + cut + color + clarity +
s(depth_percentage,5) + s(table,2) + s(length,4)
,data=Diamonds[train, ])
par(mfrow = c(1,1))
plot(gam_model_1,se=TRUE)
#Train model
lm_model_1 = lm(price ~ . -width-depth , data = Diamonds,subset = train)
summary(lm_model_1)
#Train model
lm_model_1 = lm(price ~ .-depth , data = Diamonds,subset = train)
summary(lm_model_1)
#Train model
lm_model_1 = lm(price ~ . , data = Diamonds,subset = train)
summary(lm_model_1)
gam_model_1 <- gam(price~ s(carat,4) + cut + color + clarity +
s(depth_percentage,5) + s(table,2) + s(depth,4)
,data=Diamonds[train, ])
plot(gam_model_1,se=TRUE)
gam_model_1 <- gam(price~ s(carat,4) + cut + color + clarity +
s(depth_percentage,5) + s(table,2) + s(length,4)
,data=Diamonds[train, ])
plot(gam_model_1,se=TRUE)
####### GAM train and test #######
gam_model_1 <- gam(price~ s(carat,4) + cut + color + clarity +
s(depth_percentage,5) + s(table,2) + s(length,4) +
s(width,4) + s(depth,4),data=Diamonds[train, ])
par(mfrow = c(1,1))
plot(gam_model_1,se=TRUE)
