#In this lab, we re-analyze the Wage data
library(ISLR2)
attach(Wage)
View(Wage)
gam1 <- lm(wage ~ ns(year , 4) + ns(age , 5) + education , data = Wage)
#Fitting wage to age using a regression spline is simple
library(splines)
gam1 <- lm(wage ~ ns(year , 4) + ns(age , 5) + education , data = Wage)
#All of the terms in (7.16) are fit simultaneously, taking each other into account
#to explain the response.
library(gam)
#All of the terms in (7.16) are fit simultaneously, taking each other into account
#to explain the response.
library(gam)
gam.m3 <- gam(wage ~ s(year , 4) + s(age , 5) + education , data = Wage)
#The generic plot() function recognizes that gam.m3 is an object of class Gam,
#and invokes the appropriate plot.Gam() method.
#Conveniently, even though plot.Gam() gam1 is not of class Gam but rather of class lm,
#we can still use plot.Gam() on it.
par(mfrow = c(1, 3))
plot(gam.m3, se = TRUE, col = "blue")
#Conveniently, even though plot.Gam() gam1 is not of class Gam but rather of class lm,
#we can still use plot.Gam() on it.
plot.Gam(gam1 , se = TRUE, col = "red")
plot(gam.m3, se = TRUE, col = "blue")
plot.Gam()
?plot.Gam()
#Grafico con bande di confidenza
plot(gam.m3, se = TRUE, col = "blue")
#Exclude year
gam.m1 <- gam(wage ~ s(age , 5) + education , data = Wage)
#Linear year
gam.m2 <- gam(wage ~ year + s(age , 5) + education ,data = Wage)
#smooth spline year
gam.m3 <- gam(wage ~ s(year , 4) + s(age , 5) + education , data = Wage)
anova(gam.m1, gam.m2, gam.m3, test = "F")
#la funzione predict() utilizza il modello gam.m2 per fare previsioni sui dati
#contenuti nel set Wage, producendo un vettore di valori stimati che rappresentano
#le previsioni del modello per questi dati.
preds <- predict(gam.m2, newdata = Wage)
gam_model <- gam(wage ~ s(year,4) + s(age,5) + education, data = Wage[train,]);
# s() -> smoothing spline
train <- sample(dim(Wage)[1], round(dim(Wage)[1]*0.7))
gam_model <- gam(wage ~ s(year,4) + s(age,5) + education, data = Wage[train,]);
plot(gam_model,se=TRUE)
pred_value <- predict(gam_model,newdata = Wage[-train,])
plot(Wage$wage[-train],pred_value)
#### Local Regression and interaction terms ####
#We can also use local regression fits as building blocks in a GAM, using
#the lo() function.
gam.lo <- gam( wage ~ s(year , df = 4) + lo(age , span = 0.7) + education ,
data = Wage)
plot(gam.lo, se = TRUE, col = "green")
par(mfrow = c(1, 3))
par(mfrow = c(1, 1))
plot(gam.lo, se = TRUE, col = "green")
################################################################################
################Example of GAM model in more complex Datasets
################################################################################
require(dplyr)
################################################################################
################Example of GAM model in more complex Datasets
################################################################################
require(dplyr)
rm(list = ls()) # clear all environment variable
graphics.off()  # close all plot
setwd("~/Desktop/Materiale UniversitaÌ€/Laurea Magistrale/II Anno/Statistical Learning/esercizi_extra_R/4-non_linear_modeling")
Data <- read.csv("hour.csv");
Data <- read.csv("hour.csv");
View(Data)
Data$dteday <- NULL
Data$instant <- NULL
Data$registered <- NULL;
Data$casual <- NULL;
# Convert integer in Categorical
## yr -> step function
#Data$mnth <- as.factor(Data$mnth);
## hr -> natural spline
Data$yr <- as.factor(Data$yr)
Data$holiday <- as.factor(Data$holiday)
Data$weekday <- as.factor(Data$weekday)
Data$workingday <- as.factor(Data$workingday)
Data$weathersit <- as.factor(Data$weathersit)
# simple descriptive statistics (grpstat)
str(Data);
boxplot(Data$temp)
barplot(tabulate(Data$weekday));
#
model_gam <- gam(cnt~ yr + cut(mnth,breaks=c(1,4,8,12)) + ns(hr,4) + holiday +
weekday + workingday + weathersit +s(temp,5) + ns(hum,4) + lo(windspeed,span =0.4), data=Data);
plot(model_gam, se = TRUE)
################################################################################
######### Modify Dataset
################################################################################
set.seed(24)
setwd("~/Documents/GitHub/Statistical_Learning")
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
Diamonds <- subset(Diamonds , select = - X)
colnames(Diamonds)[5] = "depth_percentage"
colnames(Diamonds)[8] = "length"
colnames(Diamonds)[9] = "width"
colnames(Diamonds)[10] = "depth"
################################################################################
######### Setting Dataset
################################################################################
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
### Transform Categorical Variables as factors
Diamonds$cut <- factor(Diamonds$cut,
levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))
Diamonds$color <- factor(Diamonds$color,
levels = c("J", "I", "H", "G", "F","E","D"))
Diamonds$clarity <- factor(Diamonds$clarity,
levels=c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"))
# no nan colums
colSums(is.na(Diamonds))
View(Diamonds)
detect_outlier <- function(x) {
Quantile1 <- quantile(x, probs=.25)
Quantile3 <- quantile(x, probs=.75)
IQR = Quantile3 - Quantile1
x > Quantile3 + (IQR*1.5) | x < Quantile1 - (IQR*1.5)
}
remove_outlier <- function(dataframe,columns=names(dataframe)) {
for (col in columns) {
dataframe <- dataframe[!detect_outlier(dataframe[[col]]), ]
}
print("Remove outliers")
print(dataframe)
}
Diamonds <- remove_outlier(Diamonds, c('carat', 'depth_percentage', 'table', 'price',
"length", 'width', "depth"))
#train and test indexes
train <- sample(nrow(Diamonds),floor(nrow(Diamonds)*0.7),replace = FALSE)
gam_model_1 <- gam(price~ ns(carat,4) + cut + color + clarity +
s(depth_percentage,5) + s(table,5) + ns(length,4) +
ns(width,4) +ns(depth,4), data=Diamonds)
plot(model_gam, se = TRUE)
plot(gam_model_1, se = TRUE)
summary(gam_model_1)
MSE_gam_model_1 = mean((gam_model_1$residuals)^2)
MSE_gam_model_1 <- mean((gam_model_1$residuals)^2)
gam_model_1$
uu
jkjkjk
MSE_gam_model_1 <- mean((gam_model_1$residuals)^2)
MSE_gam_model_1
summary(gam_model_1)$r.sq
lm_fit = lm(price ~ . , data = Diamonds)
#MSE
mean((Diamonds$price - lm_fit$fitted.values)^2)
MSE_gam_model_1
plot(Diamonds$price,gam_model_1$fitted.values)
plot(gam_model_1$residuals)
plot(gam_model_1$fitted.values,gam_model_1$residuals)
plot(Diamonds$price,gam_model_1$residuals)
####### GAM train and test #######
gam_model_train <- gam(price~ ns(carat,4) + cut + color + clarity +
s(depth_percentage,5) + s(table,5) + ns(length,4) +
ns(width,4) +ns(depth,4), data=Diamonds[train])
####### GAM train and test #######
gam_model_train <- gam(price~ ns(carat,4) + cut + color + clarity +
s(depth_percentage,5) + s(table,5) + ns(length,4) +
ns(width,4) +ns(depth,4), data=Diamonds[train, ])
plot(gam_model_train,SE="TRUE")
plot(gam_model_train,se="TRUE")
plot(gam_model_train,se="TRUE")
plot(gam_model_train,se=TRUE)
gam_pred_value <- predict(gam_model_train,newdata = Diamonds[-train,])
plot(Diamonds$price[-train],pred_value)
gam_pred_value <- predict(gam_model_train,newdata = Diamonds[-train,])
plot(Diamonds$price[-train],gam_pred_value)
plot(Diamonds$price,gam_model_1$fitted.values)
plot(Diamonds$price[-train],gam_pred_value)
mean((Diamonds$price[-train] - gam_pred_value)^2)
plot(Diamonds$price[-train] - gam_pred_value)
plot(Diamonds$price[-train],gam_pred_value)
plot(Diamonds$price[-train] - gam_pred_value)
plot(gam_model_1$residuals)
plot(gam_model_1$residuals[1:100])
plot(gam_model_1$residuals[1:150])
plot(gam_model_1$residuals)
