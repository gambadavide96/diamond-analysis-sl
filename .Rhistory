################################################################################
######### Setting Dataset
################################################################################
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
setwd("~/Documents/GitHub/Statistical_Learning")
################################################################################
######### Setting Dataset
################################################################################
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
### Transform Categorical Variables as factors
Diamonds$cut <- factor(Diamonds$cut,
levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))
Diamonds$color <- factor(Diamonds$color,
levels = c("J", "I", "H", "G", "F","E","D"))
Diamonds$clarity <- factor(Diamonds$clarity,
levels=c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"))
detect_outlier <- function(x) {
Quantile1 <- quantile(x, probs=.25)
Quantile3 <- quantile(x, probs=.75)
IQR = Quantile3 - Quantile1
x > Quantile3 + (IQR*1.5) | x < Quantile1 - (IQR*1.5)
}
remove_outlier <- function(dataframe,columns=names(dataframe)) {
for (col in columns) {
dataframe <- dataframe[!detect_outlier(dataframe[[col]]), ]
}
print("Remove outliers")
print(dataframe)
}
Diamonds <- remove_outlier(Diamonds, c('carat', 'depth_percentage', 'table', 'price',
"length", 'width', "depth"))
#train and test indexes
train <- sample(nrow(Diamonds),floor(nrow(Diamonds)*0.7),replace = FALSE)
price_test <- Diamonds$price[-train]
set.seed(2)
set.seed(2)
#train and test indexes
train <- sample(nrow(Diamonds),floor(nrow(Diamonds)*0.7),replace = FALSE)
price_test <- Diamonds$price[-train]
################################################################################
############################### LDA
################################################################################
lda_fit <- lda( color ~ . ,data = Diamonds, subset = train)
library(MASS);
################################################################################
############################### LDA
################################################################################
lda_fit <- lda( color ~ . ,data = Diamonds, subset = train)
################################################################################
############################### LDA
################################################################################
library(MASS);
lda_fit <- lda( color ~ . ,data = Diamonds, subset = train)
lda_fit
lda_predict <- predict(lda_fit, newdata = Diamonds[-train,])
lda_predict
View(lda_predict)
View(lda_predict$class)
lda_predict$class
table(lda_predict$class,Diamonds$color[train])
table(lda_predict$class,Diamonds$color[-train])
mean(lda_predict$class != Diamonds$color[-train])
################################################################################
################################################################################
#Classification problem: Predirre correttamente la quality di un
#diamante
Diamonds$quality <- ifelse(Diamonds$cut %in% c("Premium", "Ideal") &
Diamonds$color %in% c("E", "D") &
Diamonds$clarity %in% c("VVS1", "IF"),
1,
0)
View(Diamonds)
table(Diamonds$quality)
barplot(table(Diamonds$cut),
xlab = "Cut",
ylab = "Frequency",
main = "Cut distribution")
barplot(table(Diamonds$color),
xlab = "Color",
ylab = "Frequency",
main = "Color Distribution")
barplot(table(Diamonds$clarity),
xlab = "Clarity",
ylab = "Frequency",
main = "Clarity Distribution")
################################################################################
################################################################################
#Classification problem: Predirre correttamente la quality di un
#diamante
Diamonds$quality <- ifelse(Diamonds$cut %in% c("Premium", "Ideal") &
Diamonds$color %in% c("G","F","E", "D") &
Diamonds$clarity %in% c("VS1","VVS2","VVS1", "IF"),
1,
0)
View(Diamonds)
table(Diamonds$quality)
################################################################################
################################################################################
#Classification problem: Predirre correttamente la quality di un
#diamante
Diamonds$quality <- ifelse(Diamonds$cut %in% c("Very Good","Premium", "Ideal") &
Diamonds$color %in% c("G","F","E", "D") &
Diamonds$clarity %in% c("VS1","VVS2","VVS1", "IF"),
1,
0)
View(Diamonds)
table(Diamonds$quality)
hist(table(Diamonds$quality))
table(Diamonds$quality)
plot(table(Diamonds$quality))
boxplot(table(Diamonds$quality))
contrasts(Diamonds$quality) #il livello di riferimento è "D"
#Setting dataset:
Diamonds$quality <- ifelse(Diamonds$cut %in% c("Very Good","Premium", "Ideal") &
Diamonds$color %in% c("G","F","E", "D") &
Diamonds$clarity %in% c("VS1","VVS2","VVS1", "IF"),
"High",
"Low")
View(Diamonds)
table(Diamonds$quality)
contrasts(Diamonds$quality) #il livello di riferimento è "D"
Diamonds$quality <- factor(Diamonds$clarity,levels=c("High","Low"))
table(Diamonds$quality)
View(Diamonds)
#Setting dataset:
Diamonds$quality <- ifelse(Diamonds$cut %in% c("Very Good","Premium", "Ideal") &
Diamonds$color %in% c("G","F","E", "D") &
Diamonds$clarity %in% c("VS1","VVS2","VVS1", "IF"),
"High",
"Low")
View(Diamonds)
Diamonds$quality <- factor(Diamonds$quality,levels=c("High","Low"))
View(Diamonds)
table(Diamonds$quality)
contrasts(Diamonds$quality) #il livello di riferimento è "D"
Diamonds$quality <- factor(Diamonds$quality,levels=c("Low","High"))
View(Diamonds)
table(Diamonds$quality)
contrasts(Diamonds$quality) #il livello di riferimento è High
log_reg <- glm(quality ~ carat+depth_percentage+table+price+length+width+depth,
data = Diamonds ,family = binomial , subset = train)
log_reg
summary(log_reg)
log_pred <- predict(log_reg , Diamonds[-train,],type = "response")
length(Diamonds[-train])
length(Diamonds[-train,])
size(Diamonds[-train,])
size(Diamonds[-train])
length(price_test)
#Creo un vettore di previsioni tutte a low
log_label <- rep("Low", length(price_test))
log_probs <- predict(log_reg , Diamonds[-train,],type = "response")
#Creo un vettore di previsioni tutte a low
log_pred <- rep("Low", length(price_test))
#Creo un vettore di previsioni tutte a low
log_pred <- rep("Low", length(price_test))
#Setto ad High solo le previsioni maggiori della prob 0.5
log_pred[log_probs > .5] = "High"
table(log_pred,Diamonds$quality[-train])
#Test error
mean(log_pred != Diamonds$quality[-train])
table(log_pred,Diamonds$quality[-train])
mean(log_pred == Diamonds$quality[-train])
summary(log_reg)
table(Diamonds$quality)
contrasts(Diamonds$quality) #il livello di riferimento è High
balanced_data <- ovun.sample(quality ~ carat + depth + table + x + y + z,
data = Diamonds, method = "under",
N = 2 * sum(Diamonds$quality == 0))$data
install.packages("ROSE")
################################################################################
############################### Classification
################################################################################
library(ROSE)
################################################################################
############################### Classification
################################################################################
library(ROSE)
balanced_data <- ovun.sample(quality ~ carat + depth + table + x + y + z,
data = Diamonds, method = "under",
N = 2 * sum(Diamonds$quality == 0))$data
balanced_data <- ovun.sample(quality ~ carat + depth + table + length + width + depth,
data = Diamonds, method = "under",
N = 2 * sum(Diamonds$quality == 0))$data
table(balanced_data$quality)
balanced_data <- ovun.sample(quality ~ carat + depth + table + length + width + depth,
data = Diamonds, method = "under",
N = 2 * sum(Diamonds$quality == 0))$data
balanced_data <- ovun.sample(quality ~ carat + depth + table + length + width + depth,
data = Diamonds, method = "under",
N = 2 * sum(Diamonds$quality == "Low"))$data
table(balanced_data$quality)
# Calcola la dimensione effettiva del dataset
effective_dataset_size <- nrow(Diamonds)
# Imposta il parametro N per il sottocampionamento
N <- min(effective_dataset_size, 2 * sum(Diamonds$quality == "Low"))
# Bilanciamento delle classi utilizzando ovun.sample
balanced_data <- ovun.sample(quality ~ carat + depth + table + x + y + z,
data = Diamonds, method = "under",
N = N)$data
# Bilanciamento delle classi utilizzando ovun.sample
balanced_data <- ovun.sample(quality ~ carat + depth + table + length + width + depth,
data = Diamonds, method = "under",
N = N)$data
table(balanced_data$quality)
# Bilanciamento delle classi utilizzando ovun.sample
balanced_data <- ovun.sample(quality ~ carat + depth + table + length + width + depth,
data = Diamonds, method = "under",
N = N)$data
# Conta il numero di righe con etichetta "Low"
num_low <- sum(Diamonds$quality == "Low")
num_low
# Seleziona casualmente 13000 righe con etichetta "Low"
indices_to_remove <- sample(which(Diamonds$quality == "Low"), 13000)
# Rimuovi le righe selezionate dal dataset
Diamonds_bal <- Diamonds[-indices_to_remove, ]
View(Diamonds_bal)
table(Diamonds_bal$quality)
#classe sbilanciata
table(Diamonds$quality)
# Conta il numero di righe con etichetta "Low"
num_low <- sum(Diamonds$quality == "Low")
# Seleziona casualmente 13500 righe con etichetta "Low"
indices_to_remove <- sample(which(Diamonds$quality == "Low"), 13500)
# Rimuovi le righe selezionate dal dataset
Diamonds_bal <- Diamonds[-indices_to_remove, ]
View(Diamonds_bal)
#
table(Diamonds_bal$quality)
# Rimuovi le righe selezionate dal dataset
Diamonds <- Diamonds[-indices_to_remove, ]
# Classi quasi bilanciate
table(Diamonds$quality)
contrasts(Diamonds$quality) #il livello di riferimento è High
log_reg <- glm(quality ~ carat+depth_percentage+table+price+length+width+depth,
data = Diamonds ,family = binomial , subset = train)
summary(log_reg)
log_probs <- predict(log_reg , Diamonds[-train,],type = "response")
nrow(Diamonds)
View(Diamonds)
#Divido il dataset in train e test
train <- sample(nrow(Diamonds),floor(nrow(Diamonds)*0.7),replace = FALSE)
contrasts(Diamonds$quality) #il livello di riferimento è High
log_reg <- glm(quality ~ carat+depth_percentage+table+price+length+width+depth,
data = Diamonds ,family = binomial , subset = train)
summary(log_reg)
log_probs <- predict(log_reg , Diamonds[-train,],type = "response")
#Creo un vettore di previsioni tutte a low
log_pred <- rep("Low", nrow(Diamonds))
#Setto ad High solo le previsioni maggiori della prob 0.5
log_pred[log_probs > .5] = "High"
table(log_pred,Diamonds$quality[-train])
log_pred
log_probs
nrow(log_probs)
log_probs <- predict(log_reg , Diamonds[-train,],type = "response")
View(log_probs)
length(log_probs)
#Creo un vettore di previsioni tutte a low
log_pred <- rep("Low", nrow(Diamonds))
#Setto ad High solo le previsioni maggiori della prob 0.5
log_pred[log_probs > .5] = "High"
table(log_pred,Diamonds$quality[-train])
length(Diamonds$quality[-train])
length(log_pred)
#Creo un vettore di previsioni tutte a low
log_pred <- rep("Low", nrow(Diamonds[-train]))
#Setto ad High solo le previsioni maggiori della prob 0.5
log_pred[log_probs > .5] = "High"
table(log_pred,Diamonds$quality[-train])
nrow(log_pred)
nrow(Diamonds[-train])
nrow(Diamonds[-train,])
#Creo un vettore di previsioni tutte a low
log_pred <- rep("Low", nrow(Diamonds[-train,]))
#Setto ad High solo le previsioni maggiori della prob 0.5
log_pred[log_probs > .5] = "High"
table(log_pred,Diamonds$quality[-train])
#Test error
mean(log_pred != Diamonds$quality[-train])
mean(log_pred == Diamonds$quality[-train])
summary(log_reg)
library(ISLR2)
?Smarket
table(Smarket$Direction)
################################################################################
######### Setting Dataset
################################################################################
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
### Transform Categorical Variables as factors
Diamonds$cut <- factor(Diamonds$cut,
levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))
Diamonds$color <- factor(Diamonds$color,
levels = c("J", "I", "H", "G", "F","E","D"))
Diamonds$clarity <- factor(Diamonds$clarity,
levels=c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"))
set.seed(2)
#Setting dataset:
Diamonds$quality <- ifelse(Diamonds$cut %in% c("Very Good","Premium", "Ideal") &
Diamonds$color %in% c("G","F","E", "D") &
Diamonds$clarity %in% c("VS1","VVS2","VVS1", "IF"),
"High",
"Low")
detect_outlier <- function(x) {
Quantile1 <- quantile(x, probs=.25)
Quantile3 <- quantile(x, probs=.75)
IQR = Quantile3 - Quantile1
x > Quantile3 + (IQR*1.5) | x < Quantile1 - (IQR*1.5)
}
remove_outlier <- function(dataframe,columns=names(dataframe)) {
for (col in columns) {
dataframe <- dataframe[!detect_outlier(dataframe[[col]]), ]
}
print("Remove outliers")
print(dataframe)
}
Diamonds <- remove_outlier(Diamonds, c('carat', 'depth_percentage', 'table', 'price',
"length", 'width', "depth"))
View(Diamonds)
#classe sbilanciata
table(Diamonds$quality)
log_reg <- glm(quality ~ carat+depth_percentage+table+price+length+width+depth,
data = Diamonds ,family = binomial , subset = train)
#Divido il dataset in train e test
train <- sample(nrow(Diamonds),floor(nrow(Diamonds)*0.7),replace = FALSE)
log_reg <- glm(quality ~ carat+depth_percentage+table+price+length+width+depth,
data = Diamonds ,family = binomial , subset = train)
Diamonds$quality <- factor(Diamonds$quality,levels=c("High","Low"))
log_reg <- glm(quality ~ carat+depth_percentage+table+price+length+width+depth,
data = Diamonds ,family = binomial , subset = train)
summary(log_reg)
contrasts(Diamonds$quality) #il livello di riferimento è High
################################################################################
######### Setting Dataset
################################################################################
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
### Transform Categorical Variables as factors
Diamonds$cut <- factor(Diamonds$cut,
levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))
Diamonds$color <- factor(Diamonds$color,
levels = c("J", "I", "H", "G", "F","E","D"))
Diamonds$clarity <- factor(Diamonds$clarity,
levels=c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"))
set.seed(2)
detect_outlier <- function(x) {
Quantile1 <- quantile(x, probs=.25)
Quantile3 <- quantile(x, probs=.75)
IQR = Quantile3 - Quantile1
x > Quantile3 + (IQR*1.5) | x < Quantile1 - (IQR*1.5)
}
remove_outlier <- function(dataframe,columns=names(dataframe)) {
for (col in columns) {
dataframe <- dataframe[!detect_outlier(dataframe[[col]]), ]
}
print("Remove outliers")
print(dataframe)
}
Diamonds <- remove_outlier(Diamonds, c('carat', 'depth_percentage', 'table', 'price',
"length", 'width', "depth"))
#Setting dataset:
Diamonds$quality <- ifelse(Diamonds$cut %in% c("Very Good","Premium", "Ideal") &
Diamonds$color %in% c("G","F","E", "D") &
Diamonds$clarity %in% c("VS1","VVS2","VVS1", "IF"),
"High",
"Low")
Diamonds$quality <- factor(Diamonds$quality,levels=c("Low","High"))
View(Diamonds)
#classe sbilanciata
table(Diamonds$quality)
##Undersampling
new_n <- 3017 / 0.5
new_n
################################################################################
############################### Classification
################################################################################
library(ROSE)
?ovun.sample
undersampling_result <- ovun.sample(quality ~ carat + depth_percentage + table
+ length + width + depth,
data = Diamonds,
method = "under",N = new_n)
Diamonds2 <- undersampling_result$data
View(Diamonds2)
# Classi bilanciate
table(Diamonds2$quality)
#Divido il dataset in train e test
train <- sample(nrow(Diamonds),floor(nrow(Diamonds)*0.7),replace = FALSE)
contrasts(Diamonds2$quality) #il livello di riferimento è High
log_reg <- glm(quality ~ carat+depth_percentage+table+price+length+width+depth,
data = Diamonds ,family = binomial , subset = train)
summary(log_reg)
log_probs <- predict(log_reg , Diamonds[-train,],type = "response")
log_reg <- glm(quality ~ carat+depth_percentage+table+price+length+width+depth,
data = Diamonds2 ,family = binomial , subset = train)
undersampling_result <- ovun.sample(quality ~ carat + depth_percentage + table
+length + width + depth+price,
data = Diamonds,
method = "under",N = new_n)
Diamonds2 <- undersampling_result$data
View(Diamonds2)
# Classi bilanciate
table(Diamonds2$quality)
#Divido il dataset in train e test
train <- sample(nrow(Diamonds2),floor(nrow(Diamonds2)*0.7),replace = FALSE)
contrasts(Diamonds2$quality) #il livello di riferimento è High
log_reg <- glm(quality ~ carat+depth_percentage+table+price+length+width+depth,
data = Diamonds2 ,family = binomial , subset = train)
summary(log_reg)
log_probs <- predict(log_reg , Diamonds2[-train,],type = "response")
#Creo un vettore di previsioni tutte a low
log_pred <- rep("Low", nrow(Diamonds2[-train,]))
#Setto ad High solo le previsioni maggiori della prob 0.5
log_pred[log_probs > .5] = "High"
table(log_pred,Diamonds2$quality[-train])
#Test error
mean(log_pred != Diamonds$quality[-train])
#Test error
mean(log_pred != Diamonds2$quality[-train])
mean(log_pred == Diamonds2$quality[-train])
log_reg <- glm(quality ~ ., data = Diamonds2 ,family = binomial , subset = train)
summary(log_reg)
log_probs <- predict(log_reg , Diamonds2[-train,],type = "response")
#Creo un vettore di previsioni tutte a low
log_pred <- rep("Low", nrow(Diamonds2[-train,]))
#Setto ad High solo le previsioni maggiori della prob 0.5
log_pred[log_probs > .5] = "High"
table(log_pred,Diamonds2$quality[-train])
#Test error
mean(log_pred != Diamonds2$quality[-train])
mean(log_pred == Diamonds2$quality[-train])
################################################################################
############################### LDA
################################################################################
library(MASS)
lda_fit <- lda( quality ~ . ,data = Diamonds2, subset = train)
lda_fit
lda_predict <- predict(lda_fit, newdata = Diamonds[-train,])
table(lda_predict$class,Diamonds$color[-train])
lda_predict <- predict(lda_fit, newdata = Diamonds2[-train,])
table(lda_predict$class,Diamonds$quality[-train])
table(lda_predict$class,Diamonds$quality[-train])
View(Diamonds2)
lda_predict <- predict(lda_fit, newdata = Diamonds2[-train,])
lda_predict
table(lda_predict$class,Diamonds2$quality[-train])
mean(lda_predict$class != Diamonds2$color[-train])
mean(lda_predict$class != Diamonds2$quality[-train])
qda_fit <- qda( quality ~ . ,data = Diamonds2, subset = train)
qda_fit
qda_predict <- predict(qda_fit, newdata = Diamonds2[-train,])
table(qda_predict$class,Diamonds2$quality[-train])
mean(qda_predict$class != Diamonds2$quality[-train])
