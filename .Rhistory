#The argument n.trees = 5000 indicates that we want 5000 trees, and the
#option interaction.depth = 4 limits the depth of each tree.
library(gbm)
set.seed(1)
boost.boston <- gbm(medv ~ ., data = Boston[train , ],
distribution = "gaussian", n.trees = 5000,
interaction.depth = 4)
summary(boost.boston)
#We can also produce partial dependence plots for these two variables. These plots
#illustrate the marginal effect of the selected variables on the response after
#integrating out the other variables. In this case, as we might expect, median
#house prices are increasing with rm and decreasing with lstat.
plot(boost.boston , i = "rm")
plot(boost.boston , i = "lstat")
##### Test MSE ####
#We now use the boosted model to predict medv on the test set:
yhat.boost <- predict(boost.boston , newdata = Boston[-train , ], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
#If we want to, we can perform boosting with a different
#value of the shrinkage parameter lambda in (8.10). The default value is 0.001,
#but this is easily modified. Here we take lambda = 0.2.
boost.boston <- gbm(medv ~ ., data = Boston[train , ],
distribution = "gaussian", n.trees = 5000,
interaction.depth = 4, shrinkage = 0.2, verbose = F)
yhat.boost <- predict(boost.boston ,newdata = Boston[-train , ], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
# Bagging
library ( randomForest )
set.seed(1)
library ( ISLR2 )
#mtry = Number of variables randomly sampled as candidates at each split.
#ntree = Number of trees to grow.
bagg_model <- randomForest(medv ~ . ,data = Boston, subset = train,
mtry = ncol(Boston)-1, importance = TRUE,replace = TRUE)
train <- sample(1:nrow(Boston),floor(nrow(Boston)*0.5))
#mtry = Number of variables randomly sampled as candidates at each split.
#ntree = Number of trees to grow.
bagg_model <- randomForest(medv ~ . ,data = Boston, subset = train,
mtry = ncol(Boston)-1, importance = TRUE,replace = TRUE)
bagg_model
#mtry = Number of variables randomly sampled as candidates at each split.
#ntree = Number of trees to grow.
bagg_model <- randomForest(medv ~ . ,data = Boston, subset = train,
mtry = ncol(Boston)-1,
importance = TRUE,
replace = TRUE,
ntree = 100)
bagg_model
#mtry = Number of variables randomly sampled as candidates at each split.
#ntree = Number of trees to grow.
bagg_model <- randomForest(medv ~ . ,data = Boston, subset = train,
mtry = ncol(Boston)-1,
importance = TRUE,
replace = TRUE,
ntree = 500)
bagg_model
plot(bagg_model)
yhat <- predict(bagg_model, newdata = Boston[-train,])
plot(yhat,Boston$medv[-train])
abline(0,1)
mse <- mean((yhat - Boston$medv[-train])^2)
mse
# how to change number of tree?
# add ntree oprion
bagg_model <- randomForest(medv ~ . ,data = Boston, subset = train,
mtry = ncol(Boston)-1, importance = TRUE, ntree = 100)
bagg_model
plot(bagg_model)
importance(bagg_model)
# Random Forest
# default p = sqrt(m) -> see doc
forest_model <- randomForest(medv ~ . ,data = Boston, subset = train,
mtry = floor(sqrt(ncol(Boston)-1)), importance = TRUE, ntree = 100)
forest_model
# if the object has a non-null test component, then the returned object is a
# matrix where the first column is the out-of-bag estimate of error,
# and the second column is for the test set.
plot(forest_model,type = 'b',col="green",pch = "+")
par(new=TRUE)
plot(bagg_model,type = 'b',col="red",pch='o')
yhat <- predict(forest_model, newdata = Boston[-train,])
mse <- mean((yhat - Boston$medv[-train])^2)
mse
importance(forest_model)
# Boosting
library ( gbm )
set.seed(1)
ntree = 5000;
boost_model
boost_model <- gbm(medv ~ . , data = Boston[train,],
distribution = "gaussian" , n.trees = ntree,
interaction.depth = 4, shrinkage = 0.01 , verbose = F)
boost_model
yhat <- predict(boost_model, newdata = Boston[-train,], n.trees = ntree)
mse <- mean((yhat - Boston$medv[-train])^2)
mse
plot(boost.boston)
plot(boost_model)
setwd("~/Documents/GitHub/Statistical_Learning")
################################################################################
######### Setting Dataset
################################################################################
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
################################################################################
######### Setting Dataset
################################################################################
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
### Transform Categorical Variables as factors
Diamonds$cut <- factor(Diamonds$cut,
levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))
Diamonds$color <- factor(Diamonds$color,
levels = c("J", "I", "H", "G", "F","E","D"))
Diamonds$clarity <- factor(Diamonds$clarity,
levels=c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"))
View(Diamonds)
detect_outlier <- function(x) {
Quantile1 <- quantile(x, probs=.25)
Quantile3 <- quantile(x, probs=.75)
IQR = Quantile3 - Quantile1
x > Quantile3 + (IQR*1.5) | x < Quantile1 - (IQR*1.5)
}
remove_outlier <- function(dataframe,columns=names(dataframe)) {
for (col in columns) {
dataframe <- dataframe[!detect_outlier(dataframe[[col]]), ]
}
print("Remove outliers")
print(dataframe)
}
Diamonds <- remove_outlier(Diamonds, c('carat', 'depth_percentage', 'table', 'price',
"length", 'width', "depth"))
hist(Diamonds$carat, 40 ,
xlab = "Carat",
main = "Carat distribution")
barplot(table(Diamonds$cut),
xlab = "Cut",
ylab = "Frequency",
main = "Cut distribution")
barplot(table(Diamonds$color),
xlab = "Color",
ylab = "Frequency",
main = "Color Distribution")
barplot(table(Diamonds$clarity),
xlab = "Clarity",
ylab = "Frequency",
main = "Clarity Distribution")
hist(Diamonds$depth_percentage, 50 ,
xlab = "Depth Percentage",
main = "Depth Percentage distribution")
hist(Diamonds$table, 40 , xlab = "Table",  main = "Table distribution")
hist(Diamonds$price, 40 , xlab = "Price ($)",  main = "Price distribution")
hist(Diamonds$price, 40 , xlab = "Price (1000$)",  main = "Price distribution")
hist(Diamonds$width, 50 , xlab = "Width (mm)",  main = "Width distribution")
hist(Diamonds$depth, 50 , xlab = "Depth (mm)",  main = "Depth distribution")
# Carica il pacchetto dplyr
library(dplyr)
# Calcola la media di ciascuna variabile numerica
means <- colMeans(numeric_cols)
# Seleziona solo le colonne numeriche
numeric_cols <- Diamonds %>% select(where(is.numeric))
# Calcola la media di ciascuna variabile numerica
means <- colMeans(numeric_cols)
# Calcola la deviazione standard di ciascuna variabile numerica
std_devs <- apply(numeric_cols, 2, sd)
# Combina i risultati in un dataframe
summary_stats <- data.frame(mean = means, std_dev = std_devs)
# Visualizza il dataframe con le medie e le deviazioni standard
View(summary_stats)
#library(ggplot2)
#library(GGally)
library(corrplot)
par(mfrow = c(1,1))
cor_scores <- cor(subset(Diamonds , select = -c(color,clarity,cut)))
corrplot(cor_scores,method = "number")
#train and test indexes
train <- sample(nrow(Diamonds),floor(nrow(Diamonds)*0.7),replace = FALSE)
price_test <- Diamonds$price[-train]
#Train model
lm_model_1 = lm(price ~ . , data = Diamonds,subset = train)
summary(lm_model_1)
#confidence interval 95%
confint(lm_model_1)
#R^2
summary(lm_model_1)$r.sq
lm_train_RMSE
#Train RMSE
lm_train_RMSE = sqrt(mean((Diamonds$price[train] - lm_model_1$fitted.values)^2))
lm_train_RMSE
#test RMSE
pred_err = (Diamonds$price- predict(lm_model_1,Diamonds))^2
train_rmse = sqrt(mean(pred_err[train]))
test_rmse = sqrt(mean(pred_err[-train]))
#using predict() function
y_hat_lm = predict(lm_model_1,newdata = Diamonds[-train,])
lm_test_RMSE_1 = sqrt(mean((y_hat_lm - Diamonds$price[-train])^2))
lm_test_RMSE_1
###### Analisi dei Residui ######
par(mfrow = c(2,2))
#Grafici diagnostici
plot(lm_model_1)
# frequenza dei residui
hist(lm_model_1$residuals,60,
xlab = "Residuals",
main = "Empirical residual distribution")
par(mfrow = c(1,1))
# frequenza dei residui
hist(lm_model_1$residuals,60,
xlab = "Residuals",
main = "Empirical residual distribution")
# frequenza dei residui studentizzati
hist(rstudent(lm_model_1),60,
xlab = "Studentized residuals",
main = "Empirical residual distribution")
#Fitted values vs Residuals
plot(lm_model_1$fitted.values,lm_model_1$residuals,
xlab = "Fitted values",
ylab = "Residuals",
main = "Fitted values vs Residuals",
cex = 1, col = "black")
abline(a=0,b=0,lwd=1.5,col="red")
###### Test sui residui ######
shapiro.test(lm_model_1$residuals[1:5000])
bptest(lm_fit)
##### Model with interaction terms #####
lm_model_2 = lm(price ~ . + (length:width:depth) , data = Diamonds,
subset = train)
summary(lm_model_2)
#Test RMSE
fitt_value_lm_2 = predict(lm_model_2,newdata = Diamonds[-train,])
lm_test_RMSE_2 = sqrt(mean((fitt_value_lm_2 - Diamonds$price[-train])^2))
lm_test_RMSE_2
# frequenza dei residui
hist(lm_model_2$residuals,60,
xlab = "Residuals",
main = "Empirical residual distribution")
#Fitted values vs Residuals
plot(lm_model_2$fitted.values,lm_model_2$residuals,
xlab = "Fitted values",
ylab = "Residuals",
main = "Fitted values vs Residuals",
cex = 1, col = "black")
## Anova test per confrontare i due modelli di regressione lineare creati ##
anova(lm_model_1,lm_model_2, test='F')
###### Best subset selection ######
model_bwd <- regsubsets(price ~ .+ (length:width:depth), data = Diamonds[train,],
nvmax = 24)
################################################################################
############################### Subset selection methods
################################################################################
library(leaps)
###### Best subset selection ######
model_bwd <- regsubsets(price ~ .+ (length:width:depth), data = Diamonds[train,],
nvmax = 24)
###### Best subset selection ######
model_bwd <- regsubsets(price ~ .+ (length:width:depth), data = Diamonds[train,],
nvmax = 24)
names(summary(model_bwd)) #tutte le statistiche fornite
summary(model_bwd)
#We now compute the validation set error for the best
#model of each model size. We first make a model matrix from the test
#data.
#The model.matrix() function is used in many regression packages for building
#an “X” matrix from data.
test_mat <- model.matrix(price ~ . + (length:width:depth),
data = Diamonds[-train , ])
names(summary(model_bwd)) #tutte le statistiche fornite
View(test_mat)
for (i in 1:24) {
coefi <- coef(model_bwd , id = i)
pred <- test_mat[, names(coefi)] %*% coefi #prodotto matrici coefficienti per la previsione di price
val_RMSE[i] <- sqrt(mean((Diamonds$price[-train] - pred)^2))
}
#Now we run a loop, and for each size i, we
#extract the coefficients from regfit.best for the best model of that size,
#multiply them into the appropriate columns of the test model matrix to
#form the predictions, and compute the test MSE.
val_RMSE <- rep(NA, 24)
for (i in 1:24) {
coefi <- coef(model_bwd , id = i)
pred <- test_mat[, names(coefi)] %*% coefi #prodotto matrici coefficienti per la previsione di price
val_RMSE[i] <- sqrt(mean((Diamonds$price[-train] - pred)^2))
}
val_RMSE #Test RMSE per tutti i modelli calcolati
#We find that the best model is the one that contains  variables.
min_RMSE = which.min(val_RMSE)
min_RMSE
coef(model_bwd , min_RMSE)
#R^2 e modello con R^2 più alto in grafico
summary(model_bwd)$rsq
plot(summary(model_bwd)$rsq,xlab = "N° regressor",ylab = "R^2")
max_r2 <- which.max(summary(model_bwd)$rsq)
points(max_r2, summary(model_bwd)$rsq[max_r2], col = "red", cex = 2, pch = 20)
#Modello con test RMSE più basso
plot(val_RMSE,xlab = "N° regressor",ylab = " Test RMSE")
points(min_RMSE, val_RMSE[min_RMSE], col = "blue", cex = 2, pch = 20)
################################################################################
############################### Ridge Regression
################################################################################
library(glmnet)
#preparing data
#creating regressor (automatic handle categorical variables in dummy variables)
x <- model.matrix ( price ~ . +(length : width : depth) ,
Diamonds )[,-1]  #tutte le righe - la prima colonna (intercetta)
y <- Diamonds$price
#Note that by default, the glmnet() function standardizes the
#variables so that they are on the same scale. To turn off this default setting,
#use the argument standardize = FALSE
ridge_model_1 <- glmnet(x[train , ], y[train], alpha = 0,
lambda = NULL,
standardize = TRUE)
dim(coef(ridge_model_1))
#Esempi di valori di lambda
#Seleziono lambda 5 (grande) e calcolo l1 norm per lambda 5 (piccola)
ridge_model_1$lambda[5]
coef(ridge_model_1)[, 5]
sqrt(sum(coef(ridge_model_1)[-1,5]^2)) ##l1 norm (escludendo intercetta)
#Seleziono lambda 95(piccolo) e calcolo l2 norm per lambda 95 (grande)
ridge_model_1$lambda[95]
coef(ridge_model_1)[, 95]
### Andamento dei coefficienti al variare di lambda e l1 norm ###
plot(ridge_model_1, xvar = "lambda",xlab="Log(λ)",
main="Coefficients vs Log(λ) ")
plot(ridge_model_1, xvar = "norm",xlab="l1 norm",
main="Coefficients vs l1 norm")
####### Choosing the best lambda #######
cv_ridge_out <- cv.glmnet(x[train , ], y[train], alpha = 0,
lambda = NULL,
nfolds = 10)
plot(cv_ridge_out)
bestlam_ridge <- cv_ridge_out$lambda.min
bestlam_ridge #0.001 quindi bassa penalizzazione
### Test RMSE ###
ridge_model_2 <- glmnet(x[train , ], y[train], alpha = 0,
lambda = bestlam_ridge,
standardize = TRUE)
#Beta del modello trovato per il miglior lambda
coef(ridge_model_2)
fitt_value_ridge <- predict(ridge_model_2,newx = x[-train,])
test_RMSE_ridge = sqrt(mean((y[-train] - fitt_value_ridge)^2))
test_RMSE_ridge
lasso_model_1 <- glmnet(x[train , ], y[train], alpha = 1,
lambda = NULL,
standardize = TRUE)
lasso_model_1 <- glmnet(x[train , ], y[train], alpha = 1,
lambda = NULL,
standardize = TRUE)
### Andamento dei coefficienti al variare di lambda e l1 norm ###
plot(lasso_model_1, xvar = "lambda",xlab="Log(λ)",
main="Coefficients vs Log(λ) ")
plot(lasso_model_1, xvar = "norm",xlab="l1 norm",
main="Coefficients vs l1 norm")
####### Choosing the best lambda #######
cv_lasso_out <- cv.glmnet(x[train , ], y[train], alpha = 1,
lambda = NULL,
nfolds = 10)
plot(cv_lasso_out)
bestlam_lasso <- cv_lasso_out$lambda.min
bestlam_lasso #0.0001
### Final model e Test RMSE ###
lasso_model_2 <- glmnet(x[train , ], y[train], alpha = 1,
lambda = bestlam_lasso,
standardize = TRUE)
#Beta del modello trovato per il miglior lambda
coef(lasso_model_2)
fitt_value_lasso <- predict(lasso_model_2,newx = x[-train,])
test_RMSE_lasso = sqrt(mean((y[-train] - fitt_value_lasso)^2))
test_RMSE_lasso
set.seed(1) # seed for random number generator
library(boot)
library(boot)
kfold_RMSE <- rep (0 , 6)
k_fit_i <- vector("list", 6)
for(i in 1:6) {
k_fit_i[[i]] <- glm(price ~ poly(length,i) + poly(width,i) +
poly(depth,i) + poly(table,i) + poly(depth_percentage,i)
+poly(carat,i) + color + cut + clarity
,data = Diamonds ) #creo il modello di ordine i
kfold_RMSE[ i ] <- sqrt(cv.glm( Diamonds , k_fit_i[[i]] , K = 10)$delta[1]) #prendo il test RMSE per quel modello
}
kfold_RMSE
plot(1:6,kfold_RMSE,type = "b",col = "blue",
ylab = "CV error",
xlab = "Flexibility (poly degree)",
main = "Test error estimation")
#ANOVA Test
anova(k_fit_i[[1]], k_fit_i[[2]], k_fit_i[[3]], k_fit_i[[4]],
k_fit_i[[5]],k_fit_i[[6]],test = "F")
library(gam)
####### GAM train and test #######
gam_model_1 <- gam(price~ s(carat,4) + cut + color + clarity +
s(depth_percentage,5) + s(table,5) + s(length,4) +
s(width,4) + s(depth,4),data=Diamonds[train, ])
plot(gam_model_1,se=TRUE)
summary(gam_model_1)
##### Analisi dei residui #####
gam_pred_value <- predict(gam_model_1,newdata = Diamonds[-train,])
gam_model_1_residuals = Diamonds$price[-train] - gam_pred_value
#Fitted values vs Residuals
plot(gam_pred_value,gam_model_1_residuals)
#Residuals
plot(gam_model_1_residuals)
#Test RMSE
gam_model_1_RMSE = sqrt(mean((Diamonds$price[-train] - gam_pred_value)^2))
gam_model_1_RMSE #Miglioramento significativo
####### Tre on train dataset ######
set.seed(1)
tree_model_1 <- tree(price ~ ., data = Diamonds, subset = train)
summary(tree_model_1)
################################################################################
############################### Regression Trees
################################################################################
library(tree)
####### Tre on train dataset ######
set.seed(1)
tree_model_1 <- tree(price ~ ., data = Diamonds, subset = train)
summary(tree_model_1)
plot(tree_model_1)
text(tree_model_1 , pretty = 0)
tree_model_1
yhat_tree_1 <- predict(tree_model_1 , newdata = Diamonds[-train,]) #predizioni
residuals_tree_1 <- Diamonds$price[-train] - yhat_tree_1
plot(yhat_tree_1 ,Diamonds$price[-train]) #Previsioni vs dati reali
tree_model_1_RMSE = sqrt(mean((residuals_tree_1)^2)) #Test RMSE
tree_model_1_RMSE
#The function
#cv.tree() performs cross-validation in order to determine the optimal level of
#tree complexity; cost complexity pruning is used in order to select a
#sequence of trees for consideration.
cv_tree <- cv.tree(tree_model_1)
plot(cv_tree$size , cv_tree$dev, type = "b")
plot(cv_tree$size , cv_tree$dev, type = "b")
plot(cv_tree$k , cv_tree$dev, type = "b")
plot(cv_tree$size , cv_tree$dev, type = "b")
#prendo la size con errore minore
best = min(cv_tree$size[cv_tree$dev == min(cv_tree$dev)])
#prune the tree
prune_model <- prune.tree(tree_model_1 , best = best)
plot(prune_model)
text(prune_model , pretty = 0)
library(randomForest)
library(randomForest)
#### Bagging ####
bag_model_1 <- randomForest(price ~ ., data = Diamonds , subset = train,
mtry = ncol(Diamonds)-1,
importance = TRUE,
replace=TRUE,
ntree=100)
bag_model_1
summary(bag_model_1)
plot(bag_model_1)
importance(bag_model_1)
yhat_bag_1 <- predict(bag_model_1 , newdata = Diamonds[-train , ])
plot(yhat_bag_1 ,Diamonds$price[-train]) #Fiited value vs real value
plot(yhat_bag_1 ,yhat_bag_1 - Diamonds$price[-train]) #Fiited value vs Residuals
bag_RMSE=sqrt(mean((yhat_bag_1 - Diamonds$price[-train])^2)) #Test RMSE
bag_RMSE
rf_model_1 <- randomForest(price ~ ., data = Diamonds , subset = train,
mtry = floor(sqrt(ncol(Diamonds)-1)),
importance = TRUE,
replace=TRUE,
ntree=100)
plot(rf_model_1)
importance(rf_model_1)
yhat_rf <- predict(rf_model_1 , newdata = Diamonds[-train , ])
plot(yhat_rf ,price_test)
rf_RMSE = sqrt(mean((yhat_rf - Diamonds$price[-train])^2)) #Test RMSE
rf_RMSE
#### Confronto Bagging e Random Forest ####
plot(rf_model_1,type = 'b',col="green",pch = "+")
par(new=TRUE) #per sovrapporre grafico
plot(bag_model_1,type = 'b',col="red",pch='o')
legend("topright", legend = c("Random Forest", "Bagging"),
col = c("green", "red"), pch = c("+", "o"))
library(gbm)
set.seed(1)
boost_model_1 <- gbm(price ~ ., data = Diamonds[train , ],
distribution = "gaussian",
n.trees = 5000,
interaction.depth = 4)
summary(boost_model_1)
#We can also produce partial dependence plots for these two variables. These plots
#illustrate the marginal effect of the selected variables on the response after
#integrating out the other variables. In this case, as we might expect, the price
#are increasing with both variables.
plot(boost_model_1 , i = "carat")
plot(boost_model_1 , i = "width")
yhat_boost_1 <- predict(boost_model_1 , newdata = Diamonds[-train , ],
n.trees = 5000)
boost_RMSE_1 <- sqrt(mean((yhat_boost_1 - Diamonds$price[-train])^2))
boost_RMSE_1
##If we want to, we can perform boosting with a different
#value of the shrinkage parameter lambda in (8.10). The default value is 0.001,
#but this is easily modified. Here we take lambda = 0.2.
boost_model_2 <- gbm(price ~ ., data = Diamonds[train , ],
distribution = "gaussian",
n.trees = 5000,
interaction.depth = 4,
shrinkage = 0.02,
verbose=F)
yhat_boost_2 <- predict(boost_model_2 , newdata = Diamonds[-train , ],
n.trees = 5000)
boost_RMSE_2 <- sqrt(mean((yhat_boost_2 - price_test)^2))
boost_RMSE_2
### Analisi dei Residui ###
par(mfrow = c(2,2))
#Grafici diagnostici
plot(lm_model_2)
