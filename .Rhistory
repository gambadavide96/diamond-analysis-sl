y_hat_lm = predict(lm_model_1,newdata = Diamonds[-train,])
lm_MSE_1 = mean((y_hat_lm - Diamonds$price[-train])^2)
lm_MSE_1
#test MSE
pred_err = (Diamonds$price- predict(lm_model_1,Diamonds))^2
train_mse = mean(pred_err[train])
test_mse = mean(pred_err[-train])
test_mse
###### Analisi dei Residui ######
par(mfrow = c(1,1))
###### Analisi dei Residui ######
par(mfrow = c(1,1))
plot(lm_model_1)
cex = 1, col = "black",
#Fitted value vs Residuals
plot(lm_fit$fitted.values,lm_fit$residuals,xlab = "Fitted values",
ylab = "Residuals",pch = "o",
cex = 1, col = "black",
main = "Fitted Values vs Residuals")
abline(c(0,0),c(0,length(lm_fit$residuals)), col= "red", lwd = 2)
# frequenza dei residui
hist(lm_fit$residuals,60,
xlab = "Residual",
main = "Empirical residual distribution")
#Index vs Residuals
plot(lm_fit$residuals,ylab = "Residuals",pch = "o",
cex = 1, col = "black",
main = paste0("Residual plot - mean:",round(mean(lm_fit$residuals),
digits = 4),"- var:", round(var(lm_fit$residuals),digits = 2)))
abline(c(0,0),c(0,length(lm_fit$residuals)), col= "red", lwd = 2)
#Grafico residui studentizzati
plot(rstudent(lm_fit),ylab = "Studentized residuals",
cex = 1, col = "black")
library(boot)
summary(glm_fit)
#Linear regression
glm_fit <- glm(price ~ . , data = Diamonds)
summary(glm_fit)
#Cross-validation for Generalized Linear Models: cv.glm K = 10
cv_err <- cv.glm(Diamonds , glm_fit, K = 10)
#K-fold test error
kfold_test_err <- cv_err$delta[1]
kfold_test_err
#preparing data
#creating regressor (automatic handle categorical variables in dummy variables)
x <- model.matrix ( price ~ . , Diamonds )[,-1] #tutte le righe - la prima colonna (intercetta)
################################################################################
############################### Ridge Regression
################################################################################
library(glmnet)
y <- Diamonds$price
#lambda_grid
lambda_grid <- 10^seq(10,-2,length = 100) #generates 100 lambdas
lambda_grid
####### Choosing the best lambda #######
cv.out <- cv.glmnet(x[train , ], y[train], alpha = 0)
plot(cv.out)
################################################################################
######### Setting Dataset
################################################################################
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
### Transform Categorical Variables as factors
Diamonds$cut <- factor(Diamonds$cut,
levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))
Diamonds$color <- factor(Diamonds$color,
levels = c("J", "I", "H", "G", "F","E","D"))
Diamonds$clarity <- factor(Diamonds$clarity,
levels=c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"))
View(Diamonds)
#cut price
Diamonds <- subset(Diamonds, price <= 1000)
################################################################################
######### Setting Dataset
################################################################################
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
### Transform Categorical Variables as factors
Diamonds$cut <- factor(Diamonds$cut,
levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))
Diamonds$color <- factor(Diamonds$color,
levels = c("J", "I", "H", "G", "F","E","D"))
Diamonds$clarity <- factor(Diamonds$clarity,
levels=c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"))
#cut price
Diamonds <- subset(Diamonds, price <= 3000 & price>= 1000)
#train and test indexes
train <- sample(nrow(Diamonds),floor(nrow(Diamonds)*0.7),replace = FALSE)
price_test <- Diamonds$price[-train]
#Train model
lm_model_1 = lm(price ~ . , data = Diamonds,subset = train)
summary(lm_fit)
summary(lm_model_1)
#Train MSE
mean((lm_fit$residuals)^2)
#R^2
summary(lm_fit)$r.sq
#R^2
summary(lm_model_1)$r.sq
#RSE #quantifica la deviazione media dei valori osservati dai valori previsti dal
#modello di regressione.
summary(lm_model_1)$sigma
#Train MSE
lm_train_MSE = mean((Diamonds$price - lm_model_1$fitted.values)^2)
#Train MSE
mean((lm_model_1$residuals)^2)
################################################################################
######### Setting Dataset
################################################################################
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
### Transform Categorical Variables as factors
Diamonds$cut <- factor(Diamonds$cut,
levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))
Diamonds$color <- factor(Diamonds$color,
levels = c("J", "I", "H", "G", "F","E","D"))
Diamonds$clarity <- factor(Diamonds$clarity,
levels=c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"))
#cut price
Diamonds <- subset(Diamonds,price <= 1000)
#train and test indexes
train <- sample(nrow(Diamonds),floor(nrow(Diamonds)*0.7),replace = FALSE)
price_test <- Diamonds$price[-train]
#Train model
lm_model_1 = lm(price ~ . , data = Diamonds,subset = train)
summary(lm_model_1)
#Train MSE
lm_train_MSE = mean((Diamonds$price - lm_model_1$fitted.values)^2)
#Train MSE
mean((lm_model_1$residuals)^2)
abline(c(0,0),c(0,length(lm_fit$residuals)), col= "red", lwd = 2)
#Fitted value vs Residuals
plot(lm_model_1$fitted.values,lm_model_1$residuals,xlab = "Fitted values",
ylab = "Residuals",pch = "o",
cex = 1, col = "black",
main = "Fitted Values vs Residuals")
abline(a=0,b=0,lwd=2,col="red")
#Grafico residui studentizzati
plot(rstudent(lm_model_1),ylab = "Studentized residuals",
cex = 1, col = "black")
################################################################################
######### Setting Dataset
################################################################################
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
Diamonds$cut <- factor(Diamonds$cut,
levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))
Diamonds$color <- factor(Diamonds$color,
levels = c("J", "I", "H", "G", "F","E","D"))
Diamonds$clarity <- factor(Diamonds$clarity,
levels=c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"))
# no nan colums
colSums(is.na(Diamonds))
#cut price
Diamonds <- subset(Diamonds,price <= 1000 & price  >= 500)
#Train model
lm_model_1 = lm(price ~ . , data = Diamonds,subset = train)
summary(lm_model_1)
#Train MSE
lm_train_MSE = mean((Diamonds$price - lm_model_1$fitted.values)^2)
#Train MSE
mean((lm_model_1$residuals)^2)
summary(lm_model_1)
colSums(is.na(Diamonds))
# no nan colums
colSums(is.na(Diamonds))
summary(lm_model_1)
#Train MSE
lm_train_MSE = mean((Diamonds$price - lm_model_1$fitted.values)^2)
#Fitted value vs Residuals
plot(lm_model_1$fitted.values,lm_model_1$residuals,xlab = "Fitted values",
ylab = "Residuals",pch = "o",
cex = 1, col = "black",
main = "Fitted Values vs Residuals")
# frequenza dei residui
hist(lm_fit$residuals,60,
xlab = "Residual",
main = "Empirical residual distribution")
# frequenza dei residui
hist(lm_model_1$residuals,60,
xlab = "Residual",
main = "Empirical residual distribution")
#Index vs Residuals
plot(lm_model_1$residuals,ylab = "Residuals",pch = "o",
cex = 1, col = "black",
main = paste0("Residual plot - mean:",round(mean(lm_fit$residuals),
digits = 4),"- var:", round(var(lm_fit$residuals),digits = 2)))
#Index vs Residuals
plot(lm_model_1$residuals,ylab = "Residuals",pch = "o",
cex = 1, col = "black",
main = paste0("Residual plot - mean:",round(mean(lm_model_1$residuals),
digits = 4),"- var:", round(var(lm_model_1$residuals),digits = 2)))
#Fitted value vs Residuals
plot(lm_model_1$fitted.values,lm_model_1$residuals,xlab = "Fitted values",
ylab = "Residuals",pch = "o",
cex = 1, col = "black",
main = "Fitted Values vs Residuals")
#Grafico residui studentizzati
plot(rstudent(lm_model_1),ylab = "Studentized residuals",
cex = 1, col = "black")
#Linear regression
glm_fit <- glm(price ~ . , data = Diamonds)
summary(glm_fit)
#Cross-validation for Generalized Linear Models: cv.glm K = 10
cv_err <- cv.glm(Diamonds , glm_fit, K = 10)
#using predict() function
y_hat_lm = predict(lm_model_1,newdata = Diamonds[-train,])
lm_MSE_1 = mean((y_hat_lm - Diamonds$price[-train])^2)
lm_MSE_1
#R^2
summary(lm_model_1)$r.sq
library(boot)
summary(glm_fit)
#Cross-validation for Generalized Linear Models: cv.glm K = 10
cv_err <- cv.glm(Diamonds , glm_fit, K = 10)
#K-fold test error
kfold_test_err <- cv_err$delta[1]
kfold_test_err
#preparing data
#creating regressor (automatic handle categorical variables in dummy variables)
x <- model.matrix ( price ~ . , Diamonds )[,-1] #tutte le righe - la prima colonna (intercetta)
y <- Diamonds$price
#lambda_grid
lambda_grid <- 10^seq(10,-2,length = 100) #generates 100 lambdas
ridge.mod <- glmnet(x[train , ], y[train], alpha = 0,
lambda = lambda_grid, thresh = 1e-12)
#train and test indexes
train <- sample(nrow(Diamonds),floor(nrow(Diamonds)*0.7),replace = FALSE)
price_test <- Diamonds$price[-train]
#preparing data
#creating regressor (automatic handle categorical variables in dummy variables)
x <- model.matrix ( price ~ . , Diamonds )[,-1] #tutte le righe - la prima colonna (intercetta)
y <- Diamonds$price
#lambda_grid
lambda_grid <- 10^seq(10,-2,length = 100) #generates 100 lambdas
ridge.mod <- glmnet(x[train , ], y[train], alpha = 0,
lambda = lambda_grid, thresh = 1e-12)
####### Choosing the best lambda #######
cv.out <- cv.glmnet(x[train , ], y[train], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
#lambda_grid
lambda_grid <- 10^seq(10,-2,length = 10) #generates 100 lambdas
ridge.mod <- glmnet(x[train , ], y[train], alpha = 0,
lambda = lambda_grid, thresh = 1e-12)
####### Choosing the best lambda #######
cv.out <- cv.glmnet(x[train , ], y[train], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
#lambda_grid
lambda_grid <- 10^seq(10,-2,length = 1) #generates 100 lambdas
ridge.mod <- glmnet(x[train , ], y[train], alpha = 0,
lambda = lambda_grid, thresh = 1e-12)
####### Choosing the best lambda #######
cv.out <- cv.glmnet(x[train , ], y[train], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
lambda_grid
#library(ggplot2)
#library(GGally)
library(corrplot)
cor_scores <- cor(subset(Diamonds , select = -c(color,clarity,cut)))
corrplot(cor_scores,method = "number")
################################################################################
############################### Regression Trees
################################################################################
library(tree)
####### Tree on full dataset ######
tree_model_1 <- tree(price ~ . , data = Diamonds)
summary(tree_model_1)
plot(tree_model_1)
text(tree_model_1 , pretty = 0)
####### Tree on full dataset ######
tree_model_1 <- tree(price ~ . , data = Diamonds,split = "gini")
plot(tree_model_1)
####### Tree on full dataset ######
tree_model_1 <- tree(price ~ . , data = Diamonds,split = "gini")
summary(tree_model_1)
plot(tree_model_1)
#Train model
lm_model_1 = lm(price ~ . , data = Diamonds,subset = train)
summary(lm_model_1)
Diamonds <- Diamonds %>%
mutate_at(vars(price, carat, depth_percentage, table, length, width, depth), scale)
library(dplyr)
Diamonds <- Diamonds %>%
mutate_at(vars(price, carat, depth_percentage, table, length, width, depth), scale)
View(Diamonds)
#train and test indexes
train <- sample(nrow(Diamonds),floor(nrow(Diamonds)*0.7),replace = FALSE)
price_test <- Diamonds$price[-train]
#Train model
lm_model_1 = lm(price ~ . , data = Diamonds,subset = train)
summary(lm_model_1)
################################################################################
######### Setting Dataset
################################################################################
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
### Transform Categorical Variables as factors
Diamonds$cut <- factor(Diamonds$cut,
levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))
Diamonds$color <- factor(Diamonds$color,
levels = c("J", "I", "H", "G", "F","E","D"))
Diamonds$clarity <- factor(Diamonds$clarity,
levels=c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"))
detect_outlier <- function(x) {
Quantile1 <- quantile(x, probs=.25)
Quantile3 <- quantile(x, probs=.75)
IQR = Quantile3 - Quantile1
x > Quantile3 + (IQR*1.5) | x < Quantile1 - (IQR*1.5)
}
remove_outlier <- function(dataframe,columns=names(dataframe)) {
for (col in columns) {
dataframe <- dataframe[!detect_outlier(dataframe[[col]]), ]
}
print("Remove outliers")
print(dataframe)
}
Diamonds <- remove_outlier(Diamonds, c('carat', 'depth_percentage', 'table', 'price',
"length", 'width', "depth"))
#library(ggplot2)
#library(GGally)
library(corrplot)
par(mfrow = c(1,1))
cor_scores <- cor(subset(Diamonds , select = -c(color,clarity,cut)))
corrplot(cor_scores,method = "number")
cor_scores <- cor(subset(Diamonds , select = -c(color,clarity,cut)))
corrplot(cor_scores,method = "number")
Diamonds <- Diamonds %>%
mutate_at(vars(price, carat, depth_percentage, table, length, width, depth), scale)
#train and test indexes
train <- sample(nrow(Diamonds),floor(nrow(Diamonds)*0.7),replace = FALSE)
price_test <- Diamonds$price[-train]
#Train model
lm_model_1 = lm(price ~ . , data = Diamonds,subset = train)
View(Diamonds)
#The contrasts() function returns the coding that R uses for the dummy variables.
contrasts(Diamonds$cut)
#Train model
lm_model_1 = lm(price ~ . , data = Diamonds,subset = train)
summary(lm_model_1)
#Train MSE
lm_train_MSE = mean((Diamonds$price - lm_model_1$fitted.values)^2)
#Train MSE
mean((lm_model_1$residuals)^2)
#using predict() function
y_hat_lm = predict(lm_model_1,newdata = Diamonds[-train,])
lm_MSE_1 = mean((y_hat_lm - Diamonds$price[-train])^2)
lm_MSE_1
# frequenza dei residui
hist(lm_model_1$residuals,60,
xlab = "Residual",
main = "Empirical residual distribution")
###### Normality and heteroschdaschity ######
shapiro.test(lm_fit$residuals[1:5000])
###### Normality and heteroschdaschity ######
shapiro.test(lm_model_1$residuals[1:5000])
abline(c(0,0),c(0,length(lm_fit$residuals)), col= "red", lwd = 2)
#Index vs Residuals
plot(lm_model_1$residuals,ylab = "Residuals",pch = "o",
cex = 1, col = "black",
main = paste0("Residual plot - mean:",round(mean(lm_model_1$residuals),
digits = 4),"- var:", round(var(lm_model_1$residuals),digits = 2)))
#Fitted value vs Residuals
plot(lm_model_1$fitted.values,lm_model_1$residuals,xlab = "Fitted values",
ylab = "Residuals",pch = "o",
cex = 1, col = "black",
main = "Fitted Values vs Residuals")
####### GAM on full Dataset #######
gam_model_1 <- gam(price~ ns(carat,4) + cut + color + clarity +
s(depth_percentage,5) + s(table,5) + ns(length,4) +
ns(width,4) +ns(depth,4), data=Diamonds)
library(gam)
####### GAM on full Dataset #######
gam_model_1 <- gam(price~ ns(carat,4) + cut + color + clarity +
s(depth_percentage,5) + s(table,5) + ns(length,4) +
ns(width,4) +ns(depth,4), data=Diamonds)
plot(gam_model_1, se = TRUE)
summary(gam_model_1)
MSE_gam_model_1 <- mean((gam_model_1$residuals)^2) #MSE
MSE_gam_model_1
plot(gam_model_1$residuals)
plot(Diamonds$price,gam_model_1$fitted.values)
plot(gam_model_1$fitted.values,gam_model_1$residuals)
plot(Diamonds$price,gam_model_1$residuals)
####### GAM train and test #######
gam_model_train <- gam(price~ ns(carat,4) + cut + color + clarity +
s(depth_percentage,5) + s(table,5) + ns(length,4) +
ns(width,4) +ns(depth,4), data=Diamonds[train, ])
plot(gam_model_train,se=TRUE)
mean((Diamonds$price[-train] - gam_pred_value)^2)
plot(Diamonds$price[-train],gam_pred_value)
gam_pred_value <- predict(gam_model_train,newdata = Diamonds[-train,])
plot(Diamonds$price[-train],gam_pred_value)
plot(Diamonds$price[-train] - gam_pred_value)
mean((Diamonds$price[-train] - gam_pred_value)^2)
plot(gam_model_1, se = TRUE)
summary(gam_model_1)
################################################################################
######### Setting Dataset
################################################################################
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
### Transform Categorical Variables as factors
Diamonds$cut <- factor(Diamonds$cut,
levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))
Diamonds$color <- factor(Diamonds$color,
levels = c("J", "I", "H", "G", "F","E","D"))
Diamonds$clarity <- factor(Diamonds$clarity,
levels=c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"))
detect_outlier <- function(x) {
Quantile1 <- quantile(x, probs=.25)
Quantile3 <- quantile(x, probs=.75)
IQR = Quantile3 - Quantile1
x > Quantile3 + (IQR*1.5) | x < Quantile1 - (IQR*1.5)
}
remove_outlier <- function(dataframe,columns=names(dataframe)) {
for (col in columns) {
dataframe <- dataframe[!detect_outlier(dataframe[[col]]), ]
}
print("Remove outliers")
print(dataframe)
}
Diamonds <- remove_outlier(Diamonds, c('carat', 'depth_percentage', 'table', 'price',
"length", 'width', "depth"))
####### GAM on full Dataset #######
gam_model_1 <- gam(price~ ns(carat,4) + cut + color + clarity +
s(depth_percentage,5) + s(table,5) + ns(length,4) +
ns(width,4) +ns(depth,4), data=Diamonds)
plot(gam_model_1, se = TRUE)
summary(gam_model_1)
MSE_gam_model_1 <- mean((gam_model_1$residuals)^2) #MSE
MSE_gam_model_1
plot(gam_model_1$residuals)
plot(Diamonds$price,gam_model_1$fitted.values)
plot(gam_model_1$fitted.values,gam_model_1$residuals)
plot(Diamonds$price,gam_model_1$residuals)
####### GAM train and test #######
gam_model_train <- gam(price~ ns(carat,4) + cut + color + clarity +
s(depth_percentage,5) + s(table,5) + ns(length,4) +
ns(width,4) +ns(depth,4), data=Diamonds[train, ])
plot(gam_model_train,se=TRUE)
gam_pred_value <- predict(gam_model_train,newdata = Diamonds[-train,])
mean((Diamonds$price[-train] - gam_pred_value)^2)
################################################################################
############################### Regression Trees
################################################################################
library(tree)
####### Test MSE ######
set.seed(2)
tree_model_2 <- tree(price ~ ., data = Diamonds, subset = train)
summary(tree_model_2)
plot(tree_model_2)
text(tree_model_2 , pretty = 0)
yhat_tree_2 <- predict(tree_model_2 , newdata = Diamonds[-train,]) #predizioni
plot(yhat_tree_2 , price_test) #Previsioni vs dati reali
mean((yhat_tree_2 - price_test)^2) #Test MSE
#The cv.tree() function reports the number of terminal nodes of each tree considered
#(size) as well as the corresponding error rate and the value of the
#cost-complexity parameter used (k, which corresponds to aplha )
##dev corresponds to the number of cross-validation errors.
cv_tree  #(Valori molto alti!)
#The function
#cv.tree() performs cross-validation in order to determine the optimal level of
#tree complexity; cost complexity pruning is used in order to select a
#sequence of trees for consideration.
cv_tree <- cv.tree(tree_model_2)
#The cv.tree() function reports the number of terminal nodes of each tree considered
#(size) as well as the corresponding error rate and the value of the
#cost-complexity parameter used (k, which corresponds to aplha )
##dev corresponds to the number of cross-validation errors.
cv_tree  #(Valori molto alti!)
plot(cv_tree$size , cv_tree$dev, type = "b")
plot(cv_tree$k , cv_tree$dev, type = "b")
#prendo la size con errore minore
best = min(cv_tree$size[cv_tree$dev == min(cv_tree$dev)])
#prendo la k con errore minore
k = min(cv_tree$k[cv_tree$dev == min(cv_tree$dev)]) #alpha in the book
#prune the tree
prune_model <- prune.tree(tree_model_2 , best = best)
plot(prune_model)
text(prune_model , pretty = 0)
##### Test MSE on the best sub-tree #####
yhat_prune <- predict(prune_model , newdata = Diamonds[-train , ]) #Predizioni
plot(yhat_prune, price_test) #Previsioni vs dati reali
mean((yhat_prune - price_test)^2) #Test MSE
# Train and test
train<- sample(nrow(Diamonds),floor(nrow(Diamonds)*0.5),replace = FALSE)
price_test <- Diamonds$price[-train]
library(randomForest)
#### Bagging on full Dataset ####
bag_model_1 <- randomForest(price ~ ., data = Diamonds ,
mtry = ncol(Diamonds)-1,
importance = TRUE,
replace=TRUE,
ntree=2) ##Occhio valore di tree
