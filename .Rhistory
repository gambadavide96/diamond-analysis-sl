plot(lm_fit$residuals,ylab = "Residuals",pch = "o",
cex = .5, col = "darkgrey",
main = paste0("Residual plot - mean:",round(mean(lm_fit$residuals),
digits = 4),"- var:", round(var(lm_fit$residuals),digits = 2)))
plot(lm_fit$residuals,ylab = "Residuals",pch = "o",
cex = .1, col = "darkgrey",
main = paste0("Residual plot - mean:",round(mean(lm_fit$residuals),
digits = 4),"- var:", round(var(lm_fit$residuals),digits = 2)))
plot(lm_fit$residuals,ylab = "Residuals",pch = "o",
cex = .8, col = "darkgrey",
main = paste0("Residual plot - mean:",round(mean(lm_fit$residuals),
digits = 4),"- var:", round(var(lm_fit$residuals),digits = 2)))
abline(c(0,0),c(0,length(lm_fit$residuals)), col= "blue", lwd = 2)
abline(c(0,0),c(0,length(lm_fit$residuals)), col= "blue", lwd = 1)
abline(c(0,0),c(0,length(lm_fit$residuals)), col= "blue", lwd = 1)
plot(lm_fit$residuals,ylab = "Residuals",pch = "o",
cex = 1, col = "darkgrey",
main = paste0("Residual plot - mean:",round(mean(lm_fit$residuals),
digits = 4),"- var:", round(var(lm_fit$residuals),digits = 2)))
abline(c(0,0),c(0,length(lm_fit$residuals)), col= "blue", lwd = 1)
plot(lm_fit$residuals,ylab = "Residuals",pch = "o",
cex = 1, col = "black",
main = paste0("Residual plot - mean:",round(mean(lm_fit$residuals),
digits = 4),"- var:", round(var(lm_fit$residuals),digits = 2)))
abline(c(0,0),c(0,length(lm_fit$residuals)), col= "blue", lwd = 1)
abline(c(0,0),c(0,length(lm_fit$residuals)), col= "darkblue", lwd = 1)
plot(lm_fit$residuals,ylab = "Residuals",pch = "o",
cex = 1, col = "darkgrey",
main = paste0("Residual plot - mean:",round(mean(lm_fit$residuals),
digits = 4),"- var:", round(var(lm_fit$residuals),digits = 2)))
abline(c(0,0),c(0,length(lm_fit$residuals)), col= "darkblue", lwd = 1)
abline(c(0,0),c(0,length(lm_fit$residuals)), col= "darkblue", lwd = 2)
abline(c(0,0),c(0,length(lm_fit$residuals)), col= "red", lwd = 2)
plot(lm_fit$residuals,ylab = "Residuals",pch = "o",
cex = 1, col = "darkgrey",
main = paste0("Residual plot - mean:",round(mean(lm_fit$residuals),
digits = 4),"- var:", round(var(lm_fit$residuals),digits = 2)))
abline(c(0,0),c(0,length(lm_fit$residuals)), col= "red", lwd = 2)
#Fitted value vs Residuals
plot(lm_fit$residuals,predict(lm_fit),ylab = "Residuals",pch = "o",
cex = 1, col = "darkgrey",
main = paste0("Residual plot - mean:",round(mean(lm_fit$residuals),
digits = 4),"- var:", round(var(lm_fit$residuals),digits = 2)))
#Fitted value vs Residuals
plot(predict(lm_fit),lm_fit$residuals,ylab = "Residuals",pch = "o",
cex = 1, col = "darkgrey",
main = paste0("Residual plot - mean:",round(mean(lm_fit$residuals),
digits = 4),"- var:", round(var(lm_fit$residuals),digits = 2)))
#Fitted value vs Residuals
plot(lm_fit$residuals,predict(lm_fit),ylab = "Residuals",pch = "o",
cex = 1, col = "darkgrey",
main = paste0("Residual plot - mean:",round(mean(lm_fit$residuals),
digits = 4),"- var:", round(var(lm_fit$residuals),digits = 2)))
plot(lm_fit)
#Fitted value vs Residuals
plot(lm_fit$fitted.values,lm_fit$residuals,ylab = "",pch = "o",
cex = 1, col = "darkgrey",
main = paste0("Residual plot - mean:",round(mean(lm_fit$residuals),
digits = 4),"- var:", round(var(lm_fit$residuals),digits = 2)))
#Fitted value vs Residuals
plot(lm_fit$fitted.values,lm_fit$residuals,xlab = "Fitted values",
ylab = "Residuals",pch = "o",
cex = 1, col = "darkgrey",
main = "Fitted Values vs Residuals")
abline(c(0,0),c(0,length(lm_fit$residuals)), col= "red", lwd = 2)
#Grafico residui studentizzati
plot(rstudent(lm_fit))
#Grafico residui studentizzati
plot(rstudent(lm_fit),ylab = "Studentized residuals")
abline(a=0,b=0,lwd=2,col="red")
#Grafico residui studentizzati
plot(rstudent(lm_fit),ylab = "Studentized residuals",
cex = 1, col = "darkgrey")
abline(a=0,b=0,lwd=2,col="red")
#Grafico residui studentizzati
plot(rstudent(lm_fit),ylab = "Studentized residuals",
cex = 1, col = "black")
#Fitted value vs Residuals
plot(lm_fit$fitted.values,lm_fit$residuals,xlab = "Fitted values",
ylab = "Residuals",pch = "o",
cex = 1, col = "black",
main = "Fitted Values vs Residuals")
plot(lm_fit)
#Grafico residui studentizzati
plot(rstudent(lm_fit),ylab = "Studentized residuals",
cex = 1, col = "black")
set.seed(1) # seed for random number generator
#train indexes
train <- sample(nrow(Diamonds),floor(nrow(Diamonds)*0.7))
#train model
lm_fit_train <- lm(price ~ . , data = Diamonds, subset = train)
summary(lm_fit_train)
mean((lm_fit_train$residuals)^2) #Train MSE
#train model
lm_fit_train <- lm(price ~ . , data = Diamonds, subset = train)
#prediction error vector
pred_err = (Diamonds$price - predict(lm_fit_train,Diamonds))^2
train_mse = mean(pred_err[train])
train_mse
test_mse = mean(pred_err[-train])
test_mse = mean(pred_err[-train])
test_mse
#using predict() function
pred_lm <- predict(lm_fit_train , newdata = Diamonds[-train], se = T)
#using predict() function
pred_lm <- predict(lm_fit_train , newdata = Diamonds, se = T)
summary(pred_lm)
View(pred_lm$fit)
test_mse = mean(pred_err[-train])
test_mse
test_mse = mean((Diamonds$price - pred_lm$fit)^2)
test_mse
train_mse
#using predict() function
pred_lm <- predict(lm_fit_train , newdata = Diamonds[-train, ], se = T)
test_mse = mean((Diamonds$price - pred_lm$fit)^2)
View(pred_lm)
#test MSE
pred_err = (Diamonds$price- predict(lm_fit_train,Diamonds))^2[-train, ]
#test MSE
pred_err = (Diamonds$price[-train]- predict(lm_fit_train,Diamonds)[-train])^2
train_mse = mean(pred_err[train])
test_mse = mean(pred_err[-train])
test_mse
#using predict() function
pred_lm <- predict(lm_fit_train , newdata = Diamonds[-train,-price ], se = T)
#using predict() function
pred_lm <- predict(lm_fit_train , newdata = Diamonds[-train,-Diamonds$price ], se = T)
test_mse = mean((Diamonds$price - pred_lm$fit)^2)
################################################################################
############################### K-fold; k = 10
################################################################################
set.seed(1) # seed for random number generator
library(boot)
#train indexes
train <- sample(nrow(Diamonds),floor(nrow(Diamonds)*0.7))
#train model
lm_fit_train <- lm(price ~ . , data = Diamonds, subset = train)
summary(lm_fit_train)
#test MSE
pred_err = (Diamonds$price- predict(lm_fit_train,Diamonds))^2
train_mse = mean(pred_err[train])
test_mse = mean(pred_err[-train])
################################################################################
############################### K-fold; k = 10
################################################################################
set.seed(1) # seed for random number generator
library(boot)
#Linear regression
glm_fit <- glm(price ~ . , data = Diamonds)
summary(glm_fit)
#Cross-validation for Generalized Linear Models: cv.glm K = 10
cv_err <- cv.glm(Diamonds , glm_fit, K = 10)
#K-fold test error
kfold_test_err <- cv_err$delta[1]
kfold_test_err
#Cross-validation for Generalized Linear Models: cv.glm K = 10
cv_err <- cv.glm(Diamonds , glm_fit, K = 10)
#K-fold test error
kfold_test_err <- cv_err$delta[1]
kfold_test_err
################################################################################
#########################################  RIDGE REGRESSION
################################################################################
library ( ISLR2 )
install.packages("glmnet")
install.packages("glmnet")
library( glmnet )
#Hitters dataset
View(Hitters)
# Remove nan values
sum(is.na(Hitters));   #Numero di righe con valori nulli
Hitters <- na.omit(Hitters) # remove row with nan;
#The model.matrix() function is particularly useful for creating x; not only
#does it produce a matrix corresponding to the 19 predictors but it also
#automatically transforms any qualitative variables into dummy variables.
#The latter property is important because glmnet() can only take numerical,
#quantitative inputs.
x <- model.matrix ( Salary ~ . , Hitters ) [ , -1] #tutte le righe - la prima colonna
y <- Hitters$Salary
# lambda grid
#here we have chosen to implement
#the function over a grid of values ranging from lamba = 10^10 to lambda = 10^-2,
#essentially covering the full range of scenarios from the null model containing
#only the intercept, to the least squares fit
#lambda decrescente
grid <- 10^seq(10,-2,length = 100); #genero una sequenza di 100 lambda
#The glmnet() function has an alpha argument that determines what type
#of model is fit. If alpha=0 then a ridge regression model is fit, and if alpha=1
#then a lasso model is fit.
#Note that by default, the glmnet() function standardizes the
#variables so that they are on the same scale. To turn off this default setting,
#use the argument standardize = FALSE
ridge_fit <- glmnet(x,y,alpha = 0,lambda = grid,standardize=TRUE)
#Associated with each value of lambda is a vector of ridge regression coefficients,
#stored in a matrix that can be accessed by coef(). In this case is 20 x 50 matrix
dim(coef(ridge_fit))
#### Calcolare Test MSE ###
#We now split the samples into a training set and a test set in order
#to estimate the test error of ridge regression and the lasso.
set.seed(1)
train <- sample(1:nrow(x), nrow(x) / 2) #metà indici
test <- (-train)
y.test <- y[test] #riposte salary y di test
#Next we fit a ridge regression model on the training set, and evaluate
#its MSE on the test set, using lambda = 4. Note the use of the predict()
#function again. This time we get predictions for a test set, by replacing
#type="coefficients" with the newx argument.
#thresh = 1e-12 indica una soglia di convergenza molto piccola, il che significa che
#l'algoritmo cercherà una soluzione molto precisa prima di interrompere il processo di ottimizzazione.
ridge.mod <- glmnet(x[train , ], y[train], alpha = 0, lambda = grid, thresh = 1e-12)
ridge.pred <- predict(ridge.mod , s = 4, newx = x[test , ]) #y_hat predetti dal modello
?predict
ridge.pred <- predict(ridge.mod , s = 4, newx = x[test , ]) #y_hat predetti dal modello
mean((ridge.pred - y.test)^2) #Test MSE
#fitting a ridge regression model with a very large value of lambda , 10^10.
#quindi i coefficienti vanno tutti a 0, bias si alza e anche MSE
ridge.pred <- predict(ridge.mod , s = 1e10 , newx = x[test , ])
mean((ridge.pred - y.test)^2)
#preparing data
x_ridge <- model.matrix ( price ~ . , Diamonds ) #tutte le righe - la prima colonna
View(x_ridge)
#preparing data
x_ridge <- model.matrix ( price ~ . , Diamonds )[,-1] #tutte le righe - la prima colonna
View(x_ridge)
y_ridge <- Diamonds$price
#lambda_grid
lamda_grid <- 10^seq(10,-2,length = 100) #generates 100 lambdas
#Ridge Regression
ridge_fit <- glmnet(x,y,alpha = 0,lambda = lambda_grid,standardize=TRUE)
#lambda_grid
lambda_grid <- 10^seq(10,-2,length = 100) #generates 100 lambdas
#Ridge Regression
ridge_fit <- glmnet(x,y,alpha = 0,lambda = lambda_grid,standardize=TRUE)
dim(coef(ridge_fit))
#compare l2-norm at different value of lambda:
#Seleziono lambda 5 e calcolo l2 norm per lambda 5
ridge_fit$lambda[5]
coef(ridge_fit)[, 5]
#Ridge Regression
ridge_fit <- glmnet(x,y,alpha = 0,lambda = lambda_grid,standardize=TRUE)
#compare l2-norm at different value of lambda:
#Seleziono lambda 5 e calcolo l2 norm per lambda 5
ridge_fit$lambda[5]
coef(ridge_fit)[, 5]
#preparing data
#creating regressor (automatic handle categorical variables in dummy variables)
x_ridge <- model.matrix ( price ~ . , Diamonds )[,-1] #tutte le righe - la prima colonna (intercetta)
y_ridge <- Diamonds$price
#lambda_grid
lambda_grid <- 10^seq(10,-2,length = 100) #generates 100 lambdas
#Ridge Regression
ridge_fit <- glmnet(x,y,alpha = 0,lambda = lambda_grid,standardize=TRUE)
dim(coef(ridge_fit))
#compare l2-norm at different value of lambda:
#Seleziono lambda 5 e calcolo l2 norm per lambda 5
ridge_fit$lambda[5]
coef(ridge_fit)[, 5]
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
### Transform Categorical Variables as factors
Diamonds$cut <- factor(Diamonds$cut,
levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))
Diamonds$color <- factor(Diamonds$color,
levels = c("J", "I", "H", "G", "F","E","D"))
Diamonds$clarity <- factor(Diamonds$clarity,
levels=c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"))
# no nan colums
colSums(is.na(Diamonds))
View(Diamonds)
summary(Diamonds)
#preparing data
#creating regressor (automatic handle categorical variables in dummy variables)
x_ridge <- model.matrix ( price ~ . , Diamonds )[,-1] #tutte le righe - la prima colonna (intercetta)
y_ridge <- Diamonds$price
#lambda_grid
lambda_grid <- 10^seq(10,-2,length = 100) #generates 100 lambdas
#Ridge Regression
ridge_fit <- glmnet(x,y,alpha = 0,lambda = lambda_grid,standardize=TRUE)
#Ridge Regression
ridge_fit <- glmnet(x_ridge,y_ridge,alpha = 0,lambda = lambda_grid,standardize=TRUE)
dim(coef(ridge_fit))
#compare l2-norm at different value of lambda:
#Seleziono lambda 5 e calcolo l2 norm per lambda 5
ridge_fit$lambda[5]
coef(ridge_fit)[, 5]
sqrt(sum(coef(ridge_fit)[-1,5]^2)) #l2 norm (escludendo intercetta)
#Seleziono lambda 25 e calcolo l2 norm per lambda 25 (escludendo intercetta)
ridge_fit$lambda[25]
coef(ridge_fit)[, 25]
sqrt(sum(coef(ridge_fit)[-1,25]^2))
#Seleziono lambda 50 e calcolo l2 norm per lambda 50 (escludendo intercetta)
ridge_fit$lambda[50]
coef(ridge_fit)[, 50]
sqrt(sum(coef(ridge_fit)[-1, 50]^2))
#Seleziono lambda 100 e calcolo l2 norm per lambda 100 (escludendo intercetta)
ridge_fit$lambda[100]
coef(ridge_fit)[, 100]
sqrt(sum(coef(ridge_fit)[-1, 100]^2))
library( glmnet )
# Remove nan values
sum(is.na(Hitters));   #Numero di righe con valori nulli
Hitters <- na.omit(Hitters) # remove row with nan;
#The model.matrix() function is particularly useful for creating x; not only
#does it produce a matrix corresponding to the 19 predictors but it also
#automatically transforms any qualitative variables into dummy variables.
#The latter property is important because glmnet() can only take numerical,
#quantitative inputs.
x <- model.matrix ( Salary ~ . , Hitters ) [ , -1] #tutte le righe - la prima colonna
y <- Hitters$Salary
# lambda grid
#here we have chosen to implement
#the function over a grid of values ranging from lamba = 10^10 to lambda = 10^-2,
#essentially covering the full range of scenarios from the null model containing
#only the intercept, to the least squares fit
#lambda decrescente
grid <- 10^seq(10,-2,length = 100); #genero una sequenza di 100 lambda
#By default, the function performs ten-fold cross-validation, though this
#can be changed using the cv.glmnet() argument nfolds.
set.seed(1)
train <- sample(1:nrow(x), nrow(x) / 2) #metà indici
test <- (-train)
y.test <- y[test] #riposte salary y di test
cv.out <- cv.glmnet(x[train , ], y[train], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
#What is the test MSE associated with this value of lambda?
ridge.pred <- predict(ridge.mod , s = bestlam ,newx = x[test , ])
#Next we fit a ridge regression model on the training set, and evaluate
#its MSE on the test set, using lambda = 4. Note the use of the predict()
#function again. This time we get predictions for a test set, by replacing
#type="coefficients" with the newx argument.
#thresh = 1e-12 indica una soglia di convergenza molto piccola, il che significa che
#l'algoritmo cercherà una soluzione molto precisa prima di interrompere il processo di ottimizzazione.
ridge.mod <- glmnet(x[train , ], y[train], alpha = 0, lambda = grid, thresh = 1e-12)
lasso.pred <- predict(lasso.mod , s = bestlam , newx = x[test , ])
#What is the test MSE associated with this value of lambda?
ridge.pred <- predict(ridge.mod , s = bestlam ,newx = x[test , ])
mean((ridge.pred - y.test)^2)
#This represents a further improvement over the test MSE that we got using
#lambda = 4. Finally, we refit our ridge regression model on the full data set,
#using the value of lambda chosen by cross-validation, and examine the coefficient
#estimates.
out <- glmnet(x, y, alpha = 0)
predict(out , type = "coefficients", s = bestlam)[1:20, ]
#Andamento dei coefficienti vs l1 norm
plot(out)
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
### Transform Categorical Variables as factors
Diamonds$cut <- factor(Diamonds$cut,
levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))
Diamonds$color <- factor(Diamonds$color,
levels = c("J", "I", "H", "G", "F","E","D"))
Diamonds$clarity <- factor(Diamonds$clarity,
levels=c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"))
# no nan colums
colSums(is.na(Diamonds))
View(Diamonds)
summary(Diamonds)
#preparing data
#creating regressor (automatic handle categorical variables in dummy variables)
x_ridge <- model.matrix ( price ~ . , Diamonds )[,-1] #tutte le righe - la prima colonna (intercetta)
y_ridge <- Diamonds$price
#lambda_grid
lambda_grid <- 10^seq(10,-2,length = 100) #generates 100 lambdas
####### Choosing the best lambda #######
#train and test indexes
train <- sample(nrow(Diamonds),floor(nrow(Diamonds)*0.7),replace = FALSE)
test <- (-train)
test <- [-train]
test <- [-train,]
test <- (-train,)
test <- (-train)
####### Choosing the best lambda #######
#train and test indexes
train <- sample(nrow(Diamonds),floor(nrow(Diamonds)*0.7),replace = FALSE)
####### Choosing the best lambda #######
set.seed(1)
cv.out <- cv.glmnet(x[train , ], y[train], alpha = 0)
#train and test indexes
train <- sample(nrow(Diamonds),floor(nrow(Diamonds)*0.7),replace = FALSE)
cv.out <- cv.glmnet(x[train , ], y[train], alpha = 0)
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
### Transform Categorical Variables as factors
Diamonds$cut <- factor(Diamonds$cut,
levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))
Diamonds$color <- factor(Diamonds$color,
levels = c("J", "I", "H", "G", "F","E","D"))
Diamonds$clarity <- factor(Diamonds$clarity,
levels=c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"))
# no nan colums
colSums(is.na(Diamonds))
View(Diamonds)
summary(Diamonds)
#preparing data
#creating regressor (automatic handle categorical variables in dummy variables)
x_ridge <- model.matrix ( price ~ . , Diamonds )[,-1] #tutte le righe - la prima colonna (intercetta)
y_ridge <- Diamonds$price
#lambda_grid
lambda_grid <- 10^seq(10,-2,length = 100) #generates 100 lambdas
####### Choosing the best lambda #######
set.seed(1)
#train and test indexes
train <- sample(nrow(Diamonds),floor(nrow(Diamonds)*0.7),replace = FALSE)
cv.out <- cv.glmnet(x[train , ], y[train], alpha = 0)
cv.out <- cv.glmnet(x_ridge[train , ], y_ridge[train], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
ridge.mod <- glmnet(x_ridge[train , ], y_ridge[train], alpha = 0,
lambda = lamnda_grid, thresh = 1e-12)
ridge.mod <- glmnet(x_ridge[train , ], y_ridge[train], alpha = 0,
lambda = lambda_grid, thresh = 1e-12)
cv.out <- cv.glmnet(x_ridge[train , ], y_ridge[train], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
#What is the test MSE associated with this value of lambda?
ridge.pred <- predict(ridge.mod , s = bestlam ,newx = x_ridge[-train , ])
mean((ridge.pred - y.test)^2)
mean((ridge.pred - y_ridge.test)^2)
mean((ridge.pred - y_ridge[-train])^2)
mean((ridge.pred - y_ridge[-train])^2)
#preparing data
#creating regressor (automatic handle categorical variables in dummy variables)
x <- model.matrix ( price ~ . , Diamonds )[,-1] #tutte le righe - la prima colonna (intercetta)
y <- Diamonds$price
#lambda_grid
lambda_grid <- 10^seq(10,-2,length = 100) #generates 100 lambdas
ridge.mod <- glmnet(x[train , ], y[train], alpha = 0,
lambda = lambda_grid, thresh = 1e-12)
####### Choosing the best lambda #######
set.seed(1)
#train and test indexes
train <- sample(nrow(Diamonds),floor(nrow(Diamonds)*0.7),replace = FALSE)
cv.out <- cv.glmnet(x[train , ], y[train], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
#What is the test MSE associated with this value of lambda?
ridge.pred <- predict(ridge.mod , s = bestlam ,newx = x[-train , ])
mean((ridge.pred - y[-train])^2)
#we refit our ridge regression model on the full data set,
#using the value of lambda chosen by cross-validation, and examine the coefficient
#estimates.
ridge_fit <- glmnet(x, y, alpha = 0)
predict(ridge_fit , type = "coefficients", s = bestlam)[1:20, ]
predict(ridge_fit , type = "coefficients", s = bestlam)[1:20, ]
predict(ridge_fit , type = "coefficients", s = bestlam)[1:24, ]
dim(coef(ridge_fit))
#we refit our ridge regression model on the full data set,
#using the value of lambda chosen by cross-validation, and examine the coefficient
#estimates.
ridge_fit <- glmnet(x, y, alpha = 0,lambda = lambda_grid)
dim(coef(ridge_fit))
predict(ridge_fit , type = "coefficients", s = bestlam)[1:24, ]
#Andamento dei coefficienti vs l1 norm
plot(ridge_fit)
bestlam
predict(ridge_fit , type = "coefficients", s = bestlam)[1:24, ]
#we refit our ridge regression model on the full data set,
#using the value of lambda chosen by cross-validation, and examine the coefficient
#estimates.
ridge_fit <- glmnet(x, y, alpha = 0,lambda = lambda_grid,standardize = TRUE)
dim(coef(ridge_fit))
predict(ridge_fit , type = "coefficients", s = bestlam)[1:24, ]
#Andamento dei coefficienti vs l1 norm
plot(ridge_fit)
lasso_fit <- glmnet(x[train , ], y[train], alpha = 1, lambda = grid)
lasso_fit <- glmnet(x[train , ], y[train], alpha = 1, lambda = lambda_grid)
plot(lasso_fit)
#We now perform cross-validation and compute the associated test error.
set.seed(1)
#We now perform cross-validation and compute the associated test error.
set.seed(1)
lasso_mod <- glmnet(x[train , ], y[train], alpha = 1, lambda = lambda_grid)
plot(lasso_mod)
#We now perform cross-validation and compute the associated test error.
set.seed(1)
cv.out <- cv.glmnet(x[train , ], y[train], alpha = 1,nfolds = 10)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
lasso_pred <- predict(lasso_mod , s = bestlam , newx = x[-train , ])
mean((lasso_pred - y[-train])^2)
#we refit our lasso regression model on the full data set,
#using the value of lambda chosen by cross-validation, and examine the coefficient
#estimates.
lasso_fit <- glmnet(x, y, alpha = 1, lambda = grid)
#we refit our lasso regression model on the full data set,
#using the value of lambda chosen by cross-validation, and examine the coefficient
#estimates.
lasso_fit <- glmnet(x, y, alpha = 1, lambda = grid,standardize = TRUE)
#we refit our lasso regression model on the full data set,
#using the value of lambda chosen by cross-validation, and examine the coefficient
#estimates.
lasso_fit <- glmnet(x, y, alpha = 1, lambda = lambda_grid,standardize = TRUE)
lasso_coef <- predict(lasso_fit , type = "coefficients",  s = bestlam)[1:24, ]
lasso_coef
lasso_coef
plot(lasso_fit)
bestlam
cv.out$lambda
