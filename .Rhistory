tree_model_1_RMSE
#The function
#cv.tree() performs cross-validation in order to determine the optimal level of
#tree complexity; cost complexity pruning is used in order to select a
#sequence of trees for consideration.
cv_tree <- cv.tree(tree_model_1)
tree_model_1_RMSE
plot(cv_tree$size , cv_tree$dev, type = "b")
plot(cv_tree$k , cv_tree$dev, type = "b")
#prendo la size con errore minore
best = min(cv_tree$size[cv_tree$dev == min(cv_tree$dev)])
#prendo la k con errore minore
k = min(cv_tree$k[cv_tree$dev == min(cv_tree$dev)]) #alpha in the book
#prune the tree
prune_model <- prune.tree(tree_model_1 , best = best)
plot(prune_model)
text(prune_model , pretty = 0)
plot(cv_tree$size , cv_tree$dev, type = "b")
##### Test MSE on the best sub-tree #####
yhat_prune <- predict(prune_model , newdata = Diamonds[-train , ]) #Predizioni
plot(yhat_prune, price_test) #Previsioni vs dati reali
prune_model_RMSE = sqrt(mean((yhat_prune - price_test)^2)) #Test RMSE
tree_model_1_RMSE
library(randomForest)
library(randomForest)
#### Bagging ####
bag_model_1 <- randomForest(price ~ ., data = Diamonds , subset = train,
mtry = ncol(Diamonds)-1,
importance = TRUE,
replace=TRUE,
ntree=100)
rf_model_1
bag_model_1
summary(bag_model_1)
plot(bag_model_1)
importance(bag_model_1)
yhat_bag_1 <- predict(bag_model_1 , newdata = Diamonds[-train , ])
plot(yhat_bag_1 ,Diamonds$price[-train]) #Fiited value vs real value
plot(yhat_bag_1 ,yhat_bag_1 - Diamonds$price[-train]) #Fiited value vs Residuals
bag_RMSE=sqrt(mean((yhat_bag_1 - Diamonds$price[-train])^2)) #Test RMSE
bag_RMSE
rf_model_1 <- randomForest(price ~ ., data = Diamonds , subset = train,
mtry = floor(sqrt(ncol(Diamonds)-1)),
importance = TRUE,
replace=TRUE,
ntree=100)
rf_model_1
summary(rf_model_1)
plot(rf_model_1)
importance(rf_model_1)
yhat_rf <- predict(rf_model_1 , newdata = Diamonds[-train , ])
plot(yhat_rf ,price_test)
rf_RMSE = sqrt(mean((yhat_rf - Diamonds$price[-train])^2)) #Test RMSE
rf_RMSE
#### Confronto Bagging e Random Forest ####
plot(rf_model_1,type = 'b',col="green",pch = "+")
par(new=TRUE) #per sovrapporre grafico
plot(bag_model_1,type = 'b',col="red",pch='o')
legend("topright", legend = c("Random Forest", "Bagging"),
col = c("green", "red"), pch = c("+", "o"))
importance(bag_model_1)
plot(bag_model_1)
plot(bag_model_1,main = "Error vs Number of trees")
plot(bag_model_1,main = "Error vs Number of Trees")
yhat_bag_1 <- predict(bag_model_1 , newdata = Diamonds[-train , ])
plot(yhat_bag_1 ,Diamonds$price[-train]) #Fiited value vs real value
plot(yhat_bag_1 ,yhat_bag_1 - Diamonds$price[-train]) #Fiited value vs Residuals
plot(yhat_bag_1 ,yhat_bag_1 - Diamonds$price[-train],
xlab = "Predicted values",
ylab = "Residuals",
main = "Predicted values vs Residuals",
cex = 1, col = "black")
abline(a=0,b=0,lwd=1.5,col="red")
#### Confronto Bagging e Random Forest ####
plot(rf_model_1,type = 'b',col="green",pch = "+")
par(new=TRUE) #per sovrapporre grafico
plot(bag_model_1,type = 'b',col="red",pch='o')
legend("topright", legend = c("Random Forest", "Bagging"),
col = c("green", "red"), pch = c("+", "o"))
rf_RMSE
bag_RMSE
boost_model_1 <- gbm(price ~ ., data = Diamonds[train , ],
distribution = "gaussian",
n.trees = 5000,
interaction.depth = 4)
library(gbm)
boost_model_1 <- gbm(price ~ ., data = Diamonds[train , ],
distribution = "gaussian",
n.trees = 5000,
interaction.depth = 4)
summary(boost_model_1)
yhat_boost_1 <- predict(boost_model_1 , newdata = Diamonds[-train , ],
n.trees = 5000)
boost_RMSE_1 <- sqrt(mean((yhat_boost_1 - Diamonds$price[-train])^2))
boost_RMSE_1
plot(yhat_boost_1 ,yhat_boost_1 - Diamonds$price[-train],
xlab = "Predicted values",
ylab = "Residuals",
main = "Predicted values vs Residuals",
cex = 1, col = "black")
abline(a=0,b=0,lwd=1.5,col="red")
##If we want to, we can perform boosting with a different
#value of the shrinkage parameter lambda in (8.10). The default value is 0.001,
#but this is easily modified. Here we take lambda = 0.2.
boost_model_2 <- gbm(price ~ ., data = Diamonds[train , ],
distribution = "gaussian",
n.trees = 5000,
interaction.depth = 4,
shrinkage = 0.02,
verbose=F)
summary(boost_model_2)
boost_RMSE_2 <- sqrt(mean((yhat_boost_2 - price_test)^2))
yhat_boost_2 <- predict(boost_model_2 , newdata = Diamonds[-train , ],
n.trees = 5000)
boost_RMSE_2 <- sqrt(mean((yhat_boost_2 - price_test)^2))
boost_RMSE_2
plot(yhat_boost_2 ,yhat_boost_2 - Diamonds$price[-train],
xlab = "Predicted values",
ylab = "Residuals",
main = "Predicted values vs Residuals",
cex = 1, col = "black")
abline(a=0,b=0,lwd=1.5,col="red")
bag_RMSE
plot(yhat_boost_1 ,yhat_boost_1 - Diamonds$price[-train],
xlab = "Predicted values",
ylab = "Residuals",
main = "Predicted values vs Residuals",
cex = 1, col = "black")
abline(a=0,b=0,lwd=1.5,col="red")
plot(yhat_boost_2 ,yhat_boost_2 - Diamonds$price[-train],
xlab = "Predicted values",
ylab = "Residuals",
main = "Predicted values vs Residuals",
cex = 1, col = "black")
abline(a=0,b=0,lwd=1.5,col="red")
rf_RMSE
summary(boost_model_2)
yhat_boost_2 <- predict(boost_model_2 , newdata = Diamonds[-train , ],
n.trees = 5000)
boost_RMSE_2
##If we want to, we can perform boosting with a different
#value of the shrinkage parameter lambda in (8.10). The default value is 0.001,
#but this is easily modified. Here we take lambda = 0.2.
boost_model_2 <- gbm(price ~ ., data = Diamonds[train , ],
distribution = "gaussian",
n.trees = 5000,
interaction.depth = 4,
shrinkage = 0.02,
verbose=F)
yhat_boost_2 <- predict(boost_model_2 , newdata = Diamonds[-train , ],
n.trees = 5000)
boost_RMSE_2 <- sqrt(mean((yhat_boost_2 - price_test)^2))
boost_RMSE_2
plot(yhat_boost_2 ,yhat_boost_2 - Diamonds$price[-train],
xlab = "Predicted values",
ylab = "Residuals",
main = "Predicted values vs Residuals",
cex = 1, col = "black")
abline(a=0,b=0,lwd=1.5,col="red")
boost_RMSE_2
plot(yhat_boost_2 ,yhat_boost_2 - Diamonds$price[-train],
xlab = "Predicted values",
ylab = "Residuals",
main = "Predicted values vs Residuals",
cex = 1, col = "black")
abline(a=0,b=0,lwd=1.5,col="red")
#Predicted values vs Residuals
plot(y_hat_lm,(y_hat_lm - Diamonds$price[-train]),
xlab = "Fitted values",
ylab = "Residuals",
main = "Fitted values vs Residuals",
cex = 1, col = "black")
abline(a=0,b=0,lwd=1.5,col="red")
#Fitted values vs Residuals
plot(lm_model_1$fitted.values,lm_model_1$residuals,
xlab = "Fitted values",
ylab = "Residuals",
main = "Fitted values vs Residuals",
cex = 1, col = "black")
abline(a=0,b=0,lwd=1.5,col="red")
#Predicted values vs Residuals
plot(y_hat_lm,(y_hat_lm - Diamonds$price[-train]),
xlab = "Fitted values",
ylab = "Residuals",
main = "Fitted values vs Residuals",
cex = 1, col = "black")
abline(a=0,b=0,lwd=1.5,col="red")
#Predicted values vs Residuals
plot(y_hat_lm,(y_hat_lm - Diamonds$price[-train]),
xlab = "Fitted values",
ylab = "Residuals",
main = "Predicted values vs Residuals",
cex = 1, col = "black")
abline(a=0,b=0,lwd=1.5,col="red")
#Fitted values vs Residuals
plot(lm_model_1$fitted.values,lm_model_1$residuals,
xlab = "Fitted values",
ylab = "Residuals",
main = "Fitted values vs Residuals",
cex = 1, col = "black")
abline(a=0,b=0,lwd=1.5,col="red")
#Predicted values vs Residuals
plot(y_hat_lm,(y_hat_lm - Diamonds$price[-train]),
xlab = "Fitted values",
ylab = "Residuals",
main = "Predicted values vs Residuals",
cex = 1, col = "black")
abline(a=0,b=0,lwd=1.5,col="red")
#Predicted values vs Residuals
plot(y_hat_lm,(y_hat_lm - Diamonds$price[-train]),
xlab = "Predicted values",
ylab = "Residuals",
main = "Predicted values vs Residuals",
cex = 1, col = "black")
abline(a=0,b=0,lwd=1.5,col="red")
#### Confronto Bagging e Random Forest ####
plot(rf_model_1,type = 'b',col="green",pch = "+")
rf_model_1
#### Confronto Bagging e Random Forest ####
plot(rf_model_1,type = 'b',col="green",pch = "+")
par(new=TRUE) #per sovrapporre grafico
plot(bag_model_1,type = 'b',col="red",pch='o')
rf_model_1 <- randomForest(price ~ ., data = Diamonds , subset = train,
mtry = floor(sqrt(ncol(Diamonds)-1)),
importance = TRUE,
replace=TRUE,
ntree=100)
rf_model_1
library(randomForest)
#### Confronto Bagging e Random Forest ####
plot(rf_model_1,type = 'b',col="green",pch = "+")
par(new=TRUE) #per sovrapporre grafico
plot(bag_model_1,type = 'b',col="red",pch='o')
legend("topright", legend = c("Random Forest", "Bagging"),
col = c("green", "red"), pch = c("+", "o"))
#### Confronto Bagging e Random Forest ####
plot(rf_model_1,type = 'b',col="green",pch = "+")
par(new=TRUE) #per sovrapporre grafico
plot(bag_model_1,type = 'b',col="red",pch='o')
legend("topright", legend = c("Random Forest", "Bagging"),
col = c("green", "red"), pch = c("+", "o"))
par(mfrow=c(1,1)) # Imposta il layout a una singola griglia
plot(rf_model_1, type = 'b', col = "green", pch = "+", main = "Confronto Bagging e Random Forest")
points(bag_model_1, type = 'b', col = "red", pch = 'o')
legend("topright", legend = c("Random Forest", "Bagging"),
col = c("green", "red"), pch = c("+", "o"))
points(bag_model_1, type = 'b', col = "red", pch = 'o')
bag_model_1
plot(bag_model_1, type = 'b', col = "red", pch = 'o')
plot(rf_model_1, type = 'b', col = "green", pch = "+", main = "Confronto Bagging e Random Forest")
points(bag_model_1, type = 'b', col = "red", pch = 'o')
points(bag_model_1, type = 'b', col = "red", pch = 'o')
points(bag_model_1, type = 'b', col = "red", pch = 'o')
points(bag_model_1, type = 'b', col = "red", pch = 'o')
points(bag_model_1, type = 'b', col = "red", pch = 'o')
points(bag_model_1, type = 'b', col = "red", pch = 'o')
par(new=TRUE)
points(bag_model_1, type = 'b', col = "red", pch = 'o')
plot(bag_model_1, type = 'b', col = "red", pch = 'o')
legend("topright", legend = c("Random Forest", "Bagging"),
col = c("green", "red"), pch = c("+", "o"))
?par
?par()
plot(bag_model_1, type = 'b', col = "red", pch = 'o')
plot(rf_model_1, type = 'b', col = "green", pch = "+", main = "Confronto Bagging e Random Forest")
# Traccia il grafico per Random Forest
plot(rf_model_1, type = 'b', col = "green", pch = "+", main = "Confronto Random Forest e Bagging")
# Aggiungi una legenda per Random Forest
legend("topright", legend = "Random Forest", col = "green", pch = "+")
# Traccia il grafico per Bagging
plot(bag_model_1, type = 'b', col = "red", pch = 'o', add = TRUE)
# Aggiungi una legenda per Bagging
points(NA, NA, col = "red", pch = "o", legend = "Bagging")
#### Confronto Bagging e Random Forest ####
plot(rf_model_1,type = 'b',col="green",pch = "+")
par(new=TRUE) #per sovrapporre grafico
plot(bag_model_1,type = 'b',col="red",pch='o')
legend("topright", legend = c("Random Forest", "Bagging"),
col = c("green", "red"), pch = c("+", "o"))
# Traccia il grafico per Random Forest
plot(rf_model_1, type = 'b', col = "green", pch = "+", main = "Confronto Random Forest e Bagging")
#### Confronto Bagging e Random Forest ####
plot(rf_model_1,type = 'b',col="green",pch = "+")
# Traccia il grafico per Random Forest
plot(rf_model_1, type = 'b', col = "green", pch = "+", main = "Confronto Random Forest e Bagging")
#### Confronto Bagging e Random Forest ####
plot(rf_model_1,type = 'b',col="green",pch = "+")
plot(bag_model_1,type = 'b',col="red",pch='o',add= TRUE)
legend("topright", legend = c("Random Forest", "Bagging"),
col = c("green", "red"), pch = c("+", "o"))
#### Confronto Bagging e Random Forest ####
plot(rf_model_1,type = 'b',col="green",pch = "+",
main = "Random Forest vs Bagging")
plot(bag_model_1,type = 'b',col="red",pch='o',add= TRUE)
legend("topright", legend = c("Random Forest", "Bagging"),
col = c("green", "red"), pch = c("+", "o"))
rf_RMSE
bag_RMSE
rf_model_1$mse
plot(1:8,kfold_RMSE,type = "b",col = "blue",
ylab = "CV error",
xlab = "Flexibility (poly degree)",
main = "Test error estimation")
plot(lm_test_RMSE_1,lm_test_RMSE_2,kfold_RMSE,type = "b",col = "blue",
ylab = "CV error",
xlab = "Flexibility (poly degree)",
main = "Test error estimation")
plot(c(lm_test_RMSE_1lm_test_RMSE_2),kfold_RMSE,type = "b",col = "blue",
ylab = "CV error",
xlab = "Flexibility (poly degree)",
main = "Test error estimation")
plot(c(lm_test_RMSE_1,lm_test_RMSE_2),kfold_RMSE,type = "b",col = "blue",
ylab = "CV error",
xlab = "Flexibility (poly degree)",
main = "Test error estimation")
plot(c("Linear model 1","Linear model 2"),c(c(lm_test_RMSE_1,lm_test_RMSE_2)),type = "b",col = "blue",
ylab = "CV error",
xlab = "Flexibility (poly degree)",
main = "Test error estimation")
plot(c("Linear model 1","Linear model 2"),(c(lm_test_RMSE_1,lm_test_RMSE_2)),type = "b",col = "blue",
ylab = "CV error",
xlab = "Flexibility (poly degree)",
main = "Test error estimation")
plot(c("Linear model 1","Linear model 2"),c(lm_test_RMSE_1,lm_test_RMSE_2),type = "b",col = "blue",
ylab = "CV error",
xlab = "Flexibility (poly degree)",
main = "Test error estimation")
plot(c("Linear model 1","Linear model 2"),c(lm_test_RMSE_1,lm_test_RMSE_2),type = "b",col = "blue",
ylab = "CV error",
xlab = "Flexibility (poly degree)",
main = "Test error estimation")
plot(c("Linear model 1","Linear model 2"),c(lm_test_RMSE_1,lm_test_RMSE_2),type = "b",col = "blue",
ylab = "CV error",
xlab = "Flexibility (poly degree)",
main = "Test error estimation")
plot(models,errors,type = "b",col = "blue",
ylab = "CV error",
xlab = "Flexibility (poly degree)",
main = "Test error estimation")
models <- c("Linear model 1", "Linear model 2")
errors <- c(lm_test_RMSE_1, lm_test_RMSE_2)
plot(models,errors,type = "b",col = "blue",
ylab = "CV error",
xlab = "Flexibility (poly degree)",
main = "Test error estimation")
plot(models, errors, type = "b", col = "blue",
ylab = "CV error",
xlab = "Flexibility (poly degree)",
main = "Test error estimation")
models <- c(1, 2)
plot(models, errors, type = "b", col = "blue",
ylab = "CV error",
xlab = "Flexibility (poly degree)",
main = "Test error estimation")
models <- c("Linear model 1", "Linear model 2")
errors <- c(lm_test_RMSE_1, lm_test_RMSE_2)
barplot(models, errors, type = "b", col = "blue",
ylab = "CV error",
xlab = "Flexibility (poly degree)",
main = "Test error estimation")
barplot(errors,names.arg = model_names, type = "b", col = "blue",
ylab = "CV error",
xlab = "Flexibility (poly degree)",
main = "Test error estimation")
barplot(errors,names.arg = models, type = "b", col = "blue",
ylab = "CV error",
xlab = "Flexibility (poly degree)",
main = "Test error estimation")
plot(1:length(models),errors, type = "b", col = "blue",
ylab = "CV error",
xlab = "Flexibility (poly degree)",
main = "Test error estimation")
plot(1:length(models),errors, type = "b", col = "blue",
ylab = "CV error",
xlab = "Flexibility (poly degree)",
main = "Test error estimation",
xaxt = "n")
axis(side = 1, at = 1:length(model_names), labels = model_names, las = 2)
axis(side = 1, at = 1:length(models), labels = model_names, las = 2)
axis(side = 1, at = 1:length(models), labels = model, las = 2)
axis(side = 1, at = 1:length(models), labels = models, las = 2)
models <- c("LM 1", "LM 2")
errors <- c(lm_test_RMSE_1, lm_test_RMSE_2)
plot(1:length(models),errors, type = "b", col = "blue",
ylab = "CV error",
xlab = "Flexibility (poly degree)",
main = "Test error estimation",
xaxt = "n")
axis(side = 1, at = 1:length(models), labels = models, las = 2)
axis(side = 1, at = 1:length(models), labels = models, las = 1)
plot(1:length(models),errors, type = "b", col = "blue",
ylab = "CV error",
xlab = "Flexibility (poly degree)",
main = "Test error estimation",
xaxt = "n")
axis(side = 1, at = 1:length(models), labels = models, las = 1)
#We find that the best model is the one that contains  variables.
min_RMSE = which.min(val_RMSE)
min_RMSE
val_RMSE[min_RMSE]
RMSE_subselection <- val_RMSE[min_RMSE]
val_RMSE[min_RMSE]
models <- c("LM 1", "LM 2","BSS")
errors <- c(lm_test_RMSE_1, lm_test_RMSE_2,RMSE_subselection)
plot(1:length(models),errors, type = "b", col = "blue",
ylab = "CV error",
xlab = "Flexibility (poly degree)",
main = "Test error estimation",
xaxt = "n")
axis(side = 1, at = 1:length(models), labels = models, las = 1)
models <- c("Ridge","LM 1", "LM 2","BSS")
errors <- c(test_RMSE_ridge,lm_test_RMSE_1,
lm_test_RMSE_2,RMSE_subselection)
plot(1:length(models),errors, type = "b", col = "blue",
ylab = "CV error",
xlab = "Flexibility (poly degree)",
main = "Test error estimation",
xaxt = "n")
axis(side = 1, at = 1:length(models), labels = models, las = 1)
plot(1:length(models),errors, type = "b", col = "blue",
ylab = "Test RMSE",
xlab = "Models",
main = "Test RMSE comparison",
xaxt = "n")
axis(side = 1, at = 1:length(models), labels = models, las = 1)
errors <- c(test_RMSE_ridge,test_RMSE_lasso,lm_test_RMSE_1,
lm_test_RMSE_2,RMSE_subselection)
plot(1:length(models),errors, type = "b", col = "blue",
ylab = "Test RMSE",
xlab = "Models",
main = "Test RMSE comparison",
xaxt = "n")
models <- c("Ridge","Lasso","LM 1", "LM 2","BSS")
errors <- c(test_RMSE_ridge,test_RMSE_lasso,lm_test_RMSE_1,
lm_test_RMSE_2,RMSE_subselection)
plot(1:length(models),errors, type = "b", col = "blue",
ylab = "Test RMSE",
xlab = "Models",
main = "Test RMSE comparison",
xaxt = "n")
axis(side = 1, at = 1:length(models), labels = models, las = 1)
models <- c("Ridge","LM 1","Lasso","LM 2","BSS")
errors <- c(test_RMSE_ridge,test_RMSE_lasso,lm_test_RMSE_1,
lm_test_RMSE_2,RMSE_subselection)
models <- c("Ridge","LM 1","Lasso","LM 2","BSS")
errors <- c(test_RMSE_ridge,lm_test_RMSE_1,test_RMSE_lasso,
lm_test_RMSE_2,RMSE_subselection)
plot(1:length(models),errors, type = "b", col = "blue",
ylab = "Test RMSE",
xlab = "Models",
main = "Test RMSE comparison",
xaxt = "n")
axis(side = 1, at = 1:length(models), labels = models, las = 1)
kfold_RMSE
plot(1:8,kfold_RMSE,type = "b",col = "blue",
ylab = "CV error",
xlab = "Flexibility (poly degree)",
main = "Test error estimation")
best_RMSE_poly -> kfold_RMSE[8]
best_RMSE_poly <- kfold_RMSE[8]
models <- c("Ridge","LM 1","Lasso","LM 2","BSS","poly")
errors <- c(test_RMSE_ridge,lm_test_RMSE_1,test_RMSE_lasso,
lm_test_RMSE_2,RMSE_subselection,best_RMSE_poly)
plot(1:length(models),errors, type = "b", col = "blue",
ylab = "Test RMSE",
xlab = "Models",
main = "Test RMSE comparison",
xaxt = "n")
axis(side = 1, at = 1:length(models), labels = models, las = 1)
models <- c("Ridge","LM 1","Lasso","LM 2","BSS","GAM","poly")
errors <- c(test_RMSE_ridge,lm_test_RMSE_1,test_RMSE_lasso,
lm_test_RMSE_2,RMSE_subselection,gam_model_1_RMSE,best_RMSE_poly)
plot(1:length(models),errors, type = "b", col = "blue",
ylab = "Test RMSE",
xlab = "Models",
main = "Test RMSE comparison",
xaxt = "n")
axis(side = 1, at = 1:length(models), labels = models, las = 1)
models <- c("Tree","Ridge","LM 1","Lasso","LM 2","BSS","GAM","poly")
errors <- c(tree_model_1_RMSE,test_RMSE_ridge,lm_test_RMSE_1,test_RMSE_lasso,
lm_test_RMSE_2,RMSE_subselection,
gam_model_1_RMSE,best_RMSE_poly)
plot(1:length(models),errors, type = "b", col = "blue",
ylab = "Test RMSE",
xlab = "Models",
main = "Test RMSE comparison",
xaxt = "n")
axis(side = 1, at = 1:length(models), labels = models, las = 1)
models <- c("Tree","Ridge","LM 1","Lasso","LM 2",
"BSS","GAM","poly","Bagging","Boosting")
errors <- c(tree_model_1_RMSE,test_RMSE_ridge,lm_test_RMSE_1,test_RMSE_lasso,
lm_test_RMSE_2,RMSE_subselection,
gam_model_1_RMSE,best_RMSE_poly,
bag_RMSE,boost_RMSE_2)
plot(1:length(models),errors, type = "b", col = "blue",
ylab = "Test RMSE",
xlab = "Models",
main = "Test RMSE comparison",
xaxt = "n")
axis(side = 1, at = 1:length(models), labels = models, las = 1)
models <- c("Tree","Ridge","LM 1","Lasso","LM 2",
"BSS","GAM","Poly","Bagging","Boosting")
errors <- c(tree_model_1_RMSE,test_RMSE_ridge,lm_test_RMSE_1,test_RMSE_lasso,
lm_test_RMSE_2,RMSE_subselection,
gam_model_1_RMSE,best_RMSE_poly,
bag_RMSE,boost_RMSE_2)
plot(1:length(models),errors, type = "b", col = "blue",
ylab = "Test RMSE",
xlab = "Models",
main = "Test RMSE comparison",
xaxt = "n")
axis(side = 1, at = 1:length(models), labels = models, las = 1)
