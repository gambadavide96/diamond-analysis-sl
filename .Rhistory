test_cubic = mean(( Auto$mpg - predict (lm_fit_cubic,Auto)) [-train ]^2)
lm_fit_4 <- lm( mpg ~ poly ( horsepower, 4, raw = TRUE) , data = Auto ,
subset = train )
test_4 = mean(( Auto$mpg - predict (lm_fit_cubic,Auto)) [-train ]^2)
lm_fit_5 <- lm( mpg ~ poly ( horsepower, 5, raw = TRUE) , data = Auto ,
subset = train )
test_5 = mean(( Auto$mpg - predict (lm_fit_cubic,Auto)) [-train ]^2)
par(mfrow = c(1, 1))
plot(1:5,c(test_err,test_quad,test_cubic,test_4,test_5), type = "b", col="blue",
ylab = "Test MSE",
xlab = "Flexibility",
main = "Validation Test MSE")
# Leave-one-out cross-validation (LOOCV) method
library(boot)
#1-Basic Commands
#Function c(), create a vector of numbers, ?funcName() ottieni info
#sulla funzione
x = (1,3,2,5);
#1-Basic Commands
#Function c(), create a vector of numbers, ?funcName() ottieni info
#sulla funzione
x = c(1,3,2,5)
y <- c(1,4,3)
?c
length(x)
lenght(y)
length(y)
x+y
y = c(1,4,3,7)
x+y
#The ls() function allows us to look at a list of all of the objects,
#such as data and functions, that we have saved so far. The
#function rm() can be #used to delete any that we don’t want.
ls()
#Insurance
Insurance <- read.table("insurance.csv", header = TRUE, sep = ",",quote = "\"", fileEncoding = "UTF-8")
###### clearing environment
rm(list = ls())
graphics.off()
#Here we apply the best subset selection approach to the Hitters data. We
#wish to predict a baseball player’s Salary on the basis of various statistics
#associated with performance in the previous year.
library ( ISLR2 )
#In order to use the validation set approach, we begin by splitting the
#observations into a training set and a test set. We do this by creating
#a random vector, train, of elements equal to TRUE if the corresponding
#observation is in the training set, and FALSE otherwise. The vector test has
#a TRUE if the observation is in the test set, and a FALSE otherwise.
set.seed(1)
train <- sample(c(TRUE , FALSE), nrow(Hitters), replace = TRUE)
test <- (!train)
#Now, we apply regsubsets() to the training set in order to perform best
#subset selection.
regfit_best <- regsubsets(Salary ~ ., data = Hitters[train , ], nvmax = 19)
#The regsubsets() function (part of the leaps library) performs best sub- set
#selection by identifying the best model that contains a given number
#of predictors, where best is quantified using RSS.
library(leaps)
#Now, we apply regsubsets() to the training set in order to perform best
#subset selection.
regfit_best <- regsubsets(Salary ~ ., data = Hitters[train , ], nvmax = 19)
#We now compute the validation set error for the best
#model of each model size. We first make a model matrix from the test
#data.
#The model.matrix() function is used in many regression packages for building
#an “X” matrix from data.
test_mat <- model.matrix(Salary ~ ., data = Hitters[test , ])
#Now we run a loop, and for each size i, we model.matrix()
#extract the coefficients from regfit.best for the best model of that size,
#multiply them into the appropriate columns of the test model matrix to
#form the predictions, and compute the test MSE.
val_errors <- rep(NA, 19)
for (i in 1:19) {
coefi <- coef(regfit_best , id = i)
pred <- test_mat[, names(coefi)] %*% coefi
val_errors[i] <- mean((Hitters$Salary[test] - pred)^2)
}
#We now compute the validation set error for the best
#model of each model size. We first make a model matrix from the test
#data.
#The model.matrix() function is used in many regression packages for building
#an “X” matrix from data.
test_mat <- model.matrix(Salary ~ ., data = Hitters[test , ])
#Now we run a loop, and for each size i, we model.matrix()
#extract the coefficients from regfit.best for the best model of that size,
#multiply them into the appropriate columns of the test model matrix to
#form the predictions, and compute the test MSE.
val_errors <- rep(NA, 19)
for (i in 1:19) {
coefi <- coef(regfit_best , id = i)
pred <- test_mat[, names(coefi)] %*% coefi
val_errors[i] <- mean((Hitters$Salary[test] - pred)^2)
}
val_errors
train <- sample(c(TRUE , FALSE), nrow(Hitters), replace = TRUE)
test <- (!train)
#Now, we apply regsubsets() to the training set in order to perform best
#subset selection.
regfit_best <- regsubsets(Salary ~ ., data = Hitters[train , ], nvmax = 19)
#We now compute the validation set error for the best
#model of each model size. We first make a model matrix from the test
#data.
#The model.matrix() function is used in many regression packages for building
#an “X” matrix from data.
test_mat <- model.matrix(Salary ~ ., data = Hitters[test , ])
#Now we run a loop, and for each size i, we model.matrix()
#extract the coefficients from regfit.best for the best model of that size,
#multiply them into the appropriate columns of the test model matrix to
#form the predictions, and compute the test MSE.
val_errors <- rep(NA, 19)
for (i in 1:19) {
coefi <- coef(regfit_best , id = i)
pred <- test_mat[, names(coefi)] %*% coefi
val_errors[i] <- mean((Hitters$Salary[test] - pred)^2)
}
test_mat
#Here we apply the best subset selection approach to the Hitters data. We
#wish to predict a baseball player’s Salary on the basis of various statistics
#associated with performance in the previous year.
library ( ISLR2 )
#Rimuovo valori mancanti
Hitters <- na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))
for (i in 1:19) {
coefi <- coef(regfit_best , id = i)
pred <- test_mat[, names(coefi)] %*% coefi
val_errors[i] <- mean((Hitters$Salary[test] - pred)^2)
}
#The regsubsets() function (part of the leaps library) performs best sub- set
#selection by identifying the best model that contains a given number
#of predictors, where best is quantified using RSS.
library(leaps)
regfit_full <- regsubsets(Salary ~ ., Hitters)
summary(regfit_full)
#By default, regsubsets() only reports results
#up to the best eight-variable model. But the nvmax option can be used
#in order to return as many variables as are desired.
regfit_full <- regsubsets(Salary ~ ., data = Hitters , nvmax = 19)
reg_summary <- summary(regfit_full) #fino a usare 19 regressori insieme
#We can examine these to try to select the best overall model.
names(reg_summary) #tutte le statistiche fornite
#we see that the R2 statistic increases from 32%, when only
#one variable is included in the model, to almost 55%, when all variables
#are included. As expected, the R2 statistic increases monotonically as more
#variables are included.
reg_summary$rsq
plot(reg_summary$rsq,xlab = "N° regressor",ylab = "R^2")
plot(reg_summary$rss , xlab = "Number of Variables",
ylab = "RSS", type = "l")
plot(reg_summary$adjr2 , xlab = "Number of Variables",
ylab = "Adjusted RSq", type = "l")
#We will now plot a red dot to indicate the model with the largest
#adjusted R2 statistic.
which.max(reg_summary$adjr2)
points(11, reg_summary$adjr2[11], col = "red", cex = 2, pch = 20)
#In a similar fashion we can plot the Cp and BIC statistics, and indicate the
#models with the smallest statistic using which.min().
plot(reg_summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
points(10, reg_summary$cp[10], col = "red", cex = 2, pch = 20)
which.min(reg_summary$bic)
plot(reg_summary$bic , xlab = "Number of Variables", ylab = "BIC", type = "l")
points(6, reg_summary$bic[6], col = "red", cex = 2, pch = 20)
plot(regfit_full , scale = "r2")
regfit_fwd <- regsubsets(Salary ~ ., data = Hitters , nvmax = 19,
method = "forward")
#In order to use the validation set approach, we begin by splitting the
#observations into a training set and a test set. We do this by creating
#a random vector, train, of elements equal to TRUE if the corresponding
#observation is in the training set, and FALSE otherwise. The vector test has
#a TRUE if the observation is in the test set, and a FALSE otherwise.
set.seed(1)
train <- sample(c(TRUE , FALSE), nrow(Hitters), replace = TRUE)
test <- (!train)
#Now, we apply regsubsets() to the training set in order to perform best
#subset selection.
regfit_best <- regsubsets(Salary ~ ., data = Hitters[train , ], nvmax = 19)
#We now compute the validation set error for the best
#model of each model size. We first make a model matrix from the test
#data.
#The model.matrix() function is used in many regression packages for building
#an “X” matrix from data.
test_mat <- model.matrix(Salary ~ ., data = Hitters[test , ])
for (i in 1:19) {
coefi <- coef(regfit_best , id = i)
pred <- test_mat[, names(coefi)] %*% coefi
val_errors[i] <- mean((Hitters$Salary[test] - pred)^2)
}
val_errors
#We find that the best model is the one that contains seven variables.
which.min(val_errors)
coef(regfit_best , 7)
summary(regfit_best)
test_mat
val_errors
#We find that the best model is the one that contains seven variables.
which.min(val_errors)
coef(regfit_best , 7)
predict.regsubsets <- function(object , newdata , id, ...) {
form <- as.formula(object$call [[2]])
mat <- model.matrix(form , newdata)
coefi <- coef(object , id = id)
xvars <- names(coefi)
mat[, xvars] %*% coefi
}
regfit_best <- regsubsets(Salary ~ ., data = Hitters ,nvmax = 19)
coef(regfit_best , 7)
#First, we create a vector that allocates each observation to one of
#k = 10 folds, and we create a matrix in which we will store the results.
k <- 10
n <- nrow(Hitters)
set.seed(1)
folds <- sample(rep(1:k, length = n)) #assegna casualmente le osservazioni a diversi fold per la cross-validation.
folds
cv.errors <- matrix(NA, k, 19, dimnames = list(NULL , paste (1:19)))
cv.er+
è
cv.errors
for (j in 1:k) {
best.fit <- regsubsets(Salary ~ .,data = Hitters[folds != j, ], nvmax = 19)
for (i in 1:19) {
pred <- predict(best.fit , Hitters[folds == j, ], id = i)
cv.errors[j, i] <-
mean((Hitters$Salary[folds == j] - pred)^2)
}
}
#This has given us a 10×19 matrix, of which the (j, i)th element corresponds
#to the test MSE for the jth cross-validation fold for the best i-variable
#model.
cv.errors
#This has given us a 10×19 matrix, of which the (j, i)th element corresponds
#to the test MSE for the jth cross-validation fold for the best i-variable
#model.
cv.errors
#We use the apply() function to average over the columns of this matrix in order
#to obtain a vector for which the apply() ith element is the crossvalidation
#error for the i-variable model.
mean.cv.errors <- apply(cv.errors , 2, mean)
mean.cv.errors
plot(mean.cv.errors , type = "b")
which.min(mean.cv.errors)
which.min(mean.cv.errors)
mean.cv.errors
which.min(mean.cv.errors)[1]
#We see that cross-validation selects a 10-variable model. We now perform
#best subset selection on the full data set in order to obtain the 10-variable
#model.
reg.best <- regsubsets(Salary ! ., data = Hitters , nvmax = 19)
#We see that cross-validation selects a 10-variable model. We now perform
#best subset selection on the full data set in order to obtain the 10-variable
#model.
reg.best <- regsubsets(Salary ~ ., data = Hitters , nvmax = 19)
coef(reg.best , 10)
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
setwd("~/Documents/GitHub/Statistical_Learning")
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
####shuffling####
Diamonds <- Diamonds[sample(nrow(Diamonds)), ]
View(Diamonds)
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
View(Diamonds)
################################################################################
######### Setting Dataset
################################################################################
set.seed(24)
####shuffling####
Diamonds <- Diamonds[sample(nrow(Diamonds)), ]
View(Diamonds)
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
View(Diamonds)
####shuffling####
Diamonds <- Diamonds[sample(nrow(Diamonds)), ]
View(Diamonds)
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
Diamonds <- subset(Diamonds , select = - X)
colnames(Diamonds)[5] = "depth_percentage"
colnames(Diamonds)[8] = "length"
colnames(Diamonds)[9] = "width"
colnames(Diamonds)[10] = "depth"
View(Diamonds)
####shuffling####
Diamonds <- Diamonds[sample(nrow(Diamonds)), ]
View(Diamonds)
write.csv(Diamonds, "diamonds.csv", row.names = FALSE)
###### clearing environment
rm(list = ls())
graphics.off()
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
View(Diamonds)
###### clearing environment
rm(list = ls())
################################################################################
######### Setting Dataset
################################################################################
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
### Transform Categorical Variables as factors
Diamonds$cut <- factor(Diamonds$cut,
levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))
Diamonds$color <- factor(Diamonds$color,
levels = c("J", "I", "H", "G", "F","E","D"))
Diamonds$clarity <- factor(Diamonds$clarity,
levels=c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"))
# no nan colums
colSums(is.na(Diamonds))
View(Diamonds)
################################################################################
#########################################  RIDGE REGRESSION
################################################################################
library ( ISLR2 )
library( glmnet )
#Hitters dataset
View(Hitters)
Hitters <- na.omit(Hitters) # remove row with nan;
# Construct Design Matrices and also automatically transforms any qualitative
# variables into dummy variables
x <- model.matrix ( Salary ~ . , Hitters ) [ , -1] #tutte le righe - la prima colonna
#The model.matrix() function is particularly useful for creating x; not only
#does it produce a matrix corresponding to the 19 predictors but it also
#automatically transforms any qualitative variables into dummy variables.
#The latter property is important because glmnet() can only take numerical,
#quantitative inputs.
x <- model.matrix ( Salary ~ . , Hitters ) [ , -1] #tutte le righe - la prima colonna
y <- Hitters$Salary
x
x
# lambda grid
lambda <- 10^seq(-2,3,length = 50); #genero una sequenza di 50 lambda
#The glmnet() function has an alpha argument that determines what type
#of model is fit. If alpha=0 then a ridge regression model is fit, and if alpha=1
#then a lasso model is fit.
#Note that by default, the glmnet() function standardizes the
#variables so that they are on the same scale. To turn off this default setting,
#use the argument standardize = FALSE
ridge_fit <- glmnet(x,y,alpha = 0,lambda = lambda,standardize=TRUE)
#Seleziono lambda 5 e calcolo l2 norm per lambda 5
ridge_fit$lambda[5]
sqrt(sum(coef(ridge_fit)[-1,5]^2)) #l2 norm (escludendo intercetta)
#Seleziono lambda 25 e calcolo l2 norm per lambda 25 (escludendo intercetta)
ridge_fit$lambda[25]
sqrt(sum(coef(ridge_fit)[-1,25]^2))
ridge.fit$lambda[50]
ridge_fit$lambda[50]
coef(ridge_fit)[, 50]
sqrt(sum(coef(ridge_fit)[-1, 50]^2))
sqrt(sum(coef(ridge_fit)[-1,25]^2))
#Seleziono lambda 50 e calcolo l2 norm per lambda 50 (escludendo intercetta)
ridge_fit$lambda[60]
# lambda grid
#here we have chosen to implement
#the function over a grid of values ranging from lamba = 10^10 to lambda = 10^-2,
#essentially covering the full range of scenarios from the null model containing
#only the intercept, to the least squares fit
lambda <- 10^seq(-2,3,length = 100); #genero una sequenza di 100 lambda
#The glmnet() function has an alpha argument that determines what type
#of model is fit. If alpha=0 then a ridge regression model is fit, and if alpha=1
#then a lasso model is fit.
#Note that by default, the glmnet() function standardizes the
#variables so that they are on the same scale. To turn off this default setting,
#use the argument standardize = FALSE
ridge_fit <- glmnet(x,y,alpha = 0,lambda = lambda,standardize=TRUE)
#Associated with each value of lambda is a vector of ridge regression coefficients,
#stored in a matrix that can be accessed by coef(). In this case is 20 x 50 matrix
dim(coef(ridge_fit))
#Seleziono lambda 5 e calcolo l2 norm per lambda 5
ridge_fit$lambda[5]
sqrt(sum(coef(ridge_fit)[-1,5]^2)) #l2 norm (escludendo intercetta)
#Seleziono lambda 25 e calcolo l2 norm per lambda 25 (escludendo intercetta)
ridge_fit$lambda[25]
sqrt(sum(coef(ridge_fit)[-1,25]^2))
#Seleziono lambda 50 e calcolo l2 norm per lambda 50 (escludendo intercetta)
ridge_fit$lambda[50]
sqrt(sum(coef(ridge_fit)[-1, 50]^2))
# lambda grid
#here we have chosen to implement
#the function over a grid of values ranging from lamba = 10^10 to lambda = 10^-2,
#essentially covering the full range of scenarios from the null model containing
#only the intercept, to the least squares fit
lambda <- 10^seq(10,-2,length = 100); #genero una sequenza di 100 lambda
#The glmnet() function has an alpha argument that determines what type
#of model is fit. If alpha=0 then a ridge regression model is fit, and if alpha=1
#then a lasso model is fit.
#Note that by default, the glmnet() function standardizes the
#variables so that they are on the same scale. To turn off this default setting,
#use the argument standardize = FALSE
ridge_fit <- glmnet(x,y,alpha = 0,lambda = lambda,standardize=TRUE)
#Seleziono lambda 5 e calcolo l2 norm per lambda 5
ridge_fit$lambda[5]
sqrt(sum(coef(ridge_fit)[-1,5]^2)) #l2 norm (escludendo intercetta)
coef(ridge_fit)[, 5]
#Seleziono lambda 60 e calcolo l2 norm per lambda 60 (escludendo intercetta)
ridge_fit$lambda[60]
coef(ridge_fit)[, 60]
sqrt(sum(coef(ridge_fit)[-1, 60]^2))
#Seleziono lambda 100 e calcolo l2 norm per lambda 100 (escludendo intercetta)
ridge_fit$lambda[100]
coef(ridge_fit)[, 100]
sqrt(sum(coef(ridge_fit)[-1, 100]^2))
sqrt(sum(coef(ridge_fit)[-1, 60]^2))
sqrt(sum(coef(ridge_fit)[-1, 50]^2))
sqrt(sum(coef(ridge_fit)[-1,25]^2))
sqrt(sum(coef(ridge_fit)[-1, 100]^2))
#We can use the predict() function for a number of purposes. For instance,
#we can obtain the ridge regression coefficients for a new value of lambda, say 50:
predict(ridge:fit , s = 50, type = "coefficients")[1:20, ]
#We can use the predict() function for a number of purposes. For instance,
#we can obtain the ridge regression coefficients for a new value of lambda, say 50:
predict(ridge_fit , s = 50, type = "coefficients")[1:20, ]
#We now split the samples into a training set and a test set in order
#to estimate the test error of ridge regression and the lasso.
set.seed(1)
train <- sample(1:nrow(x), nrow(x) / 2)
train
test <- (-train)
y.test <- y[test]
#Next we fit a ridge regression model on the training set, and evaluate
#its MSE on the test set, using lambda = 4. Note the use of the predict()
#function again. This time we get predictions for a test set, by replacing
#type="coefficients" with the newx argument.
ridge.mod <- glmnet(x[train , ], y[train], alpha = 0, lambda = grid,
thresh = 1e-12)
#Next we fit a ridge regression model on the training set, and evaluate
#its MSE on the test set, using lambda = 4. Note the use of the predict()
#function again. This time we get predictions for a test set, by replacing
#type="coefficients" with the newx argument.
ridge.mod <- glmnet(x[train , ], y[train], alpha = 0, lambda = grid,
thresh = 1e-12)
mean((ridge.pred - y.test)^2)
# lambda grid
#here we have chosen to implement
#the function over a grid of values ranging from lamba = 10^10 to lambda = 10^-2,
#essentially covering the full range of scenarios from the null model containing
#only the intercept, to the least squares fit
lambda <- 10^seq(10,-2,length = 100); #genero una sequenza di 100 lambda
#The glmnet() function has an alpha argument that determines what type
#of model is fit. If alpha=0 then a ridge regression model is fit, and if alpha=1
#then a lasso model is fit.
#Note that by default, the glmnet() function standardizes the
#variables so that they are on the same scale. To turn off this default setting,
#use the argument standardize = FALSE
ridge_fit <- glmnet(x,y,alpha = 0,lambda = lambda,standardize=TRUE)
#Associated with each value of lambda is a vector of ridge regression coefficients,
#stored in a matrix that can be accessed by coef(). In this case is 20 x 50 matrix
dim(coef(ridge_fit))
#compare l2-norm at different value of lambda:
#Seleziono lambda 5 e calcolo l2 norm per lambda 5
ridge_fit$lambda[5]
coef(ridge_fit)[, 5]
sqrt(sum(coef(ridge_fit)[-1,5]^2)) #l2 norm (escludendo intercetta)
#Seleziono lambda 25 e calcolo l2 norm per lambda 25 (escludendo intercetta)
ridge_fit$lambda[25]
coef(ridge_fit)[, 25]
sqrt(sum(coef(ridge_fit)[-1,25]^2))
#Seleziono lambda 50 e calcolo l2 norm per lambda 50 (escludendo intercetta)
ridge_fit$lambda[50]
coef(ridge_fit)[, 50]
sqrt(sum(coef(ridge_fit)[-1, 50]^2))
#Seleziono lambda 60 e calcolo l2 norm per lambda 60 (escludendo intercetta)
ridge_fit$lambda[60]
coef(ridge_fit)[, 60]
sqrt(sum(coef(ridge_fit)[-1, 60]^2))
#Seleziono lambda 100 e calcolo l2 norm per lambda 100 (escludendo intercetta)
ridge_fit$lambda[100]
coef(ridge_fit)[, 100]
sqrt(sum(coef(ridge_fit)[-1, 100]^2))
#We can use the predict() function for a number of purposes. For instance,
#we can obtain the ridge regression coefficients for a new value of lambda, say 50:
predict(ridge_fit , s = 50, type = "coefficients")[1:20, ]
#We can use the predict() function for a number of purposes. For instance,
#we can obtain the ridge regression coefficients for a new value of lambda, say 50:
predict(ridge_fit , s = 50, type = "coefficients")
#We can use the predict() function for a number of purposes. For instance,
#we can obtain the ridge regression coefficients for a new value of lambda, say 50:
predict(ridge_fit , s = 50, type = "coefficients")[1:20]
#We can use the predict() function for a number of purposes. For instance,
#we can obtain the ridge regression coefficients for a new value of lambda, say 50:
predict(ridge_fit , s = 50, type = "coefficients")[1:20, ]
#We can use the predict() function for a number of purposes. For instance,
#we can obtain the ridge regression coefficients for a new value of lambda, say 50:
predict(ridge_fit , s = 50, type = "coefficients")[1:100, ]
#We can use the predict() function for a number of purposes. For instance,
#we can obtain the ridge regression coefficients for a new value of lambda, say 50:
predict(ridge_fit , s = 50, type = "coefficients")[1:50, ]
#We can use the predict() function for a number of purposes. For instance,
#we can obtain the ridge regression coefficients for a new value of lambda, say 50:
predict(ridge_fit , s = 50, type = "coefficients")[1:19, ]
#We can use the predict() function for a number of purposes. For instance,
#we can obtain the ridge regression coefficients for a new value of lambda, say 50:
predict(ridge_fit , s = 50, type = "coefficients")[1:10, ]
#We can use the predict() function for a number of purposes. For instance,
#we can obtain the ridge regression coefficients for a new value of lambda, say 50:
predict(ridge_fit , s = 50, type = "coefficients")[1:20, ]
