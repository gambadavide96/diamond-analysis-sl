yhat_boost_2 <- predict(boost_model_2 , newdata = Diamonds[-train , ],
n.trees = 5000)
boost_RMSE_2 <- sqrt(mean((yhat_boost_2 - price_test)^2))
boost_RMSE_2
models <- c("Tree","Ridge","LM 1","Lasso","LM 2",
"BSS","Poly","GAM","Rand Forest","Bagging","Boosting")
errors <- c(tree_model_1_RMSE,test_RMSE_ridge,lm_test_RMSE_1,test_RMSE_lasso,
lm_test_RMSE_2,RMSE_subselection,
poly_test_RMSE,gam_model_1_RMSE,rf_RMSE,
bag_RMSE,boost_RMSE_2)
plot(1:length(models),errors, type = "b", col = "blue",
ylab = "Test RMSE",
xlab = "Models",
main = "Test RMSE comparison",
xaxt = "n")
axis(side = 1, at = 1:length(models), labels = models, las = 1)
tree_model_1
tree_model_1_RMSE
bag_RMSE
gam_model_1_RMSE
rf_RMSE
boost_RMSE_2
################################################################################
############################### Classification
################################################################################
library(ROSE)
library(e1071)
library(car)
#Setting dataset:
Diamonds$quality <- ifelse(Diamonds$cut %in% c("Very Good","Premium", "Ideal") &
Diamonds$color %in% c("G","F","E", "D") &
Diamonds$clarity %in% c("VS1","VVS2","VVS1", "IF"),
"High",
"Low")
Diamonds$quality <- factor(Diamonds$quality,levels=c("Low","High"))
#classe sbilanciata
table(Diamonds$quality)
undersampling_result <- ovun.sample(quality ~ carat + depth_percentage + table
+length + width + depth+price,
data = Diamonds,
method = "under",N = new_n)
##Undersampling
new_n <- 3017 / 0.5
undersampling_result <- ovun.sample(quality ~ carat + depth_percentage + table
+length + width + depth+price,
data = Diamonds,
method = "under",N = new_n)
Diamonds2 <- undersampling_result$data
View(Diamonds2)
#Divido il dataset in train e test
train <- sample(nrow(Diamonds2),floor(nrow(Diamonds2)*0.7),replace = FALSE)
contrasts(Diamonds2$quality) #il livello di riferimento è High
log_reg <- glm(quality ~ ., data = Diamonds2 ,family = binomial , subset = train)
hist(Diamonds$carat, 40 ,
xlab = "Carat",
main = "Carat distribution")
barplot(table(Diamonds$cut),
xlab = "Cut",
ylab = "Frequency",
main = "Cut distribution")
barplot(table(Diamonds$color),
xlab = "Color",
ylab = "Frequency",
main = "Color Distribution")
barplot(table(Diamonds$clarity),
xlab = "Clarity",
ylab = "Frequency",
main = "Clarity Distribution")
contrasts(Diamonds2$quality) #il livello di riferimento è High
log_reg <- glm(quality ~ ., data = Diamonds2 ,family = binomial , subset = train)
log_reg
summary(log_reg)
#calcolo probabilità che diamante di high su dataset test
log_probs <- predict(log_reg , Diamonds2[-train,],type = "response")
#Creo un vettore di previsioni tutte a low
log_pred <- rep("Low", nrow(Diamonds2[-train,]))
#Setto ad High solo le previsioni maggiori della prob 0.5
log_pred[log_probs > .5] = "High"
#Confusion matrix
table(log_pred,Diamonds2$quality[-train])
#Test error
mean(log_pred != Diamonds2$quality[-train])
#Accuracy 88%
mean(log_pred == Diamonds2$quality[-train])
#Accuracy 88%
mean(log_pred == Diamonds2$quality[-train])
#Etichette corrette di training
label_train <- Diamonds2$quality[train]
knn_fit <- knn(train=Diamonds2[train,1:7],test = Diamonds2[-train,1:7],
cl=label_train, k=3)
################################################################################
############################### KNN
################################################################################
library(class)
library(caret)
#Etichette corrette di training
label_train <- Diamonds2$quality[train]
knn_fit <- knn(train=Diamonds2[train,1:7],test = Diamonds2[-train,1:7],
cl=label_train, k=3)
#confusion matrix
table(knn_fit,Diamonds2$quality[-train])
#Test Error del 14%
mean(knn_fit != Diamonds2$quality[-train])
#accuracy
mean(knn_fit == Diamonds2$quality[-train])
#Selecting best k with cross-validation
train_control <- trainControl(method = "cv", number = 10) # 10-fold cross-validation
# Eseguire il tuning
tune_knn <- train(quality ~ ., data = Diamonds2[train,],
method = "knn",
tuneGrid = expand.grid(k = c(1,2,3,5,10,15,20)),
trControl = train_control)
# Stampare i risultati del tuning
print(tune_knn)
knn_fit_best <- knn(train=Diamonds2[train,1:7],test = Diamonds2[-train,1:7],
cl=label_train, k=5)
#confusion matrix
table(knn_fit_best,Diamonds2$quality[-train])
#Test Error del 14%
mean(knn_fit_best != Diamonds2$quality[-train])
#accuracy 85%
mean(knn_fit_best == Diamonds2$quality[-train])
#confusion matrix
table(knn_fit_best,Diamonds2$quality[-train])
#Test Error del 14%
mean(knn_fit_best != Diamonds2$quality[-train])
#accuracy 85%
mean(knn_fit_best == Diamonds2$quality[-train])
#Cerco il miglior cost con crossvalidazione
tune_linear <- tune(svm ,quality ~ ., data = Diamonds2[train,],
kernel = "linear",
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
best_linear <- tune_linear$best.model
summary(tune_linear)
##best_linear <- svm_linear <- svm(quality ~ ., data = Diamonds2 ,subset = train ,
#kernel = "linear",
#cost = 1, scale = TRUE)
summary(best_linear)
#Previsioni sul dataset di test
svm_linear_pred <- predict(best_linear , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_linear_pred , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_linear_pred != Diamonds2$quality[-train])
#Accuracy 88.62%
mean(svm_linear_pred == Diamonds2$quality[-train])
########################## SVM con kernel linear ###############################
svm_linear <- svm(quality ~ ., data = Diamonds2 ,subset = train ,
kernel = "linear",
cost = 1, scale = TRUE)
best_linear <- svm_linear <- svm(quality ~ ., data = Diamonds2 ,subset = train ,
kernel = "linear",
cost = 1, scale = TRUE)
summary(best_linear)
#Previsioni sul dataset di test
svm_linear_pred <- predict(best_linear , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_linear_pred , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_linear_pred != Diamonds2$quality[-train])
#Accuracy 88.62%
mean(svm_linear_pred == Diamonds2$quality[-train])
best_linear <- tune_linear$best.model
summary(best_linear)
#Previsioni sul dataset di test
svm_linear_pred <- predict(best_linear , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_linear_pred , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_linear_pred != Diamonds2$quality[-train])
#Accuracy 88.62%
mean(svm_linear_pred == Diamonds2$quality[-train])
#scelgo i miglipri cost e gamma con crossvalidazione
tune_radial <- tune(svm ,quality ~ ., data = Diamonds2[train,],
kernel = "radial",
ranges = list(
cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100),
gamma = c(0.5, 1, 2, 3, 4)))
summary(tune_radial)
#best parameters: cost 1 , gamma 0.5
best_radial <- tune_radial$best.model
summary(best_radial)
#Previsioni sul dataset di test
svm_radial_pred <- predict(best_radial , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_radial_pred , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_radial_pred != Diamonds2$quality[-train])
#accuracy 88.9%
mean(svm_radial_pred == Diamonds2$quality[-train])
best_poly <- svm_linear <- svm(quality ~ ., data = Diamonds2 ,subset = train ,
kernel = "polynomial",
cost = 1, gamma=0.5,
degree=4,scale = TRUE)
summary(best_poly)
#Previsioni sul dataset di test
svm_poly_pred <- predict(best_poly , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_poly_pred , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_poly_pred != Diamonds2$quality[-train])
#accuracy 86%
mean(svm_poly_pred == Diamonds2$quality[-train])
best_poly <- svm_linear <- svm(quality ~ ., data = Diamonds2 ,subset = train ,
kernel = "polynomial",
cost = 1, gamma=0.5,
degree=5,scale = TRUE)
summary(best_poly)
#Previsioni sul dataset di test
svm_poly_pred <- predict(best_poly , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_poly_pred , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_poly_pred != Diamonds2$quality[-train])
#accuracy 86%
mean(svm_poly_pred == Diamonds2$quality[-train])
best_poly <- svm_linear <- svm(quality ~ ., data = Diamonds2 ,subset = train ,
kernel = "polynomial",
cost = 1, gamma=0.5,
degree=3,scale = TRUE)
best_poly <- svm_linear <- svm(quality ~ ., data = Diamonds2 ,subset = train ,
kernel = "polynomial",
cost = 5, gamma=0.5,
degree=3,scale = TRUE)
#Previsioni sul dataset di test
svm_poly_pred <- predict(best_poly , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_poly_pred , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_poly_pred != Diamonds2$quality[-train])
#accuracy 86%
mean(svm_poly_pred == Diamonds2$quality[-train])
#vettore per i colori:
y <- ifelse(Diamonds2$quality == "High", 1, 0)
##Carat e price
plot(Diamonds2[train,"carat"],Diamonds2[train,"price"],
col=y[train] + 2,
ylab = "Price",
xlab="Carat")
legend(
"topright",  # Posizione della leggenda
legend = c("Low", "High"),  # Etichette della leggenda
col = c(2, 3),  # Colori corrispondenti
pch = 1  # Simbolo dei punti
)
svm_linear2 <- svm(quality ~ carat + price, data = Diamonds2 ,subset = train ,
kernel = "linear",
cost = 1, scale = TRUE)
plot(svm_linear2, Diamonds2[train,],price~carat)
svm_poly2 <- svm(quality ~ carat + price, data = Diamonds2 ,subset = train ,
kernel = "polynomial",
cost = 1,
degree = 3,
gamma=0.5,
scale = TRUE)
summary(svm_poly2)
plot(svm_poly2, Diamonds2[train,],price~carat)
#Previsioni sul dataset di test
svm_poly_pred2 <- predict(svm_poly2 , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_poly_pred2 , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_poly_pred2 != Diamonds2$quality[-train])
#Accuracy: 79.23%
mean(svm_poly_pred2 == Diamonds2$quality[-train])
svm_poly2 <- svm(quality ~ carat + price, data = Diamonds2 ,subset = train ,
kernel = "polynomial",
cost = 5,
degree = 3,
gamma=0.5,
scale = TRUE)
#Previsioni sul dataset di test
svm_poly_pred2 <- predict(svm_poly2 , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_poly_pred2 , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_poly_pred2 != Diamonds2$quality[-train])
#Accuracy: 79.23%
mean(svm_poly_pred2 == Diamonds2$quality[-train])
svm_linear2 <- svm(quality ~ carat + price, data = Diamonds2 ,subset = train ,
kernel = "linear",
cost = 5, scale = TRUE)
summary(svm_linear2)
plot(svm_linear2, Diamonds2[train,],price~carat)
#Previsioni sul dataset di test
svm_linear_pred2 <- predict(svm_linear2 , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_linear_pred2 , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_linear_pred2 != Diamonds2$quality[-train])
#Accuracy: 88.68%
mean(svm_linear_pred2 == Diamonds2$quality[-train])
svm_radial2 <- svm(quality ~ carat + price, data = Diamonds2 ,subset = train ,
kernel = "radial",
cost = 5,
gamma=0.5,
scale = TRUE)
plot(svm_radial2, Diamonds2[train,],price~carat)
#Previsioni sul dataset di test
svm_radial_pred2 <- predict(svm_radial2 , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_radial_pred2 , truth = Diamonds2$quality[-train])
#Test Error
mean(svm_radial_pred2 != Diamonds2$quality[-train])
#Accuracy: 88.95%
mean(svm_radial_pred2 == Diamonds2$quality[-train])
#Previsioni sul dataset di test
svm_linear_pred2 <- predict(svm_linear2 , Diamonds2[-train,])
#Confusion matrix
table(predict = svm_linear_pred2 , truth = Diamonds2$quality[-train])
#Accuracy: 88.68%
mean(svm_linear_pred2 == Diamonds2$quality[-train])
#classe sbilanciata
table(Diamonds$quality)
#Creo un vettore di previsioni tutte a low
log_pred <- rep("Low", nrow(Diamonds2[-train,]))
#Setto ad High solo le previsioni maggiori della prob 0.5
log_pred[log_probs > .5] = "High"
#Confusion matrix
table(log_pred,Diamonds2$quality[-train])
#Accuracy 88%
mean(log_pred == Diamonds2$quality[-train])
#confusion matrix
table(knn_fit,Diamonds2$quality[-train])
#accuracy 85.91%
mean(knn_fit == Diamonds2$quality[-train])
# Stampare i risultati del tuning
print(tune_knn)
#confusion matrix
table(knn_fit_best,Diamonds2$quality[-train])
#accuracy 85%
mean(knn_fit_best == Diamonds2$quality[-train])
#confusion matrix
table(knn_fit_best,Diamonds2$quality[-train])
#Confusion matrix
table(log_pred,Diamonds2$quality[-train])
#Confusion matrix
table(predict = svm_linear_pred , truth = Diamonds2$quality[-train])
#Accuracy 89.50%
mean(svm_linear_pred == Diamonds2$quality[-train])
#Confusion matrix
table(predict = svm_radial_pred , truth = Diamonds2$quality[-train])
#Confusion matrix
table(predict = svm_poly_pred , truth = Diamonds2$quality[-train])
plot(svm_radial2, Diamonds2[train,],price~carat)
#Confusion matrix
table(predict = svm_linear_pred2 , truth = Diamonds2$quality[-train])
#Accuracy: 88.68%
mean(svm_linear_pred2 == Diamonds2$quality[-train])
####### GAM train and test #######
gam_model_1 <- gam(price~ s(carat,4) + cut + color + clarity +
s(depth_percentage,3) + s(table,3) + s(length,4) +
s(width,4) + s(depth,4),data=Diamonds[train, ])
plot(gam_model_1,se=TRUE)
####### GAM train and test #######
gam_model_1 <- gam(price~ s(carat,4) + cut + color + clarity +
s(depth_percentage,5) + s(table,3) + s(length,4) +
s(width,4) + s(depth,4),data=Diamonds[train, ])
plot(gam_model_1,se=TRUE)
################################################################################
######### Setting Dataset
################################################################################
Diamonds <- read.table("diamonds.csv", header = TRUE,
sep = ",",
quote = "\"",
fileEncoding = "UTF-8")
### Transform Categorical Variables as factors
Diamonds$cut <- factor(Diamonds$cut,
levels = c("Fair", "Good", "Very Good", "Premium", "Ideal"))
Diamonds$color <- factor(Diamonds$color,
levels = c("J", "I", "H", "G", "F","E","D"))
Diamonds$clarity <- factor(Diamonds$clarity,
levels=c("I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"))
set.seed(2)
detect_outlier <- function(x) {
Quantile1 <- quantile(x, probs=.25)
Quantile3 <- quantile(x, probs=.75)
IQR = Quantile3 - Quantile1
x > Quantile3 + (IQR*1.5) | x < Quantile1 - (IQR*1.5)
}
remove_outlier <- function(dataframe,columns=names(dataframe)) {
for (col in columns) {
dataframe <- dataframe[!detect_outlier(dataframe[[col]]), ]
}
print("Remove outliers")
print(dataframe)
}
Diamonds <- remove_outlier(Diamonds, c('carat', 'depth_percentage', 'table', 'price',
"length", 'width', "depth"))
####### GAM train and test #######
gam_model_1 <- gam(price~ s(carat,4) + cut + color + clarity +
s(depth_percentage,5) + s(table,3) + s(length,4) +
s(width,4) + s(depth,4),data=Diamonds[train, ])
par(mfrow = c(1,1))
plot(gam_model_1,se=TRUE)
##### Analisi dei residui #####
gam_pred_value <- predict(gam_model_1,newdata = Diamonds[-train,])
#Test RMSE
gam_model_1_RMSE = sqrt(mean((Diamonds$price[-train] - gam_pred_value)^2))
gam_model_1_RMSE #Miglioramento significativo
####### GAM train and test #######
gam_model_1 <- gam(price~ s(carat,4) + cut + color + clarity +
s(depth_percentage,5) + s(table,5) + s(length,4) +
s(width,4) + s(depth,4),data=Diamonds[train, ])
##### Analisi dei residui #####
gam_pred_value <- predict(gam_model_1,newdata = Diamonds[-train,])
#Test RMSE
gam_model_1_RMSE = sqrt(mean((Diamonds$price[-train] - gam_pred_value)^2))
gam_model_1_RMSE #Miglioramento significativo
#train and test indexes
train <- sample(nrow(Diamonds),floor(nrow(Diamonds)*0.7),replace = FALSE)
price_test <- Diamonds$price[-train]
####### GAM train and test #######
gam_model_1 <- gam(price~ s(carat,4) + cut + color + clarity +
s(depth_percentage,5) + s(table,3) + s(length,4) +
s(width,4) + s(depth,4),data=Diamonds[train, ])
par(mfrow = c(1,1))
plot(gam_model_1,se=TRUE)
##### Analisi dei residui #####
gam_pred_value <- predict(gam_model_1,newdata = Diamonds[-train,])
#Test RMSE
gam_model_1_RMSE = sqrt(mean((Diamonds$price[-train] - gam_pred_value)^2))
gam_model_1_RMSE #Miglioramento significativo
?svm
?barplot
barplot(1:length(models),errors, type = "b", col = "blue",
ylab = "Test RMSE",
xlab = "Models",
main = "Test RMSE comparison",
xaxt = "n")
models <- c("Tree","Ridge","LM 1","Lasso","LM 2",
"BSS","Poly","GAM","Rand Forest","Bagging","Boosting")
errors <- c(tree_model_1_RMSE,test_RMSE_ridge,lm_test_RMSE_1,test_RMSE_lasso,
lm_test_RMSE_2,RMSE_subselection,
poly_test_RMSE,gam_model_1_RMSE,rf_RMSE,
bag_RMSE,boost_RMSE_2)
barplot(1:length(models),errors, type = "b", col = "blue",
ylab = "Test RMSE",
xlab = "Models",
main = "Test RMSE comparison",
xaxt = "n")
axis(side = 1, at = 1:length(models), labels = models, las = 1)
axis(side = 1, at = 1:length(models), labels = models, las = 1)
models <- c("Tree","Ridge","LM 1","Lasso","LM 2",
"BSS","Poly","GAM","Rand Forest","Bagging","Boosting")
errors <- c(tree_model_1_RMSE,test_RMSE_ridge,lm_test_RMSE_1,test_RMSE_lasso,
lm_test_RMSE_2,RMSE_subselection,
poly_test_RMSE,gam_model_1_RMSE,rf_RMSE,
bag_RMSE,boost_RMSE_2)
plot(1:length(models),errors, type = "b", col = "blue",
ylab = "Test RMSE",
xlab = "Models",
main = "Test RMSE comparison",
xaxt = "n")
axis(side = 1, at = 1:length(models), labels = models, las = 1)
barplot(1:length(models),errors, type = "b", col = "blue",
ylab = "Test RMSE",
xlab = "Models",
main = "Test RMSE comparison",
xaxt = "n")
plot(1:length(models),errors, type = "b", col = "blue",
ylab = "Test RMSE",
xlab = "Models",
main = "Test RMSE comparison",
xaxt = "n")
axis(side = 1, at = 1:length(models), labels = models, las = 1)
barplot(1:length(models),errors, type = "b", col = "blue",
ylab = "Test RMSE",
xlab = "Models",
main = "Test RMSE comparison",
xaxt = "n")
axis(side = 1, at = 1:length(models), labels = models, las = 1)
plot(1:length(models),errors, type = "b", col = "blue",
ylab = "Test RMSE",
xlab = "Models",
main = "Test RMSE comparison",
xaxt = "n")
axis(side = 1, at = 1:length(models), labels = models, las = 1)
barplot(height = errors,
names.arg = models,
col = "blue",
ylab = "Test RMSE",
xlab = "Models",
main = "Test RMSE comparison",
las = 2)  # las = 2 per ruotare le etichette dell'asse x verticalmente
barplot(height = errors,
names.arg = models,
col = "blue",
ylab = "Test RMSE",
xlab = "Models",
main = "Test RMSE comparison",
las = 1)  # las = 2 per ruotare le etichette dell'asse x verticalmente
barplot(height = errors,
names.arg = models,
col = "blue",
ylab = "Test RMSE",
main = "Test RMSE comparison",
las = 2)  # las = 2 per ruotare le etichette dell'asse x verticalmente
barplot(height = errors,
names.arg = models,
col = "blue",
ylab = "Test RMSE",
main = "Test RMSE comparison",
las = 2)  # las = 2 per ruotare le etichette dell'asse x verticalmente
models <- c("Tree","Ridge","LM 1","Lasso","LM 2",
"BSS","Poly","GAM","R. Forest","Bagging","Boosting")
barplot(height = errors,
names.arg = models,
col = "blue",
ylab = "Test RMSE",
main = "Test RMSE comparison",
las = 2)  # las = 2 per ruotare le etichette dell'asse x verticalmente
